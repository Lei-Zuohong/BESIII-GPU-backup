//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-21458526
// Driver 375.26
// Based on LLVM 3.4svn
//

.version 5.0
.target sm_35, texmode_independent
.address_size 64

	// .globl	int_sum_16

.entry int_sum_16(
	.param .u64 .ptr .global .align 4 int_sum_16_param_0,
	.param .u64 .ptr .global .align 8 int_sum_16_param_1
)
{
	.reg .b32 	%r<39>;
	.reg .f64 	%fd<32>;
	.reg .b64 	%rd<37>;


	ld.param.u64 	%rd1, [int_sum_16_param_0];
	ld.param.u64 	%rd2, [int_sum_16_param_1];
	mov.b32	%r1, %envreg3;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mad.lo.s32 	%r4, %r3, %r2, %r1;
	mov.u32 	%r5, %tid.x;
	add.s32 	%r6, %r4, %r5;
	shl.b32 	%r7, %r6, 4;
	mul.wide.u32 	%rd3, %r7, 4;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.u32 	%r8, [%rd4];
	cvt.rn.f64.s32	%fd1, %r8;
	add.s32 	%r9, %r7, 1;
	mul.wide.u32 	%rd5, %r9, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.u32 	%r10, [%rd6];
	cvt.rn.f64.s32	%fd2, %r10;
	add.f64 	%fd3, %fd1, %fd2;
	add.s32 	%r11, %r7, 2;
	mul.wide.u32 	%rd7, %r11, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.u32 	%r12, [%rd8];
	cvt.rn.f64.s32	%fd4, %r12;
	add.f64 	%fd5, %fd3, %fd4;
	add.s32 	%r13, %r7, 3;
	mul.wide.u32 	%rd9, %r13, 4;
	add.s64 	%rd10, %rd1, %rd9;
	ld.global.u32 	%r14, [%rd10];
	cvt.rn.f64.s32	%fd6, %r14;
	add.f64 	%fd7, %fd5, %fd6;
	add.s32 	%r15, %r7, 4;
	mul.wide.u32 	%rd11, %r15, 4;
	add.s64 	%rd12, %rd1, %rd11;
	ld.global.u32 	%r16, [%rd12];
	cvt.rn.f64.s32	%fd8, %r16;
	add.f64 	%fd9, %fd7, %fd8;
	add.s32 	%r17, %r7, 5;
	mul.wide.u32 	%rd13, %r17, 4;
	add.s64 	%rd14, %rd1, %rd13;
	ld.global.u32 	%r18, [%rd14];
	cvt.rn.f64.s32	%fd10, %r18;
	add.f64 	%fd11, %fd9, %fd10;
	add.s32 	%r19, %r7, 6;
	mul.wide.u32 	%rd15, %r19, 4;
	add.s64 	%rd16, %rd1, %rd15;
	ld.global.u32 	%r20, [%rd16];
	cvt.rn.f64.s32	%fd12, %r20;
	add.f64 	%fd13, %fd11, %fd12;
	add.s32 	%r21, %r7, 7;
	mul.wide.u32 	%rd17, %r21, 4;
	add.s64 	%rd18, %rd1, %rd17;
	ld.global.u32 	%r22, [%rd18];
	cvt.rn.f64.s32	%fd14, %r22;
	add.f64 	%fd15, %fd13, %fd14;
	add.s32 	%r23, %r7, 8;
	mul.wide.u32 	%rd19, %r23, 4;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.u32 	%r24, [%rd20];
	cvt.rn.f64.s32	%fd16, %r24;
	add.f64 	%fd17, %fd15, %fd16;
	add.s32 	%r25, %r7, 9;
	mul.wide.u32 	%rd21, %r25, 4;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.u32 	%r26, [%rd22];
	cvt.rn.f64.s32	%fd18, %r26;
	add.f64 	%fd19, %fd17, %fd18;
	add.s32 	%r27, %r7, 10;
	mul.wide.u32 	%rd23, %r27, 4;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.u32 	%r28, [%rd24];
	cvt.rn.f64.s32	%fd20, %r28;
	add.f64 	%fd21, %fd19, %fd20;
	add.s32 	%r29, %r7, 11;
	mul.wide.u32 	%rd25, %r29, 4;
	add.s64 	%rd26, %rd1, %rd25;
	ld.global.u32 	%r30, [%rd26];
	cvt.rn.f64.s32	%fd22, %r30;
	add.f64 	%fd23, %fd21, %fd22;
	add.s32 	%r31, %r7, 12;
	mul.wide.u32 	%rd27, %r31, 4;
	add.s64 	%rd28, %rd1, %rd27;
	ld.global.u32 	%r32, [%rd28];
	cvt.rn.f64.s32	%fd24, %r32;
	add.f64 	%fd25, %fd23, %fd24;
	add.s32 	%r33, %r7, 13;
	mul.wide.u32 	%rd29, %r33, 4;
	add.s64 	%rd30, %rd1, %rd29;
	ld.global.u32 	%r34, [%rd30];
	cvt.rn.f64.s32	%fd26, %r34;
	add.f64 	%fd27, %fd25, %fd26;
	add.s32 	%r35, %r7, 14;
	mul.wide.u32 	%rd31, %r35, 4;
	add.s64 	%rd32, %rd1, %rd31;
	ld.global.u32 	%r36, [%rd32];
	cvt.rn.f64.s32	%fd28, %r36;
	add.f64 	%fd29, %fd27, %fd28;
	add.s32 	%r37, %r7, 15;
	mul.wide.u32 	%rd33, %r37, 4;
	add.s64 	%rd34, %rd1, %rd33;
	ld.global.u32 	%r38, [%rd34];
	cvt.rn.f64.s32	%fd30, %r38;
	add.f64 	%fd31, %fd29, %fd30;
	mul.wide.u32 	%rd35, %r6, 8;
	add.s64 	%rd36, %rd2, %rd35;
	st.global.f64 	[%rd36], %fd31;
	ret;
}

	// .globl	int_sum_N
.entry int_sum_N(
	.param .u64 .ptr .global .align 4 int_sum_N_param_0,
	.param .u32 int_sum_N_param_1,
	.param .u32 int_sum_N_param_2,
	.param .u32 int_sum_N_param_3,
	.param .u64 .ptr .global .align 8 int_sum_N_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<21>;
	.reg .f64 	%fd<8>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [int_sum_N_param_0];
	ld.param.u32 	%r8, [int_sum_N_param_1];
	ld.param.u32 	%r9, [int_sum_N_param_2];
	ld.param.u32 	%r10, [int_sum_N_param_3];
	ld.param.u64 	%rd2, [int_sum_N_param_4];
	mov.b32	%r11, %envreg3;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %ntid.x;
	mad.lo.s32 	%r1, %r12, %r13, %r11;
	mov.u32 	%r2, %tid.x;
	setp.eq.s32	%p1, %r10, 0;
	mov.f64 	%fd7, 0d0000000000000000;
	@%p1 bra 	BB1_3;

	add.s32 	%r15, %r1, %r2;
	mad.lo.s32 	%r19, %r10, %r15, %r8;
	mov.u32 	%r20, 0;
	mov.f64 	%fd7, 0d0000000000000000;

BB1_2:
	mul.wide.u32 	%rd3, %r19, 4;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.u32 	%r16, [%rd4];
	cvt.rn.f64.s32	%fd6, %r16;
	add.f64 	%fd7, %fd7, %fd6;
	add.s32 	%r19, %r19, 1;
	add.s32 	%r20, %r20, 1;
	setp.lt.u32	%p2, %r20, %r10;
	@%p2 bra 	BB1_2;

BB1_3:
	add.s32 	%r17, %r1, %r2;
	add.s32 	%r18, %r17, %r9;
	mul.wide.u32 	%rd5, %r18, 8;
	add.s64 	%rd6, %rd2, %rd5;
	st.global.f64 	[%rd6], %fd7;
	ret;
}

	// .globl	int_sum2d_16
.entry int_sum2d_16(
	.param .u64 .ptr .global .align 4 int_sum2d_16_param_0,
	.param .u32 int_sum2d_16_param_1,
	.param .u64 .ptr .global .align 8 int_sum2d_16_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<86>;
	.reg .f64 	%fd<32>;
	.reg .b64 	%rd<37>;


	ld.param.u64 	%rd1, [int_sum2d_16_param_0];
	ld.param.u32 	%r28, [int_sum2d_16_param_1];
	ld.param.u64 	%rd2, [int_sum2d_16_param_2];
	setp.eq.s32	%p1, %r28, 0;
	@%p1 bra 	BB2_3;

	mov.b32	%r30, %envreg3;
	mov.u32 	%r31, %ntid.x;
	mov.u32 	%r32, %ctaid.x;
	mov.u32 	%r33, %tid.x;
	add.s32 	%r34, %r30, %r33;
	mad.lo.s32 	%r35, %r32, %r31, %r34;
	shl.b32 	%r36, %r35, 4;
	add.s32 	%r37, %r36, 15;
	mul.lo.s32 	%r84, %r28, %r37;
	add.s32 	%r38, %r36, 14;
	mul.lo.s32 	%r83, %r28, %r38;
	add.s32 	%r39, %r36, 13;
	mul.lo.s32 	%r82, %r28, %r39;
	add.s32 	%r40, %r36, 12;
	mul.lo.s32 	%r81, %r28, %r40;
	add.s32 	%r41, %r36, 11;
	mul.lo.s32 	%r5, %r28, %r41;
	add.s32 	%r42, %r36, 10;
	mul.lo.s32 	%r6, %r28, %r42;
	add.s32 	%r43, %r36, 9;
	mul.lo.s32 	%r7, %r28, %r43;
	add.s32 	%r44, %r36, 8;
	mul.lo.s32 	%r8, %r28, %r44;
	add.s32 	%r45, %r36, 7;
	mul.lo.s32 	%r9, %r28, %r45;
	add.s32 	%r46, %r36, 6;
	mul.lo.s32 	%r10, %r28, %r46;
	add.s32 	%r47, %r36, 5;
	mul.lo.s32 	%r11, %r28, %r47;
	add.s32 	%r48, %r36, 4;
	mul.lo.s32 	%r12, %r28, %r48;
	add.s32 	%r49, %r36, 3;
	mul.lo.s32 	%r13, %r28, %r49;
	add.s32 	%r50, %r36, 2;
	mul.lo.s32 	%r14, %r28, %r50;
	add.s32 	%r51, %r36, 1;
	mul.lo.s32 	%r15, %r28, %r51;
	mul.lo.s32 	%r16, %r28, %r35;
	shl.b32 	%r17, %r16, 4;
	mov.u32 	%r85, 0;

BB2_2:
	add.s32 	%r52, %r17, %r85;
	mul.wide.u32 	%rd3, %r52, 4;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.u32 	%r53, [%rd4];
	cvt.rn.f64.s32	%fd1, %r53;
	add.s32 	%r54, %r15, %r85;
	mul.wide.u32 	%rd5, %r54, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.u32 	%r55, [%rd6];
	cvt.rn.f64.s32	%fd2, %r55;
	add.f64 	%fd3, %fd1, %fd2;
	add.s32 	%r56, %r14, %r85;
	mul.wide.u32 	%rd7, %r56, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.u32 	%r57, [%rd8];
	cvt.rn.f64.s32	%fd4, %r57;
	add.f64 	%fd5, %fd3, %fd4;
	add.s32 	%r58, %r13, %r85;
	mul.wide.u32 	%rd9, %r58, 4;
	add.s64 	%rd10, %rd1, %rd9;
	ld.global.u32 	%r59, [%rd10];
	cvt.rn.f64.s32	%fd6, %r59;
	add.f64 	%fd7, %fd5, %fd6;
	add.s32 	%r60, %r12, %r85;
	mul.wide.u32 	%rd11, %r60, 4;
	add.s64 	%rd12, %rd1, %rd11;
	ld.global.u32 	%r61, [%rd12];
	cvt.rn.f64.s32	%fd8, %r61;
	add.f64 	%fd9, %fd7, %fd8;
	add.s32 	%r62, %r11, %r85;
	mul.wide.u32 	%rd13, %r62, 4;
	add.s64 	%rd14, %rd1, %rd13;
	ld.global.u32 	%r63, [%rd14];
	cvt.rn.f64.s32	%fd10, %r63;
	add.f64 	%fd11, %fd9, %fd10;
	add.s32 	%r64, %r10, %r85;
	mul.wide.u32 	%rd15, %r64, 4;
	add.s64 	%rd16, %rd1, %rd15;
	ld.global.u32 	%r65, [%rd16];
	cvt.rn.f64.s32	%fd12, %r65;
	add.f64 	%fd13, %fd11, %fd12;
	add.s32 	%r66, %r9, %r85;
	mul.wide.u32 	%rd17, %r66, 4;
	add.s64 	%rd18, %rd1, %rd17;
	ld.global.u32 	%r67, [%rd18];
	cvt.rn.f64.s32	%fd14, %r67;
	add.f64 	%fd15, %fd13, %fd14;
	add.s32 	%r68, %r8, %r85;
	mul.wide.u32 	%rd19, %r68, 4;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.u32 	%r69, [%rd20];
	cvt.rn.f64.s32	%fd16, %r69;
	add.f64 	%fd17, %fd15, %fd16;
	add.s32 	%r70, %r7, %r85;
	mul.wide.u32 	%rd21, %r70, 4;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.u32 	%r71, [%rd22];
	cvt.rn.f64.s32	%fd18, %r71;
	add.f64 	%fd19, %fd17, %fd18;
	add.s32 	%r72, %r6, %r85;
	mul.wide.u32 	%rd23, %r72, 4;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.u32 	%r73, [%rd24];
	cvt.rn.f64.s32	%fd20, %r73;
	add.f64 	%fd21, %fd19, %fd20;
	add.s32 	%r74, %r5, %r85;
	mul.wide.u32 	%rd25, %r74, 4;
	add.s64 	%rd26, %rd1, %rd25;
	ld.global.u32 	%r75, [%rd26];
	cvt.rn.f64.s32	%fd22, %r75;
	add.f64 	%fd23, %fd21, %fd22;
	mul.wide.u32 	%rd27, %r81, 4;
	add.s64 	%rd28, %rd1, %rd27;
	ld.global.u32 	%r76, [%rd28];
	cvt.rn.f64.s32	%fd24, %r76;
	add.f64 	%fd25, %fd23, %fd24;
	mul.wide.u32 	%rd29, %r82, 4;
	add.s64 	%rd30, %rd1, %rd29;
	ld.global.u32 	%r77, [%rd30];
	cvt.rn.f64.s32	%fd26, %r77;
	add.f64 	%fd27, %fd25, %fd26;
	mul.wide.u32 	%rd31, %r83, 4;
	add.s64 	%rd32, %rd1, %rd31;
	ld.global.u32 	%r78, [%rd32];
	cvt.rn.f64.s32	%fd28, %r78;
	add.f64 	%fd29, %fd27, %fd28;
	mul.wide.u32 	%rd33, %r84, 4;
	add.s64 	%rd34, %rd1, %rd33;
	ld.global.u32 	%r79, [%rd34];
	cvt.rn.f64.s32	%fd30, %r79;
	add.f64 	%fd31, %fd29, %fd30;
	add.s32 	%r80, %r16, %r85;
	mul.wide.u32 	%rd35, %r80, 8;
	add.s64 	%rd36, %rd2, %rd35;
	st.global.f64 	[%rd36], %fd31;
	add.s32 	%r84, %r84, 1;
	add.s32 	%r83, %r83, 1;
	add.s32 	%r82, %r82, 1;
	add.s32 	%r81, %r81, 1;
	add.s32 	%r85, %r85, 1;
	setp.lt.u32	%p2, %r85, %r28;
	@%p2 bra 	BB2_2;

BB2_3:
	ret;
}

	// .globl	int_sum2d_N
.entry int_sum2d_N(
	.param .u64 .ptr .global .align 4 int_sum2d_N_param_0,
	.param .u32 int_sum2d_N_param_1,
	.param .u32 int_sum2d_N_param_2,
	.param .u32 int_sum2d_N_param_3,
	.param .u32 int_sum2d_N_param_4,
	.param .u64 .ptr .global .align 8 int_sum2d_N_param_5
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<29>;
	.reg .f64 	%fd<10>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [int_sum2d_N_param_0];
	ld.param.u32 	%r10, [int_sum2d_N_param_1];
	ld.param.u32 	%r11, [int_sum2d_N_param_2];
	ld.param.u32 	%r12, [int_sum2d_N_param_3];
	ld.param.u32 	%r13, [int_sum2d_N_param_4];
	ld.param.u64 	%rd2, [int_sum2d_N_param_5];
	setp.eq.s32	%p1, %r10, 0;
	@%p1 bra 	BB3_5;

	mov.b32	%r15, %envreg3;
	mov.u32 	%r16, %ctaid.x;
	mov.u32 	%r17, %ntid.x;
	mad.lo.s32 	%r18, %r16, %r17, %r15;
	mov.u32 	%r19, %tid.x;
	add.s32 	%r20, %r18, %r19;
	mul.lo.s32 	%r21, %r20, %r10;
	add.s32 	%r1, %r21, %r12;
	mad.lo.s32 	%r2, %r21, %r13, %r11;
	mov.u32 	%r14, 0;
	mov.u32 	%r28, %r14;

BB3_2:
	add.s32 	%r25, %r2, %r28;
	setp.eq.s32	%p2, %r13, 0;
	mov.f64 	%fd9, 0d0000000000000000;
	mov.f64 	%fd8, %fd9;
	mov.u32 	%r27, %r14;
	@%p2 bra 	BB3_4;

BB3_3:
	mov.u32 	%r6, %r27;
	mul.wide.u32 	%rd3, %r25, 4;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.u32 	%r23, [%rd4];
	cvt.rn.f64.s32	%fd6, %r23;
	add.f64 	%fd9, %fd9, %fd6;
	add.s32 	%r25, %r25, %r10;
	add.s32 	%r8, %r6, 1;
	setp.lt.u32	%p3, %r8, %r13;
	mov.u32 	%r27, %r8;
	mov.f64 	%fd8, %fd9;
	@%p3 bra 	BB3_3;

BB3_4:
	add.s32 	%r24, %r1, %r28;
	mul.wide.u32 	%rd5, %r24, 8;
	add.s64 	%rd6, %rd2, %rd5;
	st.global.f64 	[%rd6], %fd8;
	add.s32 	%r28, %r28, 1;
	setp.lt.u32	%p4, %r28, %r10;
	@%p4 bra 	BB3_2;

BB3_5:
	ret;
}

	// .globl	int_sum2d_16_weighted
.entry int_sum2d_16_weighted(
	.param .u64 .ptr .global .align 16 int_sum2d_16_weighted_param_0,
	.param .u64 .ptr .global .align 4 int_sum2d_16_weighted_param_1,
	.param .u32 int_sum2d_16_weighted_param_2,
	.param .u64 .ptr .global .align 8 int_sum2d_16_weighted_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<86>;
	.reg .f64 	%fd<32>;
	.reg .b64 	%rd<37>;


	ld.param.u64 	%rd1, [int_sum2d_16_weighted_param_1];
	ld.param.u32 	%r28, [int_sum2d_16_weighted_param_2];
	ld.param.u64 	%rd2, [int_sum2d_16_weighted_param_3];
	setp.eq.s32	%p1, %r28, 0;
	@%p1 bra 	BB4_3;

	mov.b32	%r30, %envreg3;
	mov.u32 	%r31, %ntid.x;
	mov.u32 	%r32, %ctaid.x;
	mov.u32 	%r33, %tid.x;
	add.s32 	%r34, %r30, %r33;
	mad.lo.s32 	%r35, %r32, %r31, %r34;
	shl.b32 	%r36, %r35, 4;
	add.s32 	%r37, %r36, 15;
	mul.lo.s32 	%r84, %r28, %r37;
	add.s32 	%r38, %r36, 14;
	mul.lo.s32 	%r83, %r28, %r38;
	add.s32 	%r39, %r36, 13;
	mul.lo.s32 	%r82, %r28, %r39;
	add.s32 	%r40, %r36, 12;
	mul.lo.s32 	%r81, %r28, %r40;
	add.s32 	%r41, %r36, 11;
	mul.lo.s32 	%r5, %r28, %r41;
	add.s32 	%r42, %r36, 10;
	mul.lo.s32 	%r6, %r28, %r42;
	add.s32 	%r43, %r36, 9;
	mul.lo.s32 	%r7, %r28, %r43;
	add.s32 	%r44, %r36, 8;
	mul.lo.s32 	%r8, %r28, %r44;
	add.s32 	%r45, %r36, 7;
	mul.lo.s32 	%r9, %r28, %r45;
	add.s32 	%r46, %r36, 6;
	mul.lo.s32 	%r10, %r28, %r46;
	add.s32 	%r47, %r36, 5;
	mul.lo.s32 	%r11, %r28, %r47;
	add.s32 	%r48, %r36, 4;
	mul.lo.s32 	%r12, %r28, %r48;
	add.s32 	%r49, %r36, 3;
	mul.lo.s32 	%r13, %r28, %r49;
	add.s32 	%r50, %r36, 2;
	mul.lo.s32 	%r14, %r28, %r50;
	add.s32 	%r51, %r36, 1;
	mul.lo.s32 	%r15, %r28, %r51;
	mul.lo.s32 	%r16, %r28, %r35;
	shl.b32 	%r17, %r16, 4;
	mov.u32 	%r85, 0;

BB4_2:
	add.s32 	%r52, %r17, %r85;
	mul.wide.u32 	%rd3, %r52, 4;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.u32 	%r53, [%rd4];
	cvt.rn.f64.s32	%fd1, %r53;
	add.s32 	%r54, %r15, %r85;
	mul.wide.u32 	%rd5, %r54, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.u32 	%r55, [%rd6];
	cvt.rn.f64.s32	%fd2, %r55;
	add.f64 	%fd3, %fd1, %fd2;
	add.s32 	%r56, %r14, %r85;
	mul.wide.u32 	%rd7, %r56, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.u32 	%r57, [%rd8];
	cvt.rn.f64.s32	%fd4, %r57;
	add.f64 	%fd5, %fd3, %fd4;
	add.s32 	%r58, %r13, %r85;
	mul.wide.u32 	%rd9, %r58, 4;
	add.s64 	%rd10, %rd1, %rd9;
	ld.global.u32 	%r59, [%rd10];
	cvt.rn.f64.s32	%fd6, %r59;
	add.f64 	%fd7, %fd5, %fd6;
	add.s32 	%r60, %r12, %r85;
	mul.wide.u32 	%rd11, %r60, 4;
	add.s64 	%rd12, %rd1, %rd11;
	ld.global.u32 	%r61, [%rd12];
	cvt.rn.f64.s32	%fd8, %r61;
	add.f64 	%fd9, %fd7, %fd8;
	add.s32 	%r62, %r11, %r85;
	mul.wide.u32 	%rd13, %r62, 4;
	add.s64 	%rd14, %rd1, %rd13;
	ld.global.u32 	%r63, [%rd14];
	cvt.rn.f64.s32	%fd10, %r63;
	add.f64 	%fd11, %fd9, %fd10;
	add.s32 	%r64, %r10, %r85;
	mul.wide.u32 	%rd15, %r64, 4;
	add.s64 	%rd16, %rd1, %rd15;
	ld.global.u32 	%r65, [%rd16];
	cvt.rn.f64.s32	%fd12, %r65;
	add.f64 	%fd13, %fd11, %fd12;
	add.s32 	%r66, %r9, %r85;
	mul.wide.u32 	%rd17, %r66, 4;
	add.s64 	%rd18, %rd1, %rd17;
	ld.global.u32 	%r67, [%rd18];
	cvt.rn.f64.s32	%fd14, %r67;
	add.f64 	%fd15, %fd13, %fd14;
	add.s32 	%r68, %r8, %r85;
	mul.wide.u32 	%rd19, %r68, 4;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.u32 	%r69, [%rd20];
	cvt.rn.f64.s32	%fd16, %r69;
	add.f64 	%fd17, %fd15, %fd16;
	add.s32 	%r70, %r7, %r85;
	mul.wide.u32 	%rd21, %r70, 4;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.u32 	%r71, [%rd22];
	cvt.rn.f64.s32	%fd18, %r71;
	add.f64 	%fd19, %fd17, %fd18;
	add.s32 	%r72, %r6, %r85;
	mul.wide.u32 	%rd23, %r72, 4;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.u32 	%r73, [%rd24];
	cvt.rn.f64.s32	%fd20, %r73;
	add.f64 	%fd21, %fd19, %fd20;
	add.s32 	%r74, %r5, %r85;
	mul.wide.u32 	%rd25, %r74, 4;
	add.s64 	%rd26, %rd1, %rd25;
	ld.global.u32 	%r75, [%rd26];
	cvt.rn.f64.s32	%fd22, %r75;
	add.f64 	%fd23, %fd21, %fd22;
	mul.wide.u32 	%rd27, %r81, 4;
	add.s64 	%rd28, %rd1, %rd27;
	ld.global.u32 	%r76, [%rd28];
	cvt.rn.f64.s32	%fd24, %r76;
	add.f64 	%fd25, %fd23, %fd24;
	mul.wide.u32 	%rd29, %r82, 4;
	add.s64 	%rd30, %rd1, %rd29;
	ld.global.u32 	%r77, [%rd30];
	cvt.rn.f64.s32	%fd26, %r77;
	add.f64 	%fd27, %fd25, %fd26;
	mul.wide.u32 	%rd31, %r83, 4;
	add.s64 	%rd32, %rd1, %rd31;
	ld.global.u32 	%r78, [%rd32];
	cvt.rn.f64.s32	%fd28, %r78;
	add.f64 	%fd29, %fd27, %fd28;
	mul.wide.u32 	%rd33, %r84, 4;
	add.s64 	%rd34, %rd1, %rd33;
	ld.global.u32 	%r79, [%rd34];
	cvt.rn.f64.s32	%fd30, %r79;
	add.f64 	%fd31, %fd29, %fd30;
	add.s32 	%r80, %r16, %r85;
	mul.wide.u32 	%rd35, %r80, 8;
	add.s64 	%rd36, %rd2, %rd35;
	st.global.f64 	[%rd36], %fd31;
	add.s32 	%r84, %r84, 1;
	add.s32 	%r83, %r83, 1;
	add.s32 	%r82, %r82, 1;
	add.s32 	%r81, %r81, 1;
	add.s32 	%r85, %r85, 1;
	setp.lt.u32	%p2, %r85, %r28;
	@%p2 bra 	BB4_2;

BB4_3:
	ret;
}

	// .globl	int_sum2d_N_weighted
.entry int_sum2d_N_weighted(
	.param .u64 .ptr .global .align 16 int_sum2d_N_weighted_param_0,
	.param .u64 .ptr .global .align 4 int_sum2d_N_weighted_param_1,
	.param .u32 int_sum2d_N_weighted_param_2,
	.param .u32 int_sum2d_N_weighted_param_3,
	.param .u32 int_sum2d_N_weighted_param_4,
	.param .u32 int_sum2d_N_weighted_param_5,
	.param .u64 .ptr .global .align 8 int_sum2d_N_weighted_param_6
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<29>;
	.reg .f64 	%fd<10>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [int_sum2d_N_weighted_param_1];
	ld.param.u32 	%r10, [int_sum2d_N_weighted_param_2];
	ld.param.u32 	%r11, [int_sum2d_N_weighted_param_3];
	ld.param.u32 	%r12, [int_sum2d_N_weighted_param_4];
	ld.param.u32 	%r13, [int_sum2d_N_weighted_param_5];
	ld.param.u64 	%rd2, [int_sum2d_N_weighted_param_6];
	setp.eq.s32	%p1, %r10, 0;
	@%p1 bra 	BB5_5;

	mov.b32	%r15, %envreg3;
	mov.u32 	%r16, %ctaid.x;
	mov.u32 	%r17, %ntid.x;
	mad.lo.s32 	%r18, %r16, %r17, %r15;
	mov.u32 	%r19, %tid.x;
	add.s32 	%r20, %r18, %r19;
	mul.lo.s32 	%r21, %r20, %r10;
	add.s32 	%r1, %r21, %r12;
	mad.lo.s32 	%r2, %r21, %r13, %r11;
	mov.u32 	%r14, 0;
	mov.u32 	%r28, %r14;

BB5_2:
	add.s32 	%r25, %r2, %r28;
	setp.eq.s32	%p2, %r13, 0;
	mov.f64 	%fd9, 0d0000000000000000;
	mov.f64 	%fd8, %fd9;
	mov.u32 	%r27, %r14;
	@%p2 bra 	BB5_4;

BB5_3:
	mov.u32 	%r6, %r27;
	mul.wide.u32 	%rd3, %r25, 4;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.u32 	%r23, [%rd4];
	cvt.rn.f64.s32	%fd6, %r23;
	add.f64 	%fd9, %fd9, %fd6;
	add.s32 	%r25, %r25, %r10;
	add.s32 	%r8, %r6, 1;
	setp.lt.u32	%p3, %r8, %r13;
	mov.u32 	%r27, %r8;
	mov.f64 	%fd8, %fd9;
	@%p3 bra 	BB5_3;

BB5_4:
	add.s32 	%r24, %r1, %r28;
	mul.wide.u32 	%rd5, %r24, 8;
	add.s64 	%rd6, %rd2, %rd5;
	st.global.f64 	[%rd6], %fd8;
	add.s32 	%r28, %r28, 1;
	setp.lt.u32	%p4, %r28, %r10;
	@%p4 bra 	BB5_2;

BB5_5:
	ret;
}

	// .globl	float_sum_16
.entry float_sum_16(
	.param .u64 .ptr .global .align 4 float_sum_16_param_0,
	.param .u64 .ptr .global .align 8 float_sum_16_param_1
)
{
	.reg .f32 	%f<17>;
	.reg .b32 	%r<23>;
	.reg .f64 	%fd<33>;
	.reg .b64 	%rd<37>;


	ld.param.u64 	%rd1, [float_sum_16_param_0];
	ld.param.u64 	%rd2, [float_sum_16_param_1];
	mov.b32	%r1, %envreg3;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mad.lo.s32 	%r4, %r3, %r2, %r1;
	mov.u32 	%r5, %tid.x;
	add.s32 	%r6, %r4, %r5;
	shl.b32 	%r7, %r6, 4;
	mul.wide.u32 	%rd3, %r7, 4;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.f32 	%f1, [%rd4];
	cvt.f64.f32	%fd1, %f1;
	add.f64 	%fd2, %fd1, 0d0000000000000000;
	add.s32 	%r8, %r7, 1;
	mul.wide.u32 	%rd5, %r8, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.f32 	%f2, [%rd6];
	cvt.f64.f32	%fd3, %f2;
	add.f64 	%fd4, %fd2, %fd3;
	add.s32 	%r9, %r7, 2;
	mul.wide.u32 	%rd7, %r9, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.f32 	%f3, [%rd8];
	cvt.f64.f32	%fd5, %f3;
	add.f64 	%fd6, %fd4, %fd5;
	add.s32 	%r10, %r7, 3;
	mul.wide.u32 	%rd9, %r10, 4;
	add.s64 	%rd10, %rd1, %rd9;
	ld.global.f32 	%f4, [%rd10];
	cvt.f64.f32	%fd7, %f4;
	add.f64 	%fd8, %fd6, %fd7;
	add.s32 	%r11, %r7, 4;
	mul.wide.u32 	%rd11, %r11, 4;
	add.s64 	%rd12, %rd1, %rd11;
	ld.global.f32 	%f5, [%rd12];
	cvt.f64.f32	%fd9, %f5;
	add.f64 	%fd10, %fd8, %fd9;
	add.s32 	%r12, %r7, 5;
	mul.wide.u32 	%rd13, %r12, 4;
	add.s64 	%rd14, %rd1, %rd13;
	ld.global.f32 	%f6, [%rd14];
	cvt.f64.f32	%fd11, %f6;
	add.f64 	%fd12, %fd10, %fd11;
	add.s32 	%r13, %r7, 6;
	mul.wide.u32 	%rd15, %r13, 4;
	add.s64 	%rd16, %rd1, %rd15;
	ld.global.f32 	%f7, [%rd16];
	cvt.f64.f32	%fd13, %f7;
	add.f64 	%fd14, %fd12, %fd13;
	add.s32 	%r14, %r7, 7;
	mul.wide.u32 	%rd17, %r14, 4;
	add.s64 	%rd18, %rd1, %rd17;
	ld.global.f32 	%f8, [%rd18];
	cvt.f64.f32	%fd15, %f8;
	add.f64 	%fd16, %fd14, %fd15;
	add.s32 	%r15, %r7, 8;
	mul.wide.u32 	%rd19, %r15, 4;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.f32 	%f9, [%rd20];
	cvt.f64.f32	%fd17, %f9;
	add.f64 	%fd18, %fd16, %fd17;
	add.s32 	%r16, %r7, 9;
	mul.wide.u32 	%rd21, %r16, 4;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.f32 	%f10, [%rd22];
	cvt.f64.f32	%fd19, %f10;
	add.f64 	%fd20, %fd18, %fd19;
	add.s32 	%r17, %r7, 10;
	mul.wide.u32 	%rd23, %r17, 4;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.f32 	%f11, [%rd24];
	cvt.f64.f32	%fd21, %f11;
	add.f64 	%fd22, %fd20, %fd21;
	add.s32 	%r18, %r7, 11;
	mul.wide.u32 	%rd25, %r18, 4;
	add.s64 	%rd26, %rd1, %rd25;
	ld.global.f32 	%f12, [%rd26];
	cvt.f64.f32	%fd23, %f12;
	add.f64 	%fd24, %fd22, %fd23;
	add.s32 	%r19, %r7, 12;
	mul.wide.u32 	%rd27, %r19, 4;
	add.s64 	%rd28, %rd1, %rd27;
	ld.global.f32 	%f13, [%rd28];
	cvt.f64.f32	%fd25, %f13;
	add.f64 	%fd26, %fd24, %fd25;
	add.s32 	%r20, %r7, 13;
	mul.wide.u32 	%rd29, %r20, 4;
	add.s64 	%rd30, %rd1, %rd29;
	ld.global.f32 	%f14, [%rd30];
	cvt.f64.f32	%fd27, %f14;
	add.f64 	%fd28, %fd26, %fd27;
	add.s32 	%r21, %r7, 14;
	mul.wide.u32 	%rd31, %r21, 4;
	add.s64 	%rd32, %rd1, %rd31;
	ld.global.f32 	%f15, [%rd32];
	cvt.f64.f32	%fd29, %f15;
	add.f64 	%fd30, %fd28, %fd29;
	add.s32 	%r22, %r7, 15;
	mul.wide.u32 	%rd33, %r22, 4;
	add.s64 	%rd34, %rd1, %rd33;
	ld.global.f32 	%f16, [%rd34];
	cvt.f64.f32	%fd31, %f16;
	add.f64 	%fd32, %fd30, %fd31;
	mul.wide.u32 	%rd35, %r6, 8;
	add.s64 	%rd36, %rd2, %rd35;
	st.global.f64 	[%rd36], %fd32;
	ret;
}

	// .globl	float_sum_N
.entry float_sum_N(
	.param .u64 .ptr .global .align 4 float_sum_N_param_0,
	.param .u32 float_sum_N_param_1,
	.param .u32 float_sum_N_param_2,
	.param .u32 float_sum_N_param_3,
	.param .u64 .ptr .global .align 8 float_sum_N_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<20>;
	.reg .f64 	%fd<8>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [float_sum_N_param_0];
	ld.param.u32 	%r8, [float_sum_N_param_1];
	ld.param.u32 	%r9, [float_sum_N_param_2];
	ld.param.u32 	%r10, [float_sum_N_param_3];
	ld.param.u64 	%rd2, [float_sum_N_param_4];
	mov.b32	%r11, %envreg3;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %ntid.x;
	mad.lo.s32 	%r1, %r12, %r13, %r11;
	mov.u32 	%r2, %tid.x;
	setp.eq.s32	%p1, %r10, 0;
	mov.f64 	%fd7, 0d0000000000000000;
	@%p1 bra 	BB7_3;

	add.s32 	%r15, %r1, %r2;
	mad.lo.s32 	%r18, %r10, %r15, %r8;
	mov.u32 	%r19, 0;
	mov.f64 	%fd7, 0d0000000000000000;

BB7_2:
	mul.wide.u32 	%rd3, %r18, 4;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.f32 	%f1, [%rd4];
	cvt.f64.f32	%fd6, %f1;
	add.f64 	%fd7, %fd7, %fd6;
	add.s32 	%r18, %r18, 1;
	add.s32 	%r19, %r19, 1;
	setp.lt.u32	%p2, %r19, %r10;
	@%p2 bra 	BB7_2;

BB7_3:
	add.s32 	%r16, %r1, %r2;
	add.s32 	%r17, %r16, %r9;
	mul.wide.u32 	%rd5, %r17, 8;
	add.s64 	%rd6, %rd2, %rd5;
	st.global.f64 	[%rd6], %fd7;
	ret;
}

	// .globl	float_sum2d_16
.entry float_sum2d_16(
	.param .u64 .ptr .global .align 4 float_sum2d_16_param_0,
	.param .u32 float_sum2d_16_param_1,
	.param .u64 .ptr .global .align 8 float_sum2d_16_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<17>;
	.reg .b32 	%r<70>;
	.reg .f64 	%fd<33>;
	.reg .b64 	%rd<37>;


	ld.param.u64 	%rd1, [float_sum2d_16_param_0];
	ld.param.u32 	%r28, [float_sum2d_16_param_1];
	ld.param.u64 	%rd2, [float_sum2d_16_param_2];
	setp.eq.s32	%p1, %r28, 0;
	@%p1 bra 	BB8_3;

	mov.b32	%r30, %envreg3;
	mov.u32 	%r31, %ntid.x;
	mov.u32 	%r32, %ctaid.x;
	mov.u32 	%r33, %tid.x;
	add.s32 	%r34, %r30, %r33;
	mad.lo.s32 	%r35, %r32, %r31, %r34;
	shl.b32 	%r36, %r35, 4;
	add.s32 	%r37, %r36, 15;
	mul.lo.s32 	%r68, %r28, %r37;
	add.s32 	%r38, %r36, 14;
	mul.lo.s32 	%r67, %r28, %r38;
	add.s32 	%r39, %r36, 13;
	mul.lo.s32 	%r66, %r28, %r39;
	add.s32 	%r40, %r36, 12;
	mul.lo.s32 	%r65, %r28, %r40;
	add.s32 	%r41, %r36, 11;
	mul.lo.s32 	%r5, %r28, %r41;
	add.s32 	%r42, %r36, 10;
	mul.lo.s32 	%r6, %r28, %r42;
	add.s32 	%r43, %r36, 9;
	mul.lo.s32 	%r7, %r28, %r43;
	add.s32 	%r44, %r36, 8;
	mul.lo.s32 	%r8, %r28, %r44;
	add.s32 	%r45, %r36, 7;
	mul.lo.s32 	%r9, %r28, %r45;
	add.s32 	%r46, %r36, 6;
	mul.lo.s32 	%r10, %r28, %r46;
	add.s32 	%r47, %r36, 5;
	mul.lo.s32 	%r11, %r28, %r47;
	add.s32 	%r48, %r36, 4;
	mul.lo.s32 	%r12, %r28, %r48;
	add.s32 	%r49, %r36, 3;
	mul.lo.s32 	%r13, %r28, %r49;
	add.s32 	%r50, %r36, 2;
	mul.lo.s32 	%r14, %r28, %r50;
	add.s32 	%r51, %r36, 1;
	mul.lo.s32 	%r15, %r28, %r51;
	mul.lo.s32 	%r16, %r28, %r35;
	shl.b32 	%r17, %r16, 4;
	mov.u32 	%r69, 0;

BB8_2:
	add.s32 	%r52, %r17, %r69;
	mul.wide.u32 	%rd3, %r52, 4;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.f32 	%f1, [%rd4];
	cvt.f64.f32	%fd1, %f1;
	add.f64 	%fd2, %fd1, 0d0000000000000000;
	add.s32 	%r53, %r15, %r69;
	mul.wide.u32 	%rd5, %r53, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.f32 	%f2, [%rd6];
	cvt.f64.f32	%fd3, %f2;
	add.f64 	%fd4, %fd2, %fd3;
	add.s32 	%r54, %r14, %r69;
	mul.wide.u32 	%rd7, %r54, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.f32 	%f3, [%rd8];
	cvt.f64.f32	%fd5, %f3;
	add.f64 	%fd6, %fd4, %fd5;
	add.s32 	%r55, %r13, %r69;
	mul.wide.u32 	%rd9, %r55, 4;
	add.s64 	%rd10, %rd1, %rd9;
	ld.global.f32 	%f4, [%rd10];
	cvt.f64.f32	%fd7, %f4;
	add.f64 	%fd8, %fd6, %fd7;
	add.s32 	%r56, %r12, %r69;
	mul.wide.u32 	%rd11, %r56, 4;
	add.s64 	%rd12, %rd1, %rd11;
	ld.global.f32 	%f5, [%rd12];
	cvt.f64.f32	%fd9, %f5;
	add.f64 	%fd10, %fd8, %fd9;
	add.s32 	%r57, %r11, %r69;
	mul.wide.u32 	%rd13, %r57, 4;
	add.s64 	%rd14, %rd1, %rd13;
	ld.global.f32 	%f6, [%rd14];
	cvt.f64.f32	%fd11, %f6;
	add.f64 	%fd12, %fd10, %fd11;
	add.s32 	%r58, %r10, %r69;
	mul.wide.u32 	%rd15, %r58, 4;
	add.s64 	%rd16, %rd1, %rd15;
	ld.global.f32 	%f7, [%rd16];
	cvt.f64.f32	%fd13, %f7;
	add.f64 	%fd14, %fd12, %fd13;
	add.s32 	%r59, %r9, %r69;
	mul.wide.u32 	%rd17, %r59, 4;
	add.s64 	%rd18, %rd1, %rd17;
	ld.global.f32 	%f8, [%rd18];
	cvt.f64.f32	%fd15, %f8;
	add.f64 	%fd16, %fd14, %fd15;
	add.s32 	%r60, %r8, %r69;
	mul.wide.u32 	%rd19, %r60, 4;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.f32 	%f9, [%rd20];
	cvt.f64.f32	%fd17, %f9;
	add.f64 	%fd18, %fd16, %fd17;
	add.s32 	%r61, %r7, %r69;
	mul.wide.u32 	%rd21, %r61, 4;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.f32 	%f10, [%rd22];
	cvt.f64.f32	%fd19, %f10;
	add.f64 	%fd20, %fd18, %fd19;
	add.s32 	%r62, %r6, %r69;
	mul.wide.u32 	%rd23, %r62, 4;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.f32 	%f11, [%rd24];
	cvt.f64.f32	%fd21, %f11;
	add.f64 	%fd22, %fd20, %fd21;
	add.s32 	%r63, %r5, %r69;
	mul.wide.u32 	%rd25, %r63, 4;
	add.s64 	%rd26, %rd1, %rd25;
	ld.global.f32 	%f12, [%rd26];
	cvt.f64.f32	%fd23, %f12;
	add.f64 	%fd24, %fd22, %fd23;
	mul.wide.u32 	%rd27, %r65, 4;
	add.s64 	%rd28, %rd1, %rd27;
	ld.global.f32 	%f13, [%rd28];
	cvt.f64.f32	%fd25, %f13;
	add.f64 	%fd26, %fd24, %fd25;
	mul.wide.u32 	%rd29, %r66, 4;
	add.s64 	%rd30, %rd1, %rd29;
	ld.global.f32 	%f14, [%rd30];
	cvt.f64.f32	%fd27, %f14;
	add.f64 	%fd28, %fd26, %fd27;
	mul.wide.u32 	%rd31, %r67, 4;
	add.s64 	%rd32, %rd1, %rd31;
	ld.global.f32 	%f15, [%rd32];
	cvt.f64.f32	%fd29, %f15;
	add.f64 	%fd30, %fd28, %fd29;
	mul.wide.u32 	%rd33, %r68, 4;
	add.s64 	%rd34, %rd1, %rd33;
	ld.global.f32 	%f16, [%rd34];
	cvt.f64.f32	%fd31, %f16;
	add.f64 	%fd32, %fd30, %fd31;
	add.s32 	%r64, %r16, %r69;
	mul.wide.u32 	%rd35, %r64, 8;
	add.s64 	%rd36, %rd2, %rd35;
	st.global.f64 	[%rd36], %fd32;
	add.s32 	%r68, %r68, 1;
	add.s32 	%r67, %r67, 1;
	add.s32 	%r66, %r66, 1;
	add.s32 	%r65, %r65, 1;
	add.s32 	%r69, %r69, 1;
	setp.lt.u32	%p2, %r69, %r28;
	@%p2 bra 	BB8_2;

BB8_3:
	ret;
}

	// .globl	float_sum2d_N
.entry float_sum2d_N(
	.param .u64 .ptr .global .align 4 float_sum2d_N_param_0,
	.param .u32 float_sum2d_N_param_1,
	.param .u32 float_sum2d_N_param_2,
	.param .u32 float_sum2d_N_param_3,
	.param .u32 float_sum2d_N_param_4,
	.param .u64 .ptr .global .align 8 float_sum2d_N_param_5
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<28>;
	.reg .f64 	%fd<10>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [float_sum2d_N_param_0];
	ld.param.u32 	%r10, [float_sum2d_N_param_1];
	ld.param.u32 	%r11, [float_sum2d_N_param_2];
	ld.param.u32 	%r12, [float_sum2d_N_param_3];
	ld.param.u32 	%r13, [float_sum2d_N_param_4];
	ld.param.u64 	%rd2, [float_sum2d_N_param_5];
	setp.eq.s32	%p1, %r10, 0;
	@%p1 bra 	BB9_5;

	mov.b32	%r15, %envreg3;
	mov.u32 	%r16, %ctaid.x;
	mov.u32 	%r17, %ntid.x;
	mad.lo.s32 	%r18, %r16, %r17, %r15;
	mov.u32 	%r19, %tid.x;
	add.s32 	%r20, %r18, %r19;
	mul.lo.s32 	%r21, %r20, %r10;
	add.s32 	%r1, %r21, %r12;
	mad.lo.s32 	%r2, %r21, %r13, %r11;
	mov.u32 	%r14, 0;
	mov.u32 	%r27, %r14;

BB9_2:
	add.s32 	%r24, %r2, %r27;
	setp.eq.s32	%p2, %r13, 0;
	mov.f64 	%fd9, 0d0000000000000000;
	mov.f64 	%fd8, %fd9;
	mov.u32 	%r26, %r14;
	@%p2 bra 	BB9_4;

BB9_3:
	mov.u32 	%r6, %r26;
	mul.wide.u32 	%rd3, %r24, 4;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.f32 	%f1, [%rd4];
	cvt.f64.f32	%fd6, %f1;
	add.f64 	%fd9, %fd9, %fd6;
	add.s32 	%r24, %r24, %r10;
	add.s32 	%r8, %r6, 1;
	setp.lt.u32	%p3, %r8, %r13;
	mov.u32 	%r26, %r8;
	mov.f64 	%fd8, %fd9;
	@%p3 bra 	BB9_3;

BB9_4:
	add.s32 	%r23, %r1, %r27;
	mul.wide.u32 	%rd5, %r23, 8;
	add.s64 	%rd6, %rd2, %rd5;
	st.global.f64 	[%rd6], %fd8;
	add.s32 	%r27, %r27, 1;
	setp.lt.u32	%p4, %r27, %r10;
	@%p4 bra 	BB9_2;

BB9_5:
	ret;
}

	// .globl	float_sum2d_16_weighted
.entry float_sum2d_16_weighted(
	.param .u64 .ptr .global .align 16 float_sum2d_16_weighted_param_0,
	.param .u64 .ptr .global .align 4 float_sum2d_16_weighted_param_1,
	.param .u32 float_sum2d_16_weighted_param_2,
	.param .u64 .ptr .global .align 8 float_sum2d_16_weighted_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<17>;
	.reg .b32 	%r<70>;
	.reg .f64 	%fd<33>;
	.reg .b64 	%rd<37>;


	ld.param.u64 	%rd1, [float_sum2d_16_weighted_param_1];
	ld.param.u32 	%r28, [float_sum2d_16_weighted_param_2];
	ld.param.u64 	%rd2, [float_sum2d_16_weighted_param_3];
	setp.eq.s32	%p1, %r28, 0;
	@%p1 bra 	BB10_3;

	mov.b32	%r30, %envreg3;
	mov.u32 	%r31, %ntid.x;
	mov.u32 	%r32, %ctaid.x;
	mov.u32 	%r33, %tid.x;
	add.s32 	%r34, %r30, %r33;
	mad.lo.s32 	%r35, %r32, %r31, %r34;
	shl.b32 	%r36, %r35, 4;
	add.s32 	%r37, %r36, 15;
	mul.lo.s32 	%r68, %r28, %r37;
	add.s32 	%r38, %r36, 14;
	mul.lo.s32 	%r67, %r28, %r38;
	add.s32 	%r39, %r36, 13;
	mul.lo.s32 	%r66, %r28, %r39;
	add.s32 	%r40, %r36, 12;
	mul.lo.s32 	%r65, %r28, %r40;
	add.s32 	%r41, %r36, 11;
	mul.lo.s32 	%r5, %r28, %r41;
	add.s32 	%r42, %r36, 10;
	mul.lo.s32 	%r6, %r28, %r42;
	add.s32 	%r43, %r36, 9;
	mul.lo.s32 	%r7, %r28, %r43;
	add.s32 	%r44, %r36, 8;
	mul.lo.s32 	%r8, %r28, %r44;
	add.s32 	%r45, %r36, 7;
	mul.lo.s32 	%r9, %r28, %r45;
	add.s32 	%r46, %r36, 6;
	mul.lo.s32 	%r10, %r28, %r46;
	add.s32 	%r47, %r36, 5;
	mul.lo.s32 	%r11, %r28, %r47;
	add.s32 	%r48, %r36, 4;
	mul.lo.s32 	%r12, %r28, %r48;
	add.s32 	%r49, %r36, 3;
	mul.lo.s32 	%r13, %r28, %r49;
	add.s32 	%r50, %r36, 2;
	mul.lo.s32 	%r14, %r28, %r50;
	add.s32 	%r51, %r36, 1;
	mul.lo.s32 	%r15, %r28, %r51;
	mul.lo.s32 	%r16, %r28, %r35;
	shl.b32 	%r17, %r16, 4;
	mov.u32 	%r69, 0;

BB10_2:
	add.s32 	%r52, %r17, %r69;
	mul.wide.u32 	%rd3, %r52, 4;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.f32 	%f1, [%rd4];
	cvt.f64.f32	%fd1, %f1;
	add.f64 	%fd2, %fd1, 0d0000000000000000;
	add.s32 	%r53, %r15, %r69;
	mul.wide.u32 	%rd5, %r53, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.f32 	%f2, [%rd6];
	cvt.f64.f32	%fd3, %f2;
	add.f64 	%fd4, %fd2, %fd3;
	add.s32 	%r54, %r14, %r69;
	mul.wide.u32 	%rd7, %r54, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.f32 	%f3, [%rd8];
	cvt.f64.f32	%fd5, %f3;
	add.f64 	%fd6, %fd4, %fd5;
	add.s32 	%r55, %r13, %r69;
	mul.wide.u32 	%rd9, %r55, 4;
	add.s64 	%rd10, %rd1, %rd9;
	ld.global.f32 	%f4, [%rd10];
	cvt.f64.f32	%fd7, %f4;
	add.f64 	%fd8, %fd6, %fd7;
	add.s32 	%r56, %r12, %r69;
	mul.wide.u32 	%rd11, %r56, 4;
	add.s64 	%rd12, %rd1, %rd11;
	ld.global.f32 	%f5, [%rd12];
	cvt.f64.f32	%fd9, %f5;
	add.f64 	%fd10, %fd8, %fd9;
	add.s32 	%r57, %r11, %r69;
	mul.wide.u32 	%rd13, %r57, 4;
	add.s64 	%rd14, %rd1, %rd13;
	ld.global.f32 	%f6, [%rd14];
	cvt.f64.f32	%fd11, %f6;
	add.f64 	%fd12, %fd10, %fd11;
	add.s32 	%r58, %r10, %r69;
	mul.wide.u32 	%rd15, %r58, 4;
	add.s64 	%rd16, %rd1, %rd15;
	ld.global.f32 	%f7, [%rd16];
	cvt.f64.f32	%fd13, %f7;
	add.f64 	%fd14, %fd12, %fd13;
	add.s32 	%r59, %r9, %r69;
	mul.wide.u32 	%rd17, %r59, 4;
	add.s64 	%rd18, %rd1, %rd17;
	ld.global.f32 	%f8, [%rd18];
	cvt.f64.f32	%fd15, %f8;
	add.f64 	%fd16, %fd14, %fd15;
	add.s32 	%r60, %r8, %r69;
	mul.wide.u32 	%rd19, %r60, 4;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.f32 	%f9, [%rd20];
	cvt.f64.f32	%fd17, %f9;
	add.f64 	%fd18, %fd16, %fd17;
	add.s32 	%r61, %r7, %r69;
	mul.wide.u32 	%rd21, %r61, 4;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.f32 	%f10, [%rd22];
	cvt.f64.f32	%fd19, %f10;
	add.f64 	%fd20, %fd18, %fd19;
	add.s32 	%r62, %r6, %r69;
	mul.wide.u32 	%rd23, %r62, 4;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.f32 	%f11, [%rd24];
	cvt.f64.f32	%fd21, %f11;
	add.f64 	%fd22, %fd20, %fd21;
	add.s32 	%r63, %r5, %r69;
	mul.wide.u32 	%rd25, %r63, 4;
	add.s64 	%rd26, %rd1, %rd25;
	ld.global.f32 	%f12, [%rd26];
	cvt.f64.f32	%fd23, %f12;
	add.f64 	%fd24, %fd22, %fd23;
	mul.wide.u32 	%rd27, %r65, 4;
	add.s64 	%rd28, %rd1, %rd27;
	ld.global.f32 	%f13, [%rd28];
	cvt.f64.f32	%fd25, %f13;
	add.f64 	%fd26, %fd24, %fd25;
	mul.wide.u32 	%rd29, %r66, 4;
	add.s64 	%rd30, %rd1, %rd29;
	ld.global.f32 	%f14, [%rd30];
	cvt.f64.f32	%fd27, %f14;
	add.f64 	%fd28, %fd26, %fd27;
	mul.wide.u32 	%rd31, %r67, 4;
	add.s64 	%rd32, %rd1, %rd31;
	ld.global.f32 	%f15, [%rd32];
	cvt.f64.f32	%fd29, %f15;
	add.f64 	%fd30, %fd28, %fd29;
	mul.wide.u32 	%rd33, %r68, 4;
	add.s64 	%rd34, %rd1, %rd33;
	ld.global.f32 	%f16, [%rd34];
	cvt.f64.f32	%fd31, %f16;
	add.f64 	%fd32, %fd30, %fd31;
	add.s32 	%r64, %r16, %r69;
	mul.wide.u32 	%rd35, %r64, 8;
	add.s64 	%rd36, %rd2, %rd35;
	st.global.f64 	[%rd36], %fd32;
	add.s32 	%r68, %r68, 1;
	add.s32 	%r67, %r67, 1;
	add.s32 	%r66, %r66, 1;
	add.s32 	%r65, %r65, 1;
	add.s32 	%r69, %r69, 1;
	setp.lt.u32	%p2, %r69, %r28;
	@%p2 bra 	BB10_2;

BB10_3:
	ret;
}

	// .globl	float_sum2d_N_weighted
.entry float_sum2d_N_weighted(
	.param .u64 .ptr .global .align 16 float_sum2d_N_weighted_param_0,
	.param .u64 .ptr .global .align 4 float_sum2d_N_weighted_param_1,
	.param .u32 float_sum2d_N_weighted_param_2,
	.param .u32 float_sum2d_N_weighted_param_3,
	.param .u32 float_sum2d_N_weighted_param_4,
	.param .u32 float_sum2d_N_weighted_param_5,
	.param .u64 .ptr .global .align 8 float_sum2d_N_weighted_param_6
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<28>;
	.reg .f64 	%fd<10>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [float_sum2d_N_weighted_param_1];
	ld.param.u32 	%r10, [float_sum2d_N_weighted_param_2];
	ld.param.u32 	%r11, [float_sum2d_N_weighted_param_3];
	ld.param.u32 	%r12, [float_sum2d_N_weighted_param_4];
	ld.param.u32 	%r13, [float_sum2d_N_weighted_param_5];
	ld.param.u64 	%rd2, [float_sum2d_N_weighted_param_6];
	setp.eq.s32	%p1, %r10, 0;
	@%p1 bra 	BB11_5;

	mov.b32	%r15, %envreg3;
	mov.u32 	%r16, %ctaid.x;
	mov.u32 	%r17, %ntid.x;
	mad.lo.s32 	%r18, %r16, %r17, %r15;
	mov.u32 	%r19, %tid.x;
	add.s32 	%r20, %r18, %r19;
	mul.lo.s32 	%r21, %r20, %r10;
	add.s32 	%r1, %r21, %r12;
	mad.lo.s32 	%r2, %r21, %r13, %r11;
	mov.u32 	%r14, 0;
	mov.u32 	%r27, %r14;

BB11_2:
	add.s32 	%r24, %r2, %r27;
	setp.eq.s32	%p2, %r13, 0;
	mov.f64 	%fd9, 0d0000000000000000;
	mov.f64 	%fd8, %fd9;
	mov.u32 	%r26, %r14;
	@%p2 bra 	BB11_4;

BB11_3:
	mov.u32 	%r6, %r26;
	mul.wide.u32 	%rd3, %r24, 4;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.f32 	%f1, [%rd4];
	cvt.f64.f32	%fd6, %f1;
	add.f64 	%fd9, %fd9, %fd6;
	add.s32 	%r24, %r24, %r10;
	add.s32 	%r8, %r6, 1;
	setp.lt.u32	%p3, %r8, %r13;
	mov.u32 	%r26, %r8;
	mov.f64 	%fd8, %fd9;
	@%p3 bra 	BB11_3;

BB11_4:
	add.s32 	%r23, %r1, %r27;
	mul.wide.u32 	%rd5, %r23, 8;
	add.s64 	%rd6, %rd2, %rd5;
	st.global.f64 	[%rd6], %fd8;
	add.s32 	%r27, %r27, 1;
	setp.lt.u32	%p4, %r27, %r10;
	@%p4 bra 	BB11_2;

BB11_5:
	ret;
}

	// .globl	double_sum_16
.entry double_sum_16(
	.param .u64 .ptr .global .align 8 double_sum_16_param_0,
	.param .u64 .ptr .global .align 8 double_sum_16_param_1
)
{
	.reg .b32 	%r<23>;
	.reg .f64 	%fd<33>;
	.reg .b64 	%rd<37>;


	ld.param.u64 	%rd1, [double_sum_16_param_0];
	ld.param.u64 	%rd2, [double_sum_16_param_1];
	mov.b32	%r1, %envreg3;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mad.lo.s32 	%r4, %r3, %r2, %r1;
	mov.u32 	%r5, %tid.x;
	add.s32 	%r6, %r4, %r5;
	shl.b32 	%r7, %r6, 4;
	mul.wide.u32 	%rd3, %r7, 8;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.f64 	%fd1, [%rd4];
	add.f64 	%fd2, %fd1, 0d0000000000000000;
	add.s32 	%r8, %r7, 1;
	mul.wide.u32 	%rd5, %r8, 8;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.f64 	%fd3, [%rd6];
	add.f64 	%fd4, %fd2, %fd3;
	add.s32 	%r9, %r7, 2;
	mul.wide.u32 	%rd7, %r9, 8;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.f64 	%fd5, [%rd8];
	add.f64 	%fd6, %fd4, %fd5;
	add.s32 	%r10, %r7, 3;
	mul.wide.u32 	%rd9, %r10, 8;
	add.s64 	%rd10, %rd1, %rd9;
	ld.global.f64 	%fd7, [%rd10];
	add.f64 	%fd8, %fd6, %fd7;
	add.s32 	%r11, %r7, 4;
	mul.wide.u32 	%rd11, %r11, 8;
	add.s64 	%rd12, %rd1, %rd11;
	ld.global.f64 	%fd9, [%rd12];
	add.f64 	%fd10, %fd8, %fd9;
	add.s32 	%r12, %r7, 5;
	mul.wide.u32 	%rd13, %r12, 8;
	add.s64 	%rd14, %rd1, %rd13;
	ld.global.f64 	%fd11, [%rd14];
	add.f64 	%fd12, %fd10, %fd11;
	add.s32 	%r13, %r7, 6;
	mul.wide.u32 	%rd15, %r13, 8;
	add.s64 	%rd16, %rd1, %rd15;
	ld.global.f64 	%fd13, [%rd16];
	add.f64 	%fd14, %fd12, %fd13;
	add.s32 	%r14, %r7, 7;
	mul.wide.u32 	%rd17, %r14, 8;
	add.s64 	%rd18, %rd1, %rd17;
	ld.global.f64 	%fd15, [%rd18];
	add.f64 	%fd16, %fd14, %fd15;
	add.s32 	%r15, %r7, 8;
	mul.wide.u32 	%rd19, %r15, 8;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.f64 	%fd17, [%rd20];
	add.f64 	%fd18, %fd16, %fd17;
	add.s32 	%r16, %r7, 9;
	mul.wide.u32 	%rd21, %r16, 8;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.f64 	%fd19, [%rd22];
	add.f64 	%fd20, %fd18, %fd19;
	add.s32 	%r17, %r7, 10;
	mul.wide.u32 	%rd23, %r17, 8;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.f64 	%fd21, [%rd24];
	add.f64 	%fd22, %fd20, %fd21;
	add.s32 	%r18, %r7, 11;
	mul.wide.u32 	%rd25, %r18, 8;
	add.s64 	%rd26, %rd1, %rd25;
	ld.global.f64 	%fd23, [%rd26];
	add.f64 	%fd24, %fd22, %fd23;
	add.s32 	%r19, %r7, 12;
	mul.wide.u32 	%rd27, %r19, 8;
	add.s64 	%rd28, %rd1, %rd27;
	ld.global.f64 	%fd25, [%rd28];
	add.f64 	%fd26, %fd24, %fd25;
	add.s32 	%r20, %r7, 13;
	mul.wide.u32 	%rd29, %r20, 8;
	add.s64 	%rd30, %rd1, %rd29;
	ld.global.f64 	%fd27, [%rd30];
	add.f64 	%fd28, %fd26, %fd27;
	add.s32 	%r21, %r7, 14;
	mul.wide.u32 	%rd31, %r21, 8;
	add.s64 	%rd32, %rd1, %rd31;
	ld.global.f64 	%fd29, [%rd32];
	add.f64 	%fd30, %fd28, %fd29;
	add.s32 	%r22, %r7, 15;
	mul.wide.u32 	%rd33, %r22, 8;
	add.s64 	%rd34, %rd1, %rd33;
	ld.global.f64 	%fd31, [%rd34];
	add.f64 	%fd32, %fd30, %fd31;
	mul.wide.u32 	%rd35, %r6, 8;
	add.s64 	%rd36, %rd2, %rd35;
	st.global.f64 	[%rd36], %fd32;
	ret;
}

	// .globl	double_sum_N
.entry double_sum_N(
	.param .u64 .ptr .global .align 8 double_sum_N_param_0,
	.param .u32 double_sum_N_param_1,
	.param .u32 double_sum_N_param_2,
	.param .u32 double_sum_N_param_3,
	.param .u64 .ptr .global .align 8 double_sum_N_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<20>;
	.reg .f64 	%fd<8>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [double_sum_N_param_0];
	ld.param.u32 	%r8, [double_sum_N_param_1];
	ld.param.u32 	%r9, [double_sum_N_param_2];
	ld.param.u32 	%r10, [double_sum_N_param_3];
	ld.param.u64 	%rd2, [double_sum_N_param_4];
	mov.b32	%r11, %envreg3;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %ntid.x;
	mad.lo.s32 	%r1, %r12, %r13, %r11;
	mov.u32 	%r2, %tid.x;
	setp.eq.s32	%p1, %r10, 0;
	mov.f64 	%fd7, 0d0000000000000000;
	@%p1 bra 	BB13_3;

	add.s32 	%r15, %r1, %r2;
	mad.lo.s32 	%r18, %r10, %r15, %r8;
	mov.u32 	%r19, 0;
	mov.f64 	%fd7, 0d0000000000000000;

BB13_2:
	mul.wide.u32 	%rd3, %r18, 8;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.f64 	%fd6, [%rd4];
	add.f64 	%fd7, %fd7, %fd6;
	add.s32 	%r18, %r18, 1;
	add.s32 	%r19, %r19, 1;
	setp.lt.u32	%p2, %r19, %r10;
	@%p2 bra 	BB13_2;

BB13_3:
	add.s32 	%r16, %r1, %r2;
	add.s32 	%r17, %r16, %r9;
	mul.wide.u32 	%rd5, %r17, 8;
	add.s64 	%rd6, %rd2, %rd5;
	st.global.f64 	[%rd6], %fd7;
	ret;
}

	// .globl	double_sum2d_16
.entry double_sum2d_16(
	.param .u64 .ptr .global .align 8 double_sum2d_16_param_0,
	.param .u32 double_sum2d_16_param_1,
	.param .u64 .ptr .global .align 8 double_sum2d_16_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<70>;
	.reg .f64 	%fd<33>;
	.reg .b64 	%rd<37>;


	ld.param.u64 	%rd1, [double_sum2d_16_param_0];
	ld.param.u32 	%r28, [double_sum2d_16_param_1];
	ld.param.u64 	%rd2, [double_sum2d_16_param_2];
	setp.eq.s32	%p1, %r28, 0;
	@%p1 bra 	BB14_3;

	mov.b32	%r30, %envreg3;
	mov.u32 	%r31, %ntid.x;
	mov.u32 	%r32, %ctaid.x;
	mov.u32 	%r33, %tid.x;
	add.s32 	%r34, %r30, %r33;
	mad.lo.s32 	%r35, %r32, %r31, %r34;
	shl.b32 	%r36, %r35, 4;
	add.s32 	%r37, %r36, 15;
	mul.lo.s32 	%r68, %r28, %r37;
	add.s32 	%r38, %r36, 14;
	mul.lo.s32 	%r67, %r28, %r38;
	add.s32 	%r39, %r36, 13;
	mul.lo.s32 	%r66, %r28, %r39;
	add.s32 	%r40, %r36, 12;
	mul.lo.s32 	%r65, %r28, %r40;
	add.s32 	%r41, %r36, 11;
	mul.lo.s32 	%r5, %r28, %r41;
	add.s32 	%r42, %r36, 10;
	mul.lo.s32 	%r6, %r28, %r42;
	add.s32 	%r43, %r36, 9;
	mul.lo.s32 	%r7, %r28, %r43;
	add.s32 	%r44, %r36, 8;
	mul.lo.s32 	%r8, %r28, %r44;
	add.s32 	%r45, %r36, 7;
	mul.lo.s32 	%r9, %r28, %r45;
	add.s32 	%r46, %r36, 6;
	mul.lo.s32 	%r10, %r28, %r46;
	add.s32 	%r47, %r36, 5;
	mul.lo.s32 	%r11, %r28, %r47;
	add.s32 	%r48, %r36, 4;
	mul.lo.s32 	%r12, %r28, %r48;
	add.s32 	%r49, %r36, 3;
	mul.lo.s32 	%r13, %r28, %r49;
	add.s32 	%r50, %r36, 2;
	mul.lo.s32 	%r14, %r28, %r50;
	add.s32 	%r51, %r36, 1;
	mul.lo.s32 	%r15, %r28, %r51;
	mul.lo.s32 	%r16, %r28, %r35;
	shl.b32 	%r17, %r16, 4;
	mov.u32 	%r69, 0;

BB14_2:
	add.s32 	%r52, %r17, %r69;
	mul.wide.u32 	%rd3, %r52, 8;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.f64 	%fd1, [%rd4];
	add.f64 	%fd2, %fd1, 0d0000000000000000;
	add.s32 	%r53, %r15, %r69;
	mul.wide.u32 	%rd5, %r53, 8;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.f64 	%fd3, [%rd6];
	add.f64 	%fd4, %fd2, %fd3;
	add.s32 	%r54, %r14, %r69;
	mul.wide.u32 	%rd7, %r54, 8;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.f64 	%fd5, [%rd8];
	add.f64 	%fd6, %fd4, %fd5;
	add.s32 	%r55, %r13, %r69;
	mul.wide.u32 	%rd9, %r55, 8;
	add.s64 	%rd10, %rd1, %rd9;
	ld.global.f64 	%fd7, [%rd10];
	add.f64 	%fd8, %fd6, %fd7;
	add.s32 	%r56, %r12, %r69;
	mul.wide.u32 	%rd11, %r56, 8;
	add.s64 	%rd12, %rd1, %rd11;
	ld.global.f64 	%fd9, [%rd12];
	add.f64 	%fd10, %fd8, %fd9;
	add.s32 	%r57, %r11, %r69;
	mul.wide.u32 	%rd13, %r57, 8;
	add.s64 	%rd14, %rd1, %rd13;
	ld.global.f64 	%fd11, [%rd14];
	add.f64 	%fd12, %fd10, %fd11;
	add.s32 	%r58, %r10, %r69;
	mul.wide.u32 	%rd15, %r58, 8;
	add.s64 	%rd16, %rd1, %rd15;
	ld.global.f64 	%fd13, [%rd16];
	add.f64 	%fd14, %fd12, %fd13;
	add.s32 	%r59, %r9, %r69;
	mul.wide.u32 	%rd17, %r59, 8;
	add.s64 	%rd18, %rd1, %rd17;
	ld.global.f64 	%fd15, [%rd18];
	add.f64 	%fd16, %fd14, %fd15;
	add.s32 	%r60, %r8, %r69;
	mul.wide.u32 	%rd19, %r60, 8;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.f64 	%fd17, [%rd20];
	add.f64 	%fd18, %fd16, %fd17;
	add.s32 	%r61, %r7, %r69;
	mul.wide.u32 	%rd21, %r61, 8;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.f64 	%fd19, [%rd22];
	add.f64 	%fd20, %fd18, %fd19;
	add.s32 	%r62, %r6, %r69;
	mul.wide.u32 	%rd23, %r62, 8;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.f64 	%fd21, [%rd24];
	add.f64 	%fd22, %fd20, %fd21;
	add.s32 	%r63, %r5, %r69;
	mul.wide.u32 	%rd25, %r63, 8;
	add.s64 	%rd26, %rd1, %rd25;
	ld.global.f64 	%fd23, [%rd26];
	add.f64 	%fd24, %fd22, %fd23;
	mul.wide.u32 	%rd27, %r65, 8;
	add.s64 	%rd28, %rd1, %rd27;
	ld.global.f64 	%fd25, [%rd28];
	add.f64 	%fd26, %fd24, %fd25;
	mul.wide.u32 	%rd29, %r66, 8;
	add.s64 	%rd30, %rd1, %rd29;
	ld.global.f64 	%fd27, [%rd30];
	add.f64 	%fd28, %fd26, %fd27;
	mul.wide.u32 	%rd31, %r67, 8;
	add.s64 	%rd32, %rd1, %rd31;
	ld.global.f64 	%fd29, [%rd32];
	add.f64 	%fd30, %fd28, %fd29;
	mul.wide.u32 	%rd33, %r68, 8;
	add.s64 	%rd34, %rd1, %rd33;
	ld.global.f64 	%fd31, [%rd34];
	add.f64 	%fd32, %fd30, %fd31;
	add.s32 	%r64, %r16, %r69;
	mul.wide.u32 	%rd35, %r64, 8;
	add.s64 	%rd36, %rd2, %rd35;
	st.global.f64 	[%rd36], %fd32;
	add.s32 	%r68, %r68, 1;
	add.s32 	%r67, %r67, 1;
	add.s32 	%r66, %r66, 1;
	add.s32 	%r65, %r65, 1;
	add.s32 	%r69, %r69, 1;
	setp.lt.u32	%p2, %r69, %r28;
	@%p2 bra 	BB14_2;

BB14_3:
	ret;
}

	// .globl	double_sum2d_N
.entry double_sum2d_N(
	.param .u64 .ptr .global .align 8 double_sum2d_N_param_0,
	.param .u32 double_sum2d_N_param_1,
	.param .u32 double_sum2d_N_param_2,
	.param .u32 double_sum2d_N_param_3,
	.param .u32 double_sum2d_N_param_4,
	.param .u64 .ptr .global .align 8 double_sum2d_N_param_5
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<28>;
	.reg .f64 	%fd<10>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [double_sum2d_N_param_0];
	ld.param.u32 	%r10, [double_sum2d_N_param_1];
	ld.param.u32 	%r11, [double_sum2d_N_param_2];
	ld.param.u32 	%r12, [double_sum2d_N_param_3];
	ld.param.u32 	%r13, [double_sum2d_N_param_4];
	ld.param.u64 	%rd2, [double_sum2d_N_param_5];
	setp.eq.s32	%p1, %r10, 0;
	@%p1 bra 	BB15_5;

	mov.b32	%r15, %envreg3;
	mov.u32 	%r16, %ctaid.x;
	mov.u32 	%r17, %ntid.x;
	mad.lo.s32 	%r18, %r16, %r17, %r15;
	mov.u32 	%r19, %tid.x;
	add.s32 	%r20, %r18, %r19;
	mul.lo.s32 	%r21, %r20, %r10;
	add.s32 	%r1, %r21, %r12;
	mad.lo.s32 	%r2, %r21, %r13, %r11;
	mov.u32 	%r14, 0;
	mov.u32 	%r27, %r14;

BB15_2:
	add.s32 	%r24, %r2, %r27;
	setp.eq.s32	%p2, %r13, 0;
	mov.f64 	%fd9, 0d0000000000000000;
	mov.f64 	%fd8, %fd9;
	mov.u32 	%r26, %r14;
	@%p2 bra 	BB15_4;

BB15_3:
	mov.u32 	%r6, %r26;
	mul.wide.u32 	%rd3, %r24, 8;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.f64 	%fd6, [%rd4];
	add.f64 	%fd9, %fd9, %fd6;
	add.s32 	%r24, %r24, %r10;
	add.s32 	%r8, %r6, 1;
	setp.lt.u32	%p3, %r8, %r13;
	mov.u32 	%r26, %r8;
	mov.f64 	%fd8, %fd9;
	@%p3 bra 	BB15_3;

BB15_4:
	add.s32 	%r23, %r1, %r27;
	mul.wide.u32 	%rd5, %r23, 8;
	add.s64 	%rd6, %rd2, %rd5;
	st.global.f64 	[%rd6], %fd8;
	add.s32 	%r27, %r27, 1;
	setp.lt.u32	%p4, %r27, %r10;
	@%p4 bra 	BB15_2;

BB15_5:
	ret;
}

	// .globl	double_sum2d_16_weighted
.entry double_sum2d_16_weighted(
	.param .u64 .ptr .global .align 16 double_sum2d_16_weighted_param_0,
	.param .u64 .ptr .global .align 8 double_sum2d_16_weighted_param_1,
	.param .u32 double_sum2d_16_weighted_param_2,
	.param .u64 .ptr .global .align 8 double_sum2d_16_weighted_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<70>;
	.reg .f64 	%fd<33>;
	.reg .b64 	%rd<37>;


	ld.param.u64 	%rd1, [double_sum2d_16_weighted_param_1];
	ld.param.u32 	%r28, [double_sum2d_16_weighted_param_2];
	ld.param.u64 	%rd2, [double_sum2d_16_weighted_param_3];
	setp.eq.s32	%p1, %r28, 0;
	@%p1 bra 	BB16_3;

	mov.b32	%r30, %envreg3;
	mov.u32 	%r31, %ntid.x;
	mov.u32 	%r32, %ctaid.x;
	mov.u32 	%r33, %tid.x;
	add.s32 	%r34, %r30, %r33;
	mad.lo.s32 	%r35, %r32, %r31, %r34;
	shl.b32 	%r36, %r35, 4;
	add.s32 	%r37, %r36, 15;
	mul.lo.s32 	%r68, %r28, %r37;
	add.s32 	%r38, %r36, 14;
	mul.lo.s32 	%r67, %r28, %r38;
	add.s32 	%r39, %r36, 13;
	mul.lo.s32 	%r66, %r28, %r39;
	add.s32 	%r40, %r36, 12;
	mul.lo.s32 	%r65, %r28, %r40;
	add.s32 	%r41, %r36, 11;
	mul.lo.s32 	%r5, %r28, %r41;
	add.s32 	%r42, %r36, 10;
	mul.lo.s32 	%r6, %r28, %r42;
	add.s32 	%r43, %r36, 9;
	mul.lo.s32 	%r7, %r28, %r43;
	add.s32 	%r44, %r36, 8;
	mul.lo.s32 	%r8, %r28, %r44;
	add.s32 	%r45, %r36, 7;
	mul.lo.s32 	%r9, %r28, %r45;
	add.s32 	%r46, %r36, 6;
	mul.lo.s32 	%r10, %r28, %r46;
	add.s32 	%r47, %r36, 5;
	mul.lo.s32 	%r11, %r28, %r47;
	add.s32 	%r48, %r36, 4;
	mul.lo.s32 	%r12, %r28, %r48;
	add.s32 	%r49, %r36, 3;
	mul.lo.s32 	%r13, %r28, %r49;
	add.s32 	%r50, %r36, 2;
	mul.lo.s32 	%r14, %r28, %r50;
	add.s32 	%r51, %r36, 1;
	mul.lo.s32 	%r15, %r28, %r51;
	mul.lo.s32 	%r16, %r28, %r35;
	shl.b32 	%r17, %r16, 4;
	mov.u32 	%r69, 0;

BB16_2:
	add.s32 	%r52, %r17, %r69;
	mul.wide.u32 	%rd3, %r52, 8;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.f64 	%fd1, [%rd4];
	add.f64 	%fd2, %fd1, 0d0000000000000000;
	add.s32 	%r53, %r15, %r69;
	mul.wide.u32 	%rd5, %r53, 8;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.f64 	%fd3, [%rd6];
	add.f64 	%fd4, %fd2, %fd3;
	add.s32 	%r54, %r14, %r69;
	mul.wide.u32 	%rd7, %r54, 8;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.f64 	%fd5, [%rd8];
	add.f64 	%fd6, %fd4, %fd5;
	add.s32 	%r55, %r13, %r69;
	mul.wide.u32 	%rd9, %r55, 8;
	add.s64 	%rd10, %rd1, %rd9;
	ld.global.f64 	%fd7, [%rd10];
	add.f64 	%fd8, %fd6, %fd7;
	add.s32 	%r56, %r12, %r69;
	mul.wide.u32 	%rd11, %r56, 8;
	add.s64 	%rd12, %rd1, %rd11;
	ld.global.f64 	%fd9, [%rd12];
	add.f64 	%fd10, %fd8, %fd9;
	add.s32 	%r57, %r11, %r69;
	mul.wide.u32 	%rd13, %r57, 8;
	add.s64 	%rd14, %rd1, %rd13;
	ld.global.f64 	%fd11, [%rd14];
	add.f64 	%fd12, %fd10, %fd11;
	add.s32 	%r58, %r10, %r69;
	mul.wide.u32 	%rd15, %r58, 8;
	add.s64 	%rd16, %rd1, %rd15;
	ld.global.f64 	%fd13, [%rd16];
	add.f64 	%fd14, %fd12, %fd13;
	add.s32 	%r59, %r9, %r69;
	mul.wide.u32 	%rd17, %r59, 8;
	add.s64 	%rd18, %rd1, %rd17;
	ld.global.f64 	%fd15, [%rd18];
	add.f64 	%fd16, %fd14, %fd15;
	add.s32 	%r60, %r8, %r69;
	mul.wide.u32 	%rd19, %r60, 8;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.f64 	%fd17, [%rd20];
	add.f64 	%fd18, %fd16, %fd17;
	add.s32 	%r61, %r7, %r69;
	mul.wide.u32 	%rd21, %r61, 8;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.f64 	%fd19, [%rd22];
	add.f64 	%fd20, %fd18, %fd19;
	add.s32 	%r62, %r6, %r69;
	mul.wide.u32 	%rd23, %r62, 8;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.f64 	%fd21, [%rd24];
	add.f64 	%fd22, %fd20, %fd21;
	add.s32 	%r63, %r5, %r69;
	mul.wide.u32 	%rd25, %r63, 8;
	add.s64 	%rd26, %rd1, %rd25;
	ld.global.f64 	%fd23, [%rd26];
	add.f64 	%fd24, %fd22, %fd23;
	mul.wide.u32 	%rd27, %r65, 8;
	add.s64 	%rd28, %rd1, %rd27;
	ld.global.f64 	%fd25, [%rd28];
	add.f64 	%fd26, %fd24, %fd25;
	mul.wide.u32 	%rd29, %r66, 8;
	add.s64 	%rd30, %rd1, %rd29;
	ld.global.f64 	%fd27, [%rd30];
	add.f64 	%fd28, %fd26, %fd27;
	mul.wide.u32 	%rd31, %r67, 8;
	add.s64 	%rd32, %rd1, %rd31;
	ld.global.f64 	%fd29, [%rd32];
	add.f64 	%fd30, %fd28, %fd29;
	mul.wide.u32 	%rd33, %r68, 8;
	add.s64 	%rd34, %rd1, %rd33;
	ld.global.f64 	%fd31, [%rd34];
	add.f64 	%fd32, %fd30, %fd31;
	add.s32 	%r64, %r16, %r69;
	mul.wide.u32 	%rd35, %r64, 8;
	add.s64 	%rd36, %rd2, %rd35;
	st.global.f64 	[%rd36], %fd32;
	add.s32 	%r68, %r68, 1;
	add.s32 	%r67, %r67, 1;
	add.s32 	%r66, %r66, 1;
	add.s32 	%r65, %r65, 1;
	add.s32 	%r69, %r69, 1;
	setp.lt.u32	%p2, %r69, %r28;
	@%p2 bra 	BB16_2;

BB16_3:
	ret;
}

	// .globl	double_sum2d_N_weighted
.entry double_sum2d_N_weighted(
	.param .u64 .ptr .global .align 16 double_sum2d_N_weighted_param_0,
	.param .u64 .ptr .global .align 8 double_sum2d_N_weighted_param_1,
	.param .u32 double_sum2d_N_weighted_param_2,
	.param .u32 double_sum2d_N_weighted_param_3,
	.param .u32 double_sum2d_N_weighted_param_4,
	.param .u32 double_sum2d_N_weighted_param_5,
	.param .u64 .ptr .global .align 8 double_sum2d_N_weighted_param_6
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<28>;
	.reg .f64 	%fd<10>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [double_sum2d_N_weighted_param_1];
	ld.param.u32 	%r10, [double_sum2d_N_weighted_param_2];
	ld.param.u32 	%r11, [double_sum2d_N_weighted_param_3];
	ld.param.u32 	%r12, [double_sum2d_N_weighted_param_4];
	ld.param.u32 	%r13, [double_sum2d_N_weighted_param_5];
	ld.param.u64 	%rd2, [double_sum2d_N_weighted_param_6];
	setp.eq.s32	%p1, %r10, 0;
	@%p1 bra 	BB17_5;

	mov.b32	%r15, %envreg3;
	mov.u32 	%r16, %ctaid.x;
	mov.u32 	%r17, %ntid.x;
	mad.lo.s32 	%r18, %r16, %r17, %r15;
	mov.u32 	%r19, %tid.x;
	add.s32 	%r20, %r18, %r19;
	mul.lo.s32 	%r21, %r20, %r10;
	add.s32 	%r1, %r21, %r12;
	mad.lo.s32 	%r2, %r21, %r13, %r11;
	mov.u32 	%r14, 0;
	mov.u32 	%r27, %r14;

BB17_2:
	add.s32 	%r24, %r2, %r27;
	setp.eq.s32	%p2, %r13, 0;
	mov.f64 	%fd9, 0d0000000000000000;
	mov.f64 	%fd8, %fd9;
	mov.u32 	%r26, %r14;
	@%p2 bra 	BB17_4;

BB17_3:
	mov.u32 	%r6, %r26;
	mul.wide.u32 	%rd3, %r24, 8;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.f64 	%fd6, [%rd4];
	add.f64 	%fd9, %fd9, %fd6;
	add.s32 	%r24, %r24, %r10;
	add.s32 	%r8, %r6, 1;
	setp.lt.u32	%p3, %r8, %r13;
	mov.u32 	%r26, %r8;
	mov.f64 	%fd8, %fd9;
	@%p3 bra 	BB17_3;

BB17_4:
	add.s32 	%r23, %r1, %r27;
	mul.wide.u32 	%rd5, %r23, 8;
	add.s64 	%rd6, %rd2, %rd5;
	st.global.f64 	[%rd6], %fd8;
	add.s32 	%r27, %r27, 1;
	setp.lt.u32	%p4, %r27, %r10;
	@%p4 bra 	BB17_2;

BB17_5:
	ret;
}

	// .globl	double2_sum_16
.entry double2_sum_16(
	.param .u64 .ptr .global .align 16 double2_sum_16_param_0,
	.param .u64 .ptr .global .align 8 double2_sum_16_param_1
)
{
	.reg .b32 	%r<23>;
	.reg .f64 	%fd<97>;
	.reg .b64 	%rd<37>;


	ld.param.u64 	%rd1, [double2_sum_16_param_0];
	ld.param.u64 	%rd2, [double2_sum_16_param_1];
	mov.b32	%r1, %envreg3;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mad.lo.s32 	%r4, %r3, %r2, %r1;
	mov.u32 	%r5, %tid.x;
	add.s32 	%r6, %r4, %r5;
	shl.b32 	%r7, %r6, 4;
	mul.wide.u32 	%rd3, %r7, 16;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.v2.f64 	{%fd1, %fd2}, [%rd4];
	add.f64 	%fd4, %fd1, 0d0000000000000000;
	add.f64 	%fd6, %fd2, %fd4;
	add.s32 	%r8, %r7, 1;
	mul.wide.u32 	%rd5, %r8, 16;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.v2.f64 	{%fd7, %fd8}, [%rd6];
	add.f64 	%fd10, %fd6, %fd7;
	add.f64 	%fd12, %fd8, %fd10;
	add.s32 	%r9, %r7, 2;
	mul.wide.u32 	%rd7, %r9, 16;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.v2.f64 	{%fd13, %fd14}, [%rd8];
	add.f64 	%fd16, %fd12, %fd13;
	add.f64 	%fd18, %fd14, %fd16;
	add.s32 	%r10, %r7, 3;
	mul.wide.u32 	%rd9, %r10, 16;
	add.s64 	%rd10, %rd1, %rd9;
	ld.global.v2.f64 	{%fd19, %fd20}, [%rd10];
	add.f64 	%fd22, %fd18, %fd19;
	add.f64 	%fd24, %fd20, %fd22;
	add.s32 	%r11, %r7, 4;
	mul.wide.u32 	%rd11, %r11, 16;
	add.s64 	%rd12, %rd1, %rd11;
	ld.global.v2.f64 	{%fd25, %fd26}, [%rd12];
	add.f64 	%fd28, %fd24, %fd25;
	add.f64 	%fd30, %fd26, %fd28;
	add.s32 	%r12, %r7, 5;
	mul.wide.u32 	%rd13, %r12, 16;
	add.s64 	%rd14, %rd1, %rd13;
	ld.global.v2.f64 	{%fd31, %fd32}, [%rd14];
	add.f64 	%fd34, %fd30, %fd31;
	add.f64 	%fd36, %fd32, %fd34;
	add.s32 	%r13, %r7, 6;
	mul.wide.u32 	%rd15, %r13, 16;
	add.s64 	%rd16, %rd1, %rd15;
	ld.global.v2.f64 	{%fd37, %fd38}, [%rd16];
	add.f64 	%fd40, %fd36, %fd37;
	add.f64 	%fd42, %fd38, %fd40;
	add.s32 	%r14, %r7, 7;
	mul.wide.u32 	%rd17, %r14, 16;
	add.s64 	%rd18, %rd1, %rd17;
	ld.global.v2.f64 	{%fd43, %fd44}, [%rd18];
	add.f64 	%fd46, %fd42, %fd43;
	add.f64 	%fd48, %fd44, %fd46;
	add.s32 	%r15, %r7, 8;
	mul.wide.u32 	%rd19, %r15, 16;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.v2.f64 	{%fd49, %fd50}, [%rd20];
	add.f64 	%fd52, %fd48, %fd49;
	add.f64 	%fd54, %fd50, %fd52;
	add.s32 	%r16, %r7, 9;
	mul.wide.u32 	%rd21, %r16, 16;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.v2.f64 	{%fd55, %fd56}, [%rd22];
	add.f64 	%fd58, %fd54, %fd55;
	add.f64 	%fd60, %fd56, %fd58;
	add.s32 	%r17, %r7, 10;
	mul.wide.u32 	%rd23, %r17, 16;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.v2.f64 	{%fd61, %fd62}, [%rd24];
	add.f64 	%fd64, %fd60, %fd61;
	add.f64 	%fd66, %fd62, %fd64;
	add.s32 	%r18, %r7, 11;
	mul.wide.u32 	%rd25, %r18, 16;
	add.s64 	%rd26, %rd1, %rd25;
	ld.global.v2.f64 	{%fd67, %fd68}, [%rd26];
	add.f64 	%fd70, %fd66, %fd67;
	add.f64 	%fd72, %fd68, %fd70;
	add.s32 	%r19, %r7, 12;
	mul.wide.u32 	%rd27, %r19, 16;
	add.s64 	%rd28, %rd1, %rd27;
	ld.global.v2.f64 	{%fd73, %fd74}, [%rd28];
	add.f64 	%fd76, %fd72, %fd73;
	add.f64 	%fd78, %fd74, %fd76;
	add.s32 	%r20, %r7, 13;
	mul.wide.u32 	%rd29, %r20, 16;
	add.s64 	%rd30, %rd1, %rd29;
	ld.global.v2.f64 	{%fd79, %fd80}, [%rd30];
	add.f64 	%fd82, %fd78, %fd79;
	add.f64 	%fd84, %fd80, %fd82;
	add.s32 	%r21, %r7, 14;
	mul.wide.u32 	%rd31, %r21, 16;
	add.s64 	%rd32, %rd1, %rd31;
	ld.global.v2.f64 	{%fd85, %fd86}, [%rd32];
	add.f64 	%fd88, %fd84, %fd85;
	add.f64 	%fd90, %fd86, %fd88;
	add.s32 	%r22, %r7, 15;
	mul.wide.u32 	%rd33, %r22, 16;
	add.s64 	%rd34, %rd1, %rd33;
	ld.global.v2.f64 	{%fd91, %fd92}, [%rd34];
	add.f64 	%fd94, %fd90, %fd91;
	add.f64 	%fd96, %fd92, %fd94;
	mul.wide.u32 	%rd35, %r6, 8;
	add.s64 	%rd36, %rd2, %rd35;
	st.global.f64 	[%rd36], %fd96;
	ret;
}

	// .globl	double2_sum_N
.entry double2_sum_N(
	.param .u64 .ptr .global .align 16 double2_sum_N_param_0,
	.param .u32 double2_sum_N_param_1,
	.param .u32 double2_sum_N_param_2,
	.param .u32 double2_sum_N_param_3,
	.param .u64 .ptr .global .align 8 double2_sum_N_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<20>;
	.reg .f64 	%fd<12>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [double2_sum_N_param_0];
	ld.param.u32 	%r8, [double2_sum_N_param_1];
	ld.param.u32 	%r9, [double2_sum_N_param_2];
	ld.param.u32 	%r10, [double2_sum_N_param_3];
	ld.param.u64 	%rd2, [double2_sum_N_param_4];
	mov.b32	%r11, %envreg3;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %ntid.x;
	mad.lo.s32 	%r1, %r12, %r13, %r11;
	mov.u32 	%r2, %tid.x;
	setp.eq.s32	%p1, %r10, 0;
	mov.f64 	%fd11, 0d0000000000000000;
	@%p1 bra 	BB19_3;

	add.s32 	%r15, %r1, %r2;
	mad.lo.s32 	%r18, %r10, %r15, %r8;
	mov.u32 	%r19, 0;
	mov.f64 	%fd11, 0d0000000000000000;

BB19_2:
	mul.wide.u32 	%rd3, %r18, 16;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.v2.f64 	{%fd6, %fd7}, [%rd4];
	add.f64 	%fd9, %fd11, %fd6;
	add.f64 	%fd11, %fd7, %fd9;
	add.s32 	%r18, %r18, 1;
	add.s32 	%r19, %r19, 1;
	setp.lt.u32	%p2, %r19, %r10;
	@%p2 bra 	BB19_2;

BB19_3:
	add.s32 	%r16, %r1, %r2;
	add.s32 	%r17, %r16, %r9;
	mul.wide.u32 	%rd5, %r17, 8;
	add.s64 	%rd6, %rd2, %rd5;
	st.global.f64 	[%rd6], %fd11;
	ret;
}

	// .globl	double2_sumcomponents_16
.entry double2_sumcomponents_16(
	.param .u64 .ptr .global .align 16 double2_sumcomponents_16_param_0,
	.param .u64 .ptr .global .align 8 double2_sumcomponents_16_param_1,
	.param .u64 .ptr .global .align 8 double2_sumcomponents_16_param_2
)
{
	.reg .b32 	%r<23>;
	.reg .f64 	%fd<97>;
	.reg .b64 	%rd<39>;


	ld.param.u64 	%rd1, [double2_sumcomponents_16_param_0];
	ld.param.u64 	%rd2, [double2_sumcomponents_16_param_1];
	ld.param.u64 	%rd3, [double2_sumcomponents_16_param_2];
	mov.b32	%r1, %envreg3;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mad.lo.s32 	%r4, %r3, %r2, %r1;
	mov.u32 	%r5, %tid.x;
	add.s32 	%r6, %r4, %r5;
	shl.b32 	%r7, %r6, 4;
	mul.wide.u32 	%rd4, %r7, 16;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.v2.f64 	{%fd1, %fd2}, [%rd5];
	add.f64 	%fd4, %fd1, 0d0000000000000000;
	add.f64 	%fd6, %fd2, 0d0000000000000000;
	add.s32 	%r8, %r7, 1;
	mul.wide.u32 	%rd6, %r8, 16;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.v2.f64 	{%fd7, %fd8}, [%rd7];
	add.f64 	%fd10, %fd4, %fd7;
	add.f64 	%fd12, %fd6, %fd8;
	add.s32 	%r9, %r7, 2;
	mul.wide.u32 	%rd8, %r9, 16;
	add.s64 	%rd9, %rd1, %rd8;
	ld.global.v2.f64 	{%fd13, %fd14}, [%rd9];
	add.f64 	%fd16, %fd10, %fd13;
	add.f64 	%fd18, %fd12, %fd14;
	add.s32 	%r10, %r7, 3;
	mul.wide.u32 	%rd10, %r10, 16;
	add.s64 	%rd11, %rd1, %rd10;
	ld.global.v2.f64 	{%fd19, %fd20}, [%rd11];
	add.f64 	%fd22, %fd16, %fd19;
	add.f64 	%fd24, %fd18, %fd20;
	add.s32 	%r11, %r7, 4;
	mul.wide.u32 	%rd12, %r11, 16;
	add.s64 	%rd13, %rd1, %rd12;
	ld.global.v2.f64 	{%fd25, %fd26}, [%rd13];
	add.f64 	%fd28, %fd22, %fd25;
	add.f64 	%fd30, %fd24, %fd26;
	add.s32 	%r12, %r7, 5;
	mul.wide.u32 	%rd14, %r12, 16;
	add.s64 	%rd15, %rd1, %rd14;
	ld.global.v2.f64 	{%fd31, %fd32}, [%rd15];
	add.f64 	%fd34, %fd28, %fd31;
	add.f64 	%fd36, %fd30, %fd32;
	add.s32 	%r13, %r7, 6;
	mul.wide.u32 	%rd16, %r13, 16;
	add.s64 	%rd17, %rd1, %rd16;
	ld.global.v2.f64 	{%fd37, %fd38}, [%rd17];
	add.f64 	%fd40, %fd34, %fd37;
	add.f64 	%fd42, %fd36, %fd38;
	add.s32 	%r14, %r7, 7;
	mul.wide.u32 	%rd18, %r14, 16;
	add.s64 	%rd19, %rd1, %rd18;
	ld.global.v2.f64 	{%fd43, %fd44}, [%rd19];
	add.f64 	%fd46, %fd40, %fd43;
	add.f64 	%fd48, %fd42, %fd44;
	add.s32 	%r15, %r7, 8;
	mul.wide.u32 	%rd20, %r15, 16;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.v2.f64 	{%fd49, %fd50}, [%rd21];
	add.f64 	%fd52, %fd46, %fd49;
	add.f64 	%fd54, %fd48, %fd50;
	add.s32 	%r16, %r7, 9;
	mul.wide.u32 	%rd22, %r16, 16;
	add.s64 	%rd23, %rd1, %rd22;
	ld.global.v2.f64 	{%fd55, %fd56}, [%rd23];
	add.f64 	%fd58, %fd52, %fd55;
	add.f64 	%fd60, %fd54, %fd56;
	add.s32 	%r17, %r7, 10;
	mul.wide.u32 	%rd24, %r17, 16;
	add.s64 	%rd25, %rd1, %rd24;
	ld.global.v2.f64 	{%fd61, %fd62}, [%rd25];
	add.f64 	%fd64, %fd58, %fd61;
	add.f64 	%fd66, %fd60, %fd62;
	add.s32 	%r18, %r7, 11;
	mul.wide.u32 	%rd26, %r18, 16;
	add.s64 	%rd27, %rd1, %rd26;
	ld.global.v2.f64 	{%fd67, %fd68}, [%rd27];
	add.f64 	%fd70, %fd64, %fd67;
	add.f64 	%fd72, %fd66, %fd68;
	add.s32 	%r19, %r7, 12;
	mul.wide.u32 	%rd28, %r19, 16;
	add.s64 	%rd29, %rd1, %rd28;
	ld.global.v2.f64 	{%fd73, %fd74}, [%rd29];
	add.f64 	%fd76, %fd70, %fd73;
	add.f64 	%fd78, %fd72, %fd74;
	add.s32 	%r20, %r7, 13;
	mul.wide.u32 	%rd30, %r20, 16;
	add.s64 	%rd31, %rd1, %rd30;
	ld.global.v2.f64 	{%fd79, %fd80}, [%rd31];
	add.f64 	%fd82, %fd76, %fd79;
	add.f64 	%fd84, %fd78, %fd80;
	add.s32 	%r21, %r7, 14;
	mul.wide.u32 	%rd32, %r21, 16;
	add.s64 	%rd33, %rd1, %rd32;
	ld.global.v2.f64 	{%fd85, %fd86}, [%rd33];
	add.f64 	%fd88, %fd82, %fd85;
	add.f64 	%fd90, %fd84, %fd86;
	add.s32 	%r22, %r7, 15;
	mul.wide.u32 	%rd34, %r22, 16;
	add.s64 	%rd35, %rd1, %rd34;
	ld.global.v2.f64 	{%fd91, %fd92}, [%rd35];
	add.f64 	%fd94, %fd88, %fd91;
	add.f64 	%fd96, %fd90, %fd92;
	mul.wide.u32 	%rd36, %r6, 8;
	add.s64 	%rd37, %rd2, %rd36;
	st.global.f64 	[%rd37], %fd94;
	add.s64 	%rd38, %rd3, %rd36;
	st.global.f64 	[%rd38], %fd96;
	ret;
}

	// .globl	double2_sumcomponents_N
.entry double2_sumcomponents_N(
	.param .u64 .ptr .global .align 16 double2_sumcomponents_N_param_0,
	.param .u32 double2_sumcomponents_N_param_1,
	.param .u32 double2_sumcomponents_N_param_2,
	.param .u32 double2_sumcomponents_N_param_3,
	.param .u64 .ptr .global .align 8 double2_sumcomponents_N_param_4,
	.param .u64 .ptr .global .align 8 double2_sumcomponents_N_param_5
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<20>;
	.reg .f64 	%fd<17>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [double2_sumcomponents_N_param_0];
	ld.param.u32 	%r8, [double2_sumcomponents_N_param_1];
	ld.param.u32 	%r9, [double2_sumcomponents_N_param_2];
	ld.param.u32 	%r10, [double2_sumcomponents_N_param_3];
	ld.param.u64 	%rd2, [double2_sumcomponents_N_param_4];
	ld.param.u64 	%rd3, [double2_sumcomponents_N_param_5];
	mov.b32	%r11, %envreg3;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %ntid.x;
	mad.lo.s32 	%r1, %r12, %r13, %r11;
	mov.u32 	%r2, %tid.x;
	setp.eq.s32	%p1, %r10, 0;
	mov.f64 	%fd16, 0d0000000000000000;
	mov.f64 	%fd15, %fd16;
	@%p1 bra 	BB21_3;

	add.s32 	%r15, %r1, %r2;
	mad.lo.s32 	%r18, %r10, %r15, %r8;
	mov.u32 	%r19, 0;
	mov.f64 	%fd16, 0d0000000000000000;
	mov.f64 	%fd15, %fd16;

BB21_2:
	mul.wide.u32 	%rd4, %r18, 16;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.v2.f64 	{%fd11, %fd12}, [%rd5];
	add.f64 	%fd16, %fd16, %fd11;
	add.f64 	%fd15, %fd15, %fd12;
	add.s32 	%r18, %r18, 1;
	add.s32 	%r19, %r19, 1;
	setp.lt.u32	%p2, %r19, %r10;
	@%p2 bra 	BB21_2;

BB21_3:
	add.s32 	%r16, %r1, %r2;
	add.s32 	%r17, %r16, %r9;
	mul.wide.u32 	%rd6, %r17, 8;
	add.s64 	%rd7, %rd2, %rd6;
	st.global.f64 	[%rd7], %fd16;
	add.s64 	%rd8, %rd3, %rd6;
	st.global.f64 	[%rd8], %fd15;
	ret;
}

	// .globl	double2_sumcomponents_weighted_16
.entry double2_sumcomponents_weighted_16(
	.param .u64 .ptr .global .align 16 double2_sumcomponents_weighted_16_param_0,
	.param .u64 .ptr .global .align 4 double2_sumcomponents_weighted_16_param_1,
	.param .u64 .ptr .global .align 8 double2_sumcomponents_weighted_16_param_2,
	.param .u64 .ptr .global .align 8 double2_sumcomponents_weighted_16_param_3
)
{
	.reg .f32 	%f<17>;
	.reg .b32 	%r<23>;
	.reg .f64 	%fd<113>;
	.reg .b64 	%rd<72>;


	ld.param.u64 	%rd1, [double2_sumcomponents_weighted_16_param_0];
	ld.param.u64 	%rd2, [double2_sumcomponents_weighted_16_param_1];
	ld.param.u64 	%rd3, [double2_sumcomponents_weighted_16_param_2];
	ld.param.u64 	%rd4, [double2_sumcomponents_weighted_16_param_3];
	mov.b32	%r1, %envreg3;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mad.lo.s32 	%r4, %r3, %r2, %r1;
	mov.u32 	%r5, %tid.x;
	add.s32 	%r6, %r4, %r5;
	shl.b32 	%r7, %r6, 4;
	mul.wide.u32 	%rd5, %r7, 16;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.v2.f64 	{%fd1, %fd2}, [%rd6];
	mul.wide.u32 	%rd7, %r7, 4;
	add.s64 	%rd8, %rd2, %rd7;
	ld.global.f32 	%f1, [%rd8];
	cvt.f64.f32	%fd4, %f1;
	fma.rn.f64 	%fd5, %fd1, %fd4, 0d0000000000000000;
	fma.rn.f64 	%fd7, %fd4, %fd2, 0d0000000000000000;
	add.s32 	%r8, %r7, 1;
	mul.wide.u32 	%rd9, %r8, 16;
	add.s64 	%rd10, %rd1, %rd9;
	ld.global.v2.f64 	{%fd8, %fd9}, [%rd10];
	mul.wide.u32 	%rd11, %r8, 4;
	add.s64 	%rd12, %rd2, %rd11;
	ld.global.f32 	%f2, [%rd12];
	cvt.f64.f32	%fd11, %f2;
	fma.rn.f64 	%fd12, %fd8, %fd11, %fd5;
	fma.rn.f64 	%fd14, %fd11, %fd9, %fd7;
	add.s32 	%r9, %r7, 2;
	mul.wide.u32 	%rd13, %r9, 16;
	add.s64 	%rd14, %rd1, %rd13;
	ld.global.v2.f64 	{%fd15, %fd16}, [%rd14];
	mul.wide.u32 	%rd15, %r9, 4;
	add.s64 	%rd16, %rd2, %rd15;
	ld.global.f32 	%f3, [%rd16];
	cvt.f64.f32	%fd18, %f3;
	fma.rn.f64 	%fd19, %fd15, %fd18, %fd12;
	fma.rn.f64 	%fd21, %fd18, %fd16, %fd14;
	add.s32 	%r10, %r7, 3;
	mul.wide.u32 	%rd17, %r10, 16;
	add.s64 	%rd18, %rd1, %rd17;
	ld.global.v2.f64 	{%fd22, %fd23}, [%rd18];
	mul.wide.u32 	%rd19, %r10, 4;
	add.s64 	%rd20, %rd2, %rd19;
	ld.global.f32 	%f4, [%rd20];
	cvt.f64.f32	%fd25, %f4;
	fma.rn.f64 	%fd26, %fd22, %fd25, %fd19;
	fma.rn.f64 	%fd28, %fd25, %fd23, %fd21;
	add.s32 	%r11, %r7, 4;
	mul.wide.u32 	%rd21, %r11, 16;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.v2.f64 	{%fd29, %fd30}, [%rd22];
	mul.wide.u32 	%rd23, %r11, 4;
	add.s64 	%rd24, %rd2, %rd23;
	ld.global.f32 	%f5, [%rd24];
	cvt.f64.f32	%fd32, %f5;
	fma.rn.f64 	%fd33, %fd29, %fd32, %fd26;
	fma.rn.f64 	%fd35, %fd32, %fd30, %fd28;
	add.s32 	%r12, %r7, 5;
	mul.wide.u32 	%rd25, %r12, 16;
	add.s64 	%rd26, %rd1, %rd25;
	ld.global.v2.f64 	{%fd36, %fd37}, [%rd26];
	mul.wide.u32 	%rd27, %r12, 4;
	add.s64 	%rd28, %rd2, %rd27;
	ld.global.f32 	%f6, [%rd28];
	cvt.f64.f32	%fd39, %f6;
	fma.rn.f64 	%fd40, %fd36, %fd39, %fd33;
	fma.rn.f64 	%fd42, %fd39, %fd37, %fd35;
	add.s32 	%r13, %r7, 6;
	mul.wide.u32 	%rd29, %r13, 16;
	add.s64 	%rd30, %rd1, %rd29;
	ld.global.v2.f64 	{%fd43, %fd44}, [%rd30];
	mul.wide.u32 	%rd31, %r13, 4;
	add.s64 	%rd32, %rd2, %rd31;
	ld.global.f32 	%f7, [%rd32];
	cvt.f64.f32	%fd46, %f7;
	fma.rn.f64 	%fd47, %fd43, %fd46, %fd40;
	fma.rn.f64 	%fd49, %fd46, %fd44, %fd42;
	add.s32 	%r14, %r7, 7;
	mul.wide.u32 	%rd33, %r14, 16;
	add.s64 	%rd34, %rd1, %rd33;
	ld.global.v2.f64 	{%fd50, %fd51}, [%rd34];
	mul.wide.u32 	%rd35, %r14, 4;
	add.s64 	%rd36, %rd2, %rd35;
	ld.global.f32 	%f8, [%rd36];
	cvt.f64.f32	%fd53, %f8;
	fma.rn.f64 	%fd54, %fd50, %fd53, %fd47;
	fma.rn.f64 	%fd56, %fd53, %fd51, %fd49;
	add.s32 	%r15, %r7, 8;
	mul.wide.u32 	%rd37, %r15, 16;
	add.s64 	%rd38, %rd1, %rd37;
	ld.global.v2.f64 	{%fd57, %fd58}, [%rd38];
	mul.wide.u32 	%rd39, %r15, 4;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.f32 	%f9, [%rd40];
	cvt.f64.f32	%fd60, %f9;
	fma.rn.f64 	%fd61, %fd57, %fd60, %fd54;
	fma.rn.f64 	%fd63, %fd60, %fd58, %fd56;
	add.s32 	%r16, %r7, 9;
	mul.wide.u32 	%rd41, %r16, 16;
	add.s64 	%rd42, %rd1, %rd41;
	ld.global.v2.f64 	{%fd64, %fd65}, [%rd42];
	mul.wide.u32 	%rd43, %r16, 4;
	add.s64 	%rd44, %rd2, %rd43;
	ld.global.f32 	%f10, [%rd44];
	cvt.f64.f32	%fd67, %f10;
	fma.rn.f64 	%fd68, %fd64, %fd67, %fd61;
	fma.rn.f64 	%fd70, %fd67, %fd65, %fd63;
	add.s32 	%r17, %r7, 10;
	mul.wide.u32 	%rd45, %r17, 16;
	add.s64 	%rd46, %rd1, %rd45;
	ld.global.v2.f64 	{%fd71, %fd72}, [%rd46];
	mul.wide.u32 	%rd47, %r17, 4;
	add.s64 	%rd48, %rd2, %rd47;
	ld.global.f32 	%f11, [%rd48];
	cvt.f64.f32	%fd74, %f11;
	fma.rn.f64 	%fd75, %fd71, %fd74, %fd68;
	fma.rn.f64 	%fd77, %fd74, %fd72, %fd70;
	add.s32 	%r18, %r7, 11;
	mul.wide.u32 	%rd49, %r18, 16;
	add.s64 	%rd50, %rd1, %rd49;
	ld.global.v2.f64 	{%fd78, %fd79}, [%rd50];
	mul.wide.u32 	%rd51, %r18, 4;
	add.s64 	%rd52, %rd2, %rd51;
	ld.global.f32 	%f12, [%rd52];
	cvt.f64.f32	%fd81, %f12;
	fma.rn.f64 	%fd82, %fd78, %fd81, %fd75;
	fma.rn.f64 	%fd84, %fd81, %fd79, %fd77;
	add.s32 	%r19, %r7, 12;
	mul.wide.u32 	%rd53, %r19, 16;
	add.s64 	%rd54, %rd1, %rd53;
	ld.global.v2.f64 	{%fd85, %fd86}, [%rd54];
	mul.wide.u32 	%rd55, %r19, 4;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.f32 	%f13, [%rd56];
	cvt.f64.f32	%fd88, %f13;
	fma.rn.f64 	%fd89, %fd85, %fd88, %fd82;
	fma.rn.f64 	%fd91, %fd88, %fd86, %fd84;
	add.s32 	%r20, %r7, 13;
	mul.wide.u32 	%rd57, %r20, 16;
	add.s64 	%rd58, %rd1, %rd57;
	ld.global.v2.f64 	{%fd92, %fd93}, [%rd58];
	mul.wide.u32 	%rd59, %r20, 4;
	add.s64 	%rd60, %rd2, %rd59;
	ld.global.f32 	%f14, [%rd60];
	cvt.f64.f32	%fd95, %f14;
	fma.rn.f64 	%fd96, %fd92, %fd95, %fd89;
	fma.rn.f64 	%fd98, %fd95, %fd93, %fd91;
	add.s32 	%r21, %r7, 14;
	mul.wide.u32 	%rd61, %r21, 16;
	add.s64 	%rd62, %rd1, %rd61;
	ld.global.v2.f64 	{%fd99, %fd100}, [%rd62];
	mul.wide.u32 	%rd63, %r21, 4;
	add.s64 	%rd64, %rd2, %rd63;
	ld.global.f32 	%f15, [%rd64];
	cvt.f64.f32	%fd102, %f15;
	fma.rn.f64 	%fd103, %fd99, %fd102, %fd96;
	fma.rn.f64 	%fd105, %fd102, %fd100, %fd98;
	add.s32 	%r22, %r7, 15;
	mul.wide.u32 	%rd65, %r22, 16;
	add.s64 	%rd66, %rd1, %rd65;
	ld.global.v2.f64 	{%fd106, %fd107}, [%rd66];
	mul.wide.u32 	%rd67, %r22, 4;
	add.s64 	%rd68, %rd2, %rd67;
	ld.global.f32 	%f16, [%rd68];
	cvt.f64.f32	%fd109, %f16;
	fma.rn.f64 	%fd110, %fd106, %fd109, %fd103;
	fma.rn.f64 	%fd112, %fd109, %fd107, %fd105;
	mul.wide.u32 	%rd69, %r6, 8;
	add.s64 	%rd70, %rd3, %rd69;
	st.global.f64 	[%rd70], %fd110;
	add.s64 	%rd71, %rd4, %rd69;
	st.global.f64 	[%rd71], %fd112;
	ret;
}

	// .globl	double2_sumcomponents_weighted_N
.entry double2_sumcomponents_weighted_N(
	.param .u64 .ptr .global .align 16 double2_sumcomponents_weighted_N_param_0,
	.param .u64 .ptr .global .align 4 double2_sumcomponents_weighted_N_param_1,
	.param .u32 double2_sumcomponents_weighted_N_param_2,
	.param .u32 double2_sumcomponents_weighted_N_param_3,
	.param .u32 double2_sumcomponents_weighted_N_param_4,
	.param .u64 .ptr .global .align 8 double2_sumcomponents_weighted_N_param_5,
	.param .u64 .ptr .global .align 8 double2_sumcomponents_weighted_N_param_6
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<24>;
	.reg .f64 	%fd<18>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [double2_sumcomponents_weighted_N_param_0];
	ld.param.u64 	%rd2, [double2_sumcomponents_weighted_N_param_1];
	ld.param.u32 	%r11, [double2_sumcomponents_weighted_N_param_2];
	ld.param.u32 	%r12, [double2_sumcomponents_weighted_N_param_3];
	ld.param.u32 	%r13, [double2_sumcomponents_weighted_N_param_4];
	ld.param.u64 	%rd3, [double2_sumcomponents_weighted_N_param_5];
	ld.param.u64 	%rd4, [double2_sumcomponents_weighted_N_param_6];
	mov.b32	%r14, %envreg3;
	mov.u32 	%r15, %ctaid.x;
	mov.u32 	%r16, %ntid.x;
	mad.lo.s32 	%r1, %r15, %r16, %r14;
	mov.u32 	%r2, %tid.x;
	setp.eq.s32	%p1, %r13, 0;
	mov.f64 	%fd17, 0d0000000000000000;
	mov.f64 	%fd16, %fd17;
	@%p1 bra 	BB23_3;

	add.s32 	%r18, %r1, %r2;
	mul.lo.s32 	%r21, %r13, %r18;
	add.s32 	%r22, %r11, %r21;
	mov.u32 	%r23, 0;
	mov.f64 	%fd17, 0d0000000000000000;
	mov.f64 	%fd16, %fd17;

BB23_2:
	mul.wide.u32 	%rd5, %r21, 16;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.v2.f64 	{%fd11, %fd12}, [%rd6];
	mul.wide.u32 	%rd7, %r22, 4;
	add.s64 	%rd8, %rd2, %rd7;
	ld.global.f32 	%f1, [%rd8];
	cvt.f64.f32	%fd14, %f1;
	fma.rn.f64 	%fd17, %fd11, %fd14, %fd17;
	fma.rn.f64 	%fd16, %fd14, %fd12, %fd16;
	add.s32 	%r22, %r22, 1;
	add.s32 	%r21, %r21, 1;
	add.s32 	%r23, %r23, 1;
	setp.lt.u32	%p2, %r23, %r13;
	@%p2 bra 	BB23_2;

BB23_3:
	add.s32 	%r19, %r1, %r2;
	add.s32 	%r20, %r19, %r12;
	mul.wide.u32 	%rd9, %r20, 8;
	add.s64 	%rd10, %rd3, %rd9;
	st.global.f64 	[%rd10], %fd17;
	add.s64 	%rd11, %rd4, %rd9;
	st.global.f64 	[%rd11], %fd16;
	ret;
}

	// .globl	double2_sum2d_16
.entry double2_sum2d_16(
	.param .u64 .ptr .global .align 16 double2_sum2d_16_param_0,
	.param .u32 double2_sum2d_16_param_1,
	.param .u64 .ptr .global .align 8 double2_sum2d_16_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<70>;
	.reg .f64 	%fd<97>;
	.reg .b64 	%rd<37>;


	ld.param.u64 	%rd1, [double2_sum2d_16_param_0];
	ld.param.u32 	%r28, [double2_sum2d_16_param_1];
	ld.param.u64 	%rd2, [double2_sum2d_16_param_2];
	setp.eq.s32	%p1, %r28, 0;
	@%p1 bra 	BB24_3;

	mov.b32	%r30, %envreg3;
	mov.u32 	%r31, %ntid.x;
	mov.u32 	%r32, %ctaid.x;
	mov.u32 	%r33, %tid.x;
	add.s32 	%r34, %r30, %r33;
	mad.lo.s32 	%r35, %r32, %r31, %r34;
	shl.b32 	%r36, %r35, 4;
	add.s32 	%r37, %r36, 15;
	mul.lo.s32 	%r68, %r28, %r37;
	add.s32 	%r38, %r36, 14;
	mul.lo.s32 	%r67, %r28, %r38;
	add.s32 	%r39, %r36, 13;
	mul.lo.s32 	%r66, %r28, %r39;
	add.s32 	%r40, %r36, 12;
	mul.lo.s32 	%r65, %r28, %r40;
	add.s32 	%r41, %r36, 11;
	mul.lo.s32 	%r5, %r28, %r41;
	add.s32 	%r42, %r36, 10;
	mul.lo.s32 	%r6, %r28, %r42;
	add.s32 	%r43, %r36, 9;
	mul.lo.s32 	%r7, %r28, %r43;
	add.s32 	%r44, %r36, 8;
	mul.lo.s32 	%r8, %r28, %r44;
	add.s32 	%r45, %r36, 7;
	mul.lo.s32 	%r9, %r28, %r45;
	add.s32 	%r46, %r36, 6;
	mul.lo.s32 	%r10, %r28, %r46;
	add.s32 	%r47, %r36, 5;
	mul.lo.s32 	%r11, %r28, %r47;
	add.s32 	%r48, %r36, 4;
	mul.lo.s32 	%r12, %r28, %r48;
	add.s32 	%r49, %r36, 3;
	mul.lo.s32 	%r13, %r28, %r49;
	add.s32 	%r50, %r36, 2;
	mul.lo.s32 	%r14, %r28, %r50;
	add.s32 	%r51, %r36, 1;
	mul.lo.s32 	%r15, %r28, %r51;
	mul.lo.s32 	%r16, %r28, %r35;
	shl.b32 	%r17, %r16, 4;
	mov.u32 	%r69, 0;

BB24_2:
	add.s32 	%r52, %r17, %r69;
	mul.wide.u32 	%rd3, %r52, 16;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.v2.f64 	{%fd1, %fd2}, [%rd4];
	add.f64 	%fd4, %fd1, 0d0000000000000000;
	add.f64 	%fd6, %fd2, %fd4;
	add.s32 	%r53, %r15, %r69;
	mul.wide.u32 	%rd5, %r53, 16;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.v2.f64 	{%fd7, %fd8}, [%rd6];
	add.f64 	%fd10, %fd6, %fd7;
	add.f64 	%fd12, %fd8, %fd10;
	add.s32 	%r54, %r14, %r69;
	mul.wide.u32 	%rd7, %r54, 16;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.v2.f64 	{%fd13, %fd14}, [%rd8];
	add.f64 	%fd16, %fd12, %fd13;
	add.f64 	%fd18, %fd14, %fd16;
	add.s32 	%r55, %r13, %r69;
	mul.wide.u32 	%rd9, %r55, 16;
	add.s64 	%rd10, %rd1, %rd9;
	ld.global.v2.f64 	{%fd19, %fd20}, [%rd10];
	add.f64 	%fd22, %fd18, %fd19;
	add.f64 	%fd24, %fd20, %fd22;
	add.s32 	%r56, %r12, %r69;
	mul.wide.u32 	%rd11, %r56, 16;
	add.s64 	%rd12, %rd1, %rd11;
	ld.global.v2.f64 	{%fd25, %fd26}, [%rd12];
	add.f64 	%fd28, %fd24, %fd25;
	add.f64 	%fd30, %fd26, %fd28;
	add.s32 	%r57, %r11, %r69;
	mul.wide.u32 	%rd13, %r57, 16;
	add.s64 	%rd14, %rd1, %rd13;
	ld.global.v2.f64 	{%fd31, %fd32}, [%rd14];
	add.f64 	%fd34, %fd30, %fd31;
	add.f64 	%fd36, %fd32, %fd34;
	add.s32 	%r58, %r10, %r69;
	mul.wide.u32 	%rd15, %r58, 16;
	add.s64 	%rd16, %rd1, %rd15;
	ld.global.v2.f64 	{%fd37, %fd38}, [%rd16];
	add.f64 	%fd40, %fd36, %fd37;
	add.f64 	%fd42, %fd38, %fd40;
	add.s32 	%r59, %r9, %r69;
	mul.wide.u32 	%rd17, %r59, 16;
	add.s64 	%rd18, %rd1, %rd17;
	ld.global.v2.f64 	{%fd43, %fd44}, [%rd18];
	add.f64 	%fd46, %fd42, %fd43;
	add.f64 	%fd48, %fd44, %fd46;
	add.s32 	%r60, %r8, %r69;
	mul.wide.u32 	%rd19, %r60, 16;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.v2.f64 	{%fd49, %fd50}, [%rd20];
	add.f64 	%fd52, %fd48, %fd49;
	add.f64 	%fd54, %fd50, %fd52;
	add.s32 	%r61, %r7, %r69;
	mul.wide.u32 	%rd21, %r61, 16;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.v2.f64 	{%fd55, %fd56}, [%rd22];
	add.f64 	%fd58, %fd54, %fd55;
	add.f64 	%fd60, %fd56, %fd58;
	add.s32 	%r62, %r6, %r69;
	mul.wide.u32 	%rd23, %r62, 16;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.v2.f64 	{%fd61, %fd62}, [%rd24];
	add.f64 	%fd64, %fd60, %fd61;
	add.f64 	%fd66, %fd62, %fd64;
	add.s32 	%r63, %r5, %r69;
	mul.wide.u32 	%rd25, %r63, 16;
	add.s64 	%rd26, %rd1, %rd25;
	ld.global.v2.f64 	{%fd67, %fd68}, [%rd26];
	add.f64 	%fd70, %fd66, %fd67;
	add.f64 	%fd72, %fd68, %fd70;
	mul.wide.u32 	%rd27, %r65, 16;
	add.s64 	%rd28, %rd1, %rd27;
	ld.global.v2.f64 	{%fd73, %fd74}, [%rd28];
	add.f64 	%fd76, %fd72, %fd73;
	add.f64 	%fd78, %fd74, %fd76;
	mul.wide.u32 	%rd29, %r66, 16;
	add.s64 	%rd30, %rd1, %rd29;
	ld.global.v2.f64 	{%fd79, %fd80}, [%rd30];
	add.f64 	%fd82, %fd78, %fd79;
	add.f64 	%fd84, %fd80, %fd82;
	mul.wide.u32 	%rd31, %r67, 16;
	add.s64 	%rd32, %rd1, %rd31;
	ld.global.v2.f64 	{%fd85, %fd86}, [%rd32];
	add.f64 	%fd88, %fd84, %fd85;
	add.f64 	%fd90, %fd86, %fd88;
	mul.wide.u32 	%rd33, %r68, 16;
	add.s64 	%rd34, %rd1, %rd33;
	ld.global.v2.f64 	{%fd91, %fd92}, [%rd34];
	add.f64 	%fd94, %fd90, %fd91;
	add.f64 	%fd96, %fd92, %fd94;
	add.s32 	%r64, %r16, %r69;
	mul.wide.u32 	%rd35, %r64, 8;
	add.s64 	%rd36, %rd2, %rd35;
	st.global.f64 	[%rd36], %fd96;
	add.s32 	%r68, %r68, 1;
	add.s32 	%r67, %r67, 1;
	add.s32 	%r66, %r66, 1;
	add.s32 	%r65, %r65, 1;
	add.s32 	%r69, %r69, 1;
	setp.lt.u32	%p2, %r69, %r28;
	@%p2 bra 	BB24_2;

BB24_3:
	ret;
}

	// .globl	double2_sum2d_N
.entry double2_sum2d_N(
	.param .u64 .ptr .global .align 16 double2_sum2d_N_param_0,
	.param .u32 double2_sum2d_N_param_1,
	.param .u32 double2_sum2d_N_param_2,
	.param .u32 double2_sum2d_N_param_3,
	.param .u32 double2_sum2d_N_param_4,
	.param .u64 .ptr .global .align 8 double2_sum2d_N_param_5
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<28>;
	.reg .f64 	%fd<14>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [double2_sum2d_N_param_0];
	ld.param.u32 	%r10, [double2_sum2d_N_param_1];
	ld.param.u32 	%r11, [double2_sum2d_N_param_2];
	ld.param.u32 	%r12, [double2_sum2d_N_param_3];
	ld.param.u32 	%r13, [double2_sum2d_N_param_4];
	ld.param.u64 	%rd2, [double2_sum2d_N_param_5];
	setp.eq.s32	%p1, %r10, 0;
	@%p1 bra 	BB25_5;

	mov.b32	%r15, %envreg3;
	mov.u32 	%r16, %ctaid.x;
	mov.u32 	%r17, %ntid.x;
	mad.lo.s32 	%r18, %r16, %r17, %r15;
	mov.u32 	%r19, %tid.x;
	add.s32 	%r20, %r18, %r19;
	mul.lo.s32 	%r21, %r20, %r10;
	add.s32 	%r1, %r21, %r12;
	mad.lo.s32 	%r2, %r21, %r13, %r11;
	mov.u32 	%r14, 0;
	mov.u32 	%r27, %r14;

BB25_2:
	add.s32 	%r24, %r2, %r27;
	setp.eq.s32	%p2, %r13, 0;
	mov.f64 	%fd13, 0d0000000000000000;
	mov.f64 	%fd12, %fd13;
	mov.u32 	%r26, %r14;
	@%p2 bra 	BB25_4;

BB25_3:
	mov.u32 	%r6, %r26;
	mul.wide.u32 	%rd3, %r24, 16;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.v2.f64 	{%fd6, %fd7}, [%rd4];
	add.f64 	%fd9, %fd13, %fd6;
	add.f64 	%fd13, %fd7, %fd9;
	add.s32 	%r24, %r24, %r10;
	add.s32 	%r8, %r6, 1;
	setp.lt.u32	%p3, %r8, %r13;
	mov.u32 	%r26, %r8;
	mov.f64 	%fd12, %fd13;
	@%p3 bra 	BB25_3;

BB25_4:
	add.s32 	%r23, %r1, %r27;
	mul.wide.u32 	%rd5, %r23, 8;
	add.s64 	%rd6, %rd2, %rd5;
	st.global.f64 	[%rd6], %fd12;
	add.s32 	%r27, %r27, 1;
	setp.lt.u32	%p4, %r27, %r10;
	@%p4 bra 	BB25_2;

BB25_5:
	ret;
}

	// .globl	double2_sum2d_16_weighted
.entry double2_sum2d_16_weighted(
	.param .u64 .ptr .global .align 16 double2_sum2d_16_weighted_param_0,
	.param .u64 .ptr .global .align 16 double2_sum2d_16_weighted_param_1,
	.param .u32 double2_sum2d_16_weighted_param_2,
	.param .u64 .ptr .global .align 8 double2_sum2d_16_weighted_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<70>;
	.reg .f64 	%fd<97>;
	.reg .b64 	%rd<37>;


	ld.param.u64 	%rd1, [double2_sum2d_16_weighted_param_1];
	ld.param.u32 	%r28, [double2_sum2d_16_weighted_param_2];
	ld.param.u64 	%rd2, [double2_sum2d_16_weighted_param_3];
	setp.eq.s32	%p1, %r28, 0;
	@%p1 bra 	BB26_3;

	mov.b32	%r30, %envreg3;
	mov.u32 	%r31, %ntid.x;
	mov.u32 	%r32, %ctaid.x;
	mov.u32 	%r33, %tid.x;
	add.s32 	%r34, %r30, %r33;
	mad.lo.s32 	%r35, %r32, %r31, %r34;
	shl.b32 	%r36, %r35, 4;
	add.s32 	%r37, %r36, 15;
	mul.lo.s32 	%r68, %r28, %r37;
	add.s32 	%r38, %r36, 14;
	mul.lo.s32 	%r67, %r28, %r38;
	add.s32 	%r39, %r36, 13;
	mul.lo.s32 	%r66, %r28, %r39;
	add.s32 	%r40, %r36, 12;
	mul.lo.s32 	%r65, %r28, %r40;
	add.s32 	%r41, %r36, 11;
	mul.lo.s32 	%r5, %r28, %r41;
	add.s32 	%r42, %r36, 10;
	mul.lo.s32 	%r6, %r28, %r42;
	add.s32 	%r43, %r36, 9;
	mul.lo.s32 	%r7, %r28, %r43;
	add.s32 	%r44, %r36, 8;
	mul.lo.s32 	%r8, %r28, %r44;
	add.s32 	%r45, %r36, 7;
	mul.lo.s32 	%r9, %r28, %r45;
	add.s32 	%r46, %r36, 6;
	mul.lo.s32 	%r10, %r28, %r46;
	add.s32 	%r47, %r36, 5;
	mul.lo.s32 	%r11, %r28, %r47;
	add.s32 	%r48, %r36, 4;
	mul.lo.s32 	%r12, %r28, %r48;
	add.s32 	%r49, %r36, 3;
	mul.lo.s32 	%r13, %r28, %r49;
	add.s32 	%r50, %r36, 2;
	mul.lo.s32 	%r14, %r28, %r50;
	add.s32 	%r51, %r36, 1;
	mul.lo.s32 	%r15, %r28, %r51;
	mul.lo.s32 	%r16, %r28, %r35;
	shl.b32 	%r17, %r16, 4;
	mov.u32 	%r69, 0;

BB26_2:
	add.s32 	%r52, %r17, %r69;
	mul.wide.u32 	%rd3, %r52, 16;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.v2.f64 	{%fd1, %fd2}, [%rd4];
	add.f64 	%fd4, %fd1, 0d0000000000000000;
	add.f64 	%fd6, %fd2, %fd4;
	add.s32 	%r53, %r15, %r69;
	mul.wide.u32 	%rd5, %r53, 16;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.v2.f64 	{%fd7, %fd8}, [%rd6];
	add.f64 	%fd10, %fd6, %fd7;
	add.f64 	%fd12, %fd8, %fd10;
	add.s32 	%r54, %r14, %r69;
	mul.wide.u32 	%rd7, %r54, 16;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.v2.f64 	{%fd13, %fd14}, [%rd8];
	add.f64 	%fd16, %fd12, %fd13;
	add.f64 	%fd18, %fd14, %fd16;
	add.s32 	%r55, %r13, %r69;
	mul.wide.u32 	%rd9, %r55, 16;
	add.s64 	%rd10, %rd1, %rd9;
	ld.global.v2.f64 	{%fd19, %fd20}, [%rd10];
	add.f64 	%fd22, %fd18, %fd19;
	add.f64 	%fd24, %fd20, %fd22;
	add.s32 	%r56, %r12, %r69;
	mul.wide.u32 	%rd11, %r56, 16;
	add.s64 	%rd12, %rd1, %rd11;
	ld.global.v2.f64 	{%fd25, %fd26}, [%rd12];
	add.f64 	%fd28, %fd24, %fd25;
	add.f64 	%fd30, %fd26, %fd28;
	add.s32 	%r57, %r11, %r69;
	mul.wide.u32 	%rd13, %r57, 16;
	add.s64 	%rd14, %rd1, %rd13;
	ld.global.v2.f64 	{%fd31, %fd32}, [%rd14];
	add.f64 	%fd34, %fd30, %fd31;
	add.f64 	%fd36, %fd32, %fd34;
	add.s32 	%r58, %r10, %r69;
	mul.wide.u32 	%rd15, %r58, 16;
	add.s64 	%rd16, %rd1, %rd15;
	ld.global.v2.f64 	{%fd37, %fd38}, [%rd16];
	add.f64 	%fd40, %fd36, %fd37;
	add.f64 	%fd42, %fd38, %fd40;
	add.s32 	%r59, %r9, %r69;
	mul.wide.u32 	%rd17, %r59, 16;
	add.s64 	%rd18, %rd1, %rd17;
	ld.global.v2.f64 	{%fd43, %fd44}, [%rd18];
	add.f64 	%fd46, %fd42, %fd43;
	add.f64 	%fd48, %fd44, %fd46;
	add.s32 	%r60, %r8, %r69;
	mul.wide.u32 	%rd19, %r60, 16;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.v2.f64 	{%fd49, %fd50}, [%rd20];
	add.f64 	%fd52, %fd48, %fd49;
	add.f64 	%fd54, %fd50, %fd52;
	add.s32 	%r61, %r7, %r69;
	mul.wide.u32 	%rd21, %r61, 16;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.v2.f64 	{%fd55, %fd56}, [%rd22];
	add.f64 	%fd58, %fd54, %fd55;
	add.f64 	%fd60, %fd56, %fd58;
	add.s32 	%r62, %r6, %r69;
	mul.wide.u32 	%rd23, %r62, 16;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.v2.f64 	{%fd61, %fd62}, [%rd24];
	add.f64 	%fd64, %fd60, %fd61;
	add.f64 	%fd66, %fd62, %fd64;
	add.s32 	%r63, %r5, %r69;
	mul.wide.u32 	%rd25, %r63, 16;
	add.s64 	%rd26, %rd1, %rd25;
	ld.global.v2.f64 	{%fd67, %fd68}, [%rd26];
	add.f64 	%fd70, %fd66, %fd67;
	add.f64 	%fd72, %fd68, %fd70;
	mul.wide.u32 	%rd27, %r65, 16;
	add.s64 	%rd28, %rd1, %rd27;
	ld.global.v2.f64 	{%fd73, %fd74}, [%rd28];
	add.f64 	%fd76, %fd72, %fd73;
	add.f64 	%fd78, %fd74, %fd76;
	mul.wide.u32 	%rd29, %r66, 16;
	add.s64 	%rd30, %rd1, %rd29;
	ld.global.v2.f64 	{%fd79, %fd80}, [%rd30];
	add.f64 	%fd82, %fd78, %fd79;
	add.f64 	%fd84, %fd80, %fd82;
	mul.wide.u32 	%rd31, %r67, 16;
	add.s64 	%rd32, %rd1, %rd31;
	ld.global.v2.f64 	{%fd85, %fd86}, [%rd32];
	add.f64 	%fd88, %fd84, %fd85;
	add.f64 	%fd90, %fd86, %fd88;
	mul.wide.u32 	%rd33, %r68, 16;
	add.s64 	%rd34, %rd1, %rd33;
	ld.global.v2.f64 	{%fd91, %fd92}, [%rd34];
	add.f64 	%fd94, %fd90, %fd91;
	add.f64 	%fd96, %fd92, %fd94;
	add.s32 	%r64, %r16, %r69;
	mul.wide.u32 	%rd35, %r64, 8;
	add.s64 	%rd36, %rd2, %rd35;
	st.global.f64 	[%rd36], %fd96;
	add.s32 	%r68, %r68, 1;
	add.s32 	%r67, %r67, 1;
	add.s32 	%r66, %r66, 1;
	add.s32 	%r65, %r65, 1;
	add.s32 	%r69, %r69, 1;
	setp.lt.u32	%p2, %r69, %r28;
	@%p2 bra 	BB26_2;

BB26_3:
	ret;
}

	// .globl	double2_sum2d_N_weighted
.entry double2_sum2d_N_weighted(
	.param .u64 .ptr .global .align 16 double2_sum2d_N_weighted_param_0,
	.param .u64 .ptr .global .align 16 double2_sum2d_N_weighted_param_1,
	.param .u32 double2_sum2d_N_weighted_param_2,
	.param .u32 double2_sum2d_N_weighted_param_3,
	.param .u32 double2_sum2d_N_weighted_param_4,
	.param .u32 double2_sum2d_N_weighted_param_5,
	.param .u64 .ptr .global .align 8 double2_sum2d_N_weighted_param_6
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<28>;
	.reg .f64 	%fd<14>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [double2_sum2d_N_weighted_param_1];
	ld.param.u32 	%r10, [double2_sum2d_N_weighted_param_2];
	ld.param.u32 	%r11, [double2_sum2d_N_weighted_param_3];
	ld.param.u32 	%r12, [double2_sum2d_N_weighted_param_4];
	ld.param.u32 	%r13, [double2_sum2d_N_weighted_param_5];
	ld.param.u64 	%rd2, [double2_sum2d_N_weighted_param_6];
	setp.eq.s32	%p1, %r10, 0;
	@%p1 bra 	BB27_5;

	mov.b32	%r15, %envreg3;
	mov.u32 	%r16, %ctaid.x;
	mov.u32 	%r17, %ntid.x;
	mad.lo.s32 	%r18, %r16, %r17, %r15;
	mov.u32 	%r19, %tid.x;
	add.s32 	%r20, %r18, %r19;
	mul.lo.s32 	%r21, %r20, %r10;
	add.s32 	%r1, %r21, %r12;
	mad.lo.s32 	%r2, %r21, %r13, %r11;
	mov.u32 	%r14, 0;
	mov.u32 	%r27, %r14;

BB27_2:
	add.s32 	%r24, %r2, %r27;
	setp.eq.s32	%p2, %r13, 0;
	mov.f64 	%fd13, 0d0000000000000000;
	mov.f64 	%fd12, %fd13;
	mov.u32 	%r26, %r14;
	@%p2 bra 	BB27_4;

BB27_3:
	mov.u32 	%r6, %r26;
	mul.wide.u32 	%rd3, %r24, 16;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.v2.f64 	{%fd6, %fd7}, [%rd4];
	add.f64 	%fd9, %fd13, %fd6;
	add.f64 	%fd13, %fd7, %fd9;
	add.s32 	%r24, %r24, %r10;
	add.s32 	%r8, %r6, 1;
	setp.lt.u32	%p3, %r8, %r13;
	mov.u32 	%r26, %r8;
	mov.f64 	%fd12, %fd13;
	@%p3 bra 	BB27_3;

BB27_4:
	add.s32 	%r23, %r1, %r27;
	mul.wide.u32 	%rd5, %r23, 8;
	add.s64 	%rd6, %rd2, %rd5;
	st.global.f64 	[%rd6], %fd12;
	add.s32 	%r27, %r27, 1;
	setp.lt.u32	%p4, %r27, %r10;
	@%p4 bra 	BB27_2;

BB27_5:
	ret;
}

	// .globl	float2_sum_16
.entry float2_sum_16(
	.param .u64 .ptr .global .align 8 float2_sum_16_param_0,
	.param .u64 .ptr .global .align 8 float2_sum_16_param_1
)
{
	.reg .f32 	%f<65>;
	.reg .b32 	%r<23>;
	.reg .f64 	%fd<65>;
	.reg .b64 	%rd<37>;


	ld.param.u64 	%rd1, [float2_sum_16_param_0];
	ld.param.u64 	%rd2, [float2_sum_16_param_1];
	mov.b32	%r1, %envreg3;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mad.lo.s32 	%r4, %r3, %r2, %r1;
	mov.u32 	%r5, %tid.x;
	add.s32 	%r6, %r4, %r5;
	shl.b32 	%r7, %r6, 4;
	mul.wide.u32 	%rd3, %r7, 8;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.v2.f32 	{%f1, %f2}, [%rd4];
	cvt.f64.f32	%fd1, %f1;
	add.f64 	%fd2, %fd1, 0d0000000000000000;
	cvt.f64.f32	%fd3, %f2;
	add.f64 	%fd4, %fd3, %fd2;
	add.s32 	%r8, %r7, 1;
	mul.wide.u32 	%rd5, %r8, 8;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.v2.f32 	{%f5, %f6}, [%rd6];
	cvt.f64.f32	%fd5, %f5;
	add.f64 	%fd6, %fd4, %fd5;
	cvt.f64.f32	%fd7, %f6;
	add.f64 	%fd8, %fd7, %fd6;
	add.s32 	%r9, %r7, 2;
	mul.wide.u32 	%rd7, %r9, 8;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.v2.f32 	{%f9, %f10}, [%rd8];
	cvt.f64.f32	%fd9, %f9;
	add.f64 	%fd10, %fd8, %fd9;
	cvt.f64.f32	%fd11, %f10;
	add.f64 	%fd12, %fd11, %fd10;
	add.s32 	%r10, %r7, 3;
	mul.wide.u32 	%rd9, %r10, 8;
	add.s64 	%rd10, %rd1, %rd9;
	ld.global.v2.f32 	{%f13, %f14}, [%rd10];
	cvt.f64.f32	%fd13, %f13;
	add.f64 	%fd14, %fd12, %fd13;
	cvt.f64.f32	%fd15, %f14;
	add.f64 	%fd16, %fd15, %fd14;
	add.s32 	%r11, %r7, 4;
	mul.wide.u32 	%rd11, %r11, 8;
	add.s64 	%rd12, %rd1, %rd11;
	ld.global.v2.f32 	{%f17, %f18}, [%rd12];
	cvt.f64.f32	%fd17, %f17;
	add.f64 	%fd18, %fd16, %fd17;
	cvt.f64.f32	%fd19, %f18;
	add.f64 	%fd20, %fd19, %fd18;
	add.s32 	%r12, %r7, 5;
	mul.wide.u32 	%rd13, %r12, 8;
	add.s64 	%rd14, %rd1, %rd13;
	ld.global.v2.f32 	{%f21, %f22}, [%rd14];
	cvt.f64.f32	%fd21, %f21;
	add.f64 	%fd22, %fd20, %fd21;
	cvt.f64.f32	%fd23, %f22;
	add.f64 	%fd24, %fd23, %fd22;
	add.s32 	%r13, %r7, 6;
	mul.wide.u32 	%rd15, %r13, 8;
	add.s64 	%rd16, %rd1, %rd15;
	ld.global.v2.f32 	{%f25, %f26}, [%rd16];
	cvt.f64.f32	%fd25, %f25;
	add.f64 	%fd26, %fd24, %fd25;
	cvt.f64.f32	%fd27, %f26;
	add.f64 	%fd28, %fd27, %fd26;
	add.s32 	%r14, %r7, 7;
	mul.wide.u32 	%rd17, %r14, 8;
	add.s64 	%rd18, %rd1, %rd17;
	ld.global.v2.f32 	{%f29, %f30}, [%rd18];
	cvt.f64.f32	%fd29, %f29;
	add.f64 	%fd30, %fd28, %fd29;
	cvt.f64.f32	%fd31, %f30;
	add.f64 	%fd32, %fd31, %fd30;
	add.s32 	%r15, %r7, 8;
	mul.wide.u32 	%rd19, %r15, 8;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.v2.f32 	{%f33, %f34}, [%rd20];
	cvt.f64.f32	%fd33, %f33;
	add.f64 	%fd34, %fd32, %fd33;
	cvt.f64.f32	%fd35, %f34;
	add.f64 	%fd36, %fd35, %fd34;
	add.s32 	%r16, %r7, 9;
	mul.wide.u32 	%rd21, %r16, 8;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.v2.f32 	{%f37, %f38}, [%rd22];
	cvt.f64.f32	%fd37, %f37;
	add.f64 	%fd38, %fd36, %fd37;
	cvt.f64.f32	%fd39, %f38;
	add.f64 	%fd40, %fd39, %fd38;
	add.s32 	%r17, %r7, 10;
	mul.wide.u32 	%rd23, %r17, 8;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.v2.f32 	{%f41, %f42}, [%rd24];
	cvt.f64.f32	%fd41, %f41;
	add.f64 	%fd42, %fd40, %fd41;
	cvt.f64.f32	%fd43, %f42;
	add.f64 	%fd44, %fd43, %fd42;
	add.s32 	%r18, %r7, 11;
	mul.wide.u32 	%rd25, %r18, 8;
	add.s64 	%rd26, %rd1, %rd25;
	ld.global.v2.f32 	{%f45, %f46}, [%rd26];
	cvt.f64.f32	%fd45, %f45;
	add.f64 	%fd46, %fd44, %fd45;
	cvt.f64.f32	%fd47, %f46;
	add.f64 	%fd48, %fd47, %fd46;
	add.s32 	%r19, %r7, 12;
	mul.wide.u32 	%rd27, %r19, 8;
	add.s64 	%rd28, %rd1, %rd27;
	ld.global.v2.f32 	{%f49, %f50}, [%rd28];
	cvt.f64.f32	%fd49, %f49;
	add.f64 	%fd50, %fd48, %fd49;
	cvt.f64.f32	%fd51, %f50;
	add.f64 	%fd52, %fd51, %fd50;
	add.s32 	%r20, %r7, 13;
	mul.wide.u32 	%rd29, %r20, 8;
	add.s64 	%rd30, %rd1, %rd29;
	ld.global.v2.f32 	{%f53, %f54}, [%rd30];
	cvt.f64.f32	%fd53, %f53;
	add.f64 	%fd54, %fd52, %fd53;
	cvt.f64.f32	%fd55, %f54;
	add.f64 	%fd56, %fd55, %fd54;
	add.s32 	%r21, %r7, 14;
	mul.wide.u32 	%rd31, %r21, 8;
	add.s64 	%rd32, %rd1, %rd31;
	ld.global.v2.f32 	{%f57, %f58}, [%rd32];
	cvt.f64.f32	%fd57, %f57;
	add.f64 	%fd58, %fd56, %fd57;
	cvt.f64.f32	%fd59, %f58;
	add.f64 	%fd60, %fd59, %fd58;
	add.s32 	%r22, %r7, 15;
	mul.wide.u32 	%rd33, %r22, 8;
	add.s64 	%rd34, %rd1, %rd33;
	ld.global.v2.f32 	{%f61, %f62}, [%rd34];
	cvt.f64.f32	%fd61, %f61;
	add.f64 	%fd62, %fd60, %fd61;
	cvt.f64.f32	%fd63, %f62;
	add.f64 	%fd64, %fd63, %fd62;
	mul.wide.u32 	%rd35, %r6, 8;
	add.s64 	%rd36, %rd2, %rd35;
	st.global.f64 	[%rd36], %fd64;
	ret;
}

	// .globl	float2_sum_N
.entry float2_sum_N(
	.param .u64 .ptr .global .align 8 float2_sum_N_param_0,
	.param .u32 float2_sum_N_param_1,
	.param .u32 float2_sum_N_param_2,
	.param .u32 float2_sum_N_param_3,
	.param .u64 .ptr .global .align 8 float2_sum_N_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<20>;
	.reg .f64 	%fd<10>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [float2_sum_N_param_0];
	ld.param.u32 	%r8, [float2_sum_N_param_1];
	ld.param.u32 	%r9, [float2_sum_N_param_2];
	ld.param.u32 	%r10, [float2_sum_N_param_3];
	ld.param.u64 	%rd2, [float2_sum_N_param_4];
	mov.b32	%r11, %envreg3;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %ntid.x;
	mad.lo.s32 	%r1, %r12, %r13, %r11;
	mov.u32 	%r2, %tid.x;
	setp.eq.s32	%p1, %r10, 0;
	mov.f64 	%fd9, 0d0000000000000000;
	@%p1 bra 	BB29_3;

	add.s32 	%r15, %r1, %r2;
	mad.lo.s32 	%r18, %r10, %r15, %r8;
	mov.u32 	%r19, 0;
	mov.f64 	%fd9, 0d0000000000000000;

BB29_2:
	mul.wide.u32 	%rd3, %r18, 8;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.v2.f32 	{%f1, %f2}, [%rd4];
	cvt.f64.f32	%fd6, %f1;
	add.f64 	%fd7, %fd9, %fd6;
	cvt.f64.f32	%fd8, %f2;
	add.f64 	%fd9, %fd8, %fd7;
	add.s32 	%r18, %r18, 1;
	add.s32 	%r19, %r19, 1;
	setp.lt.u32	%p2, %r19, %r10;
	@%p2 bra 	BB29_2;

BB29_3:
	add.s32 	%r16, %r1, %r2;
	add.s32 	%r17, %r16, %r9;
	mul.wide.u32 	%rd5, %r17, 8;
	add.s64 	%rd6, %rd2, %rd5;
	st.global.f64 	[%rd6], %fd9;
	ret;
}

	// .globl	float2_sumcomponents_16
.entry float2_sumcomponents_16(
	.param .u64 .ptr .global .align 8 float2_sumcomponents_16_param_0,
	.param .u64 .ptr .global .align 8 float2_sumcomponents_16_param_1,
	.param .u64 .ptr .global .align 8 float2_sumcomponents_16_param_2
)
{
	.reg .f32 	%f<65>;
	.reg .b32 	%r<23>;
	.reg .f64 	%fd<65>;
	.reg .b64 	%rd<39>;


	ld.param.u64 	%rd1, [float2_sumcomponents_16_param_0];
	ld.param.u64 	%rd2, [float2_sumcomponents_16_param_1];
	ld.param.u64 	%rd3, [float2_sumcomponents_16_param_2];
	mov.b32	%r1, %envreg3;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mad.lo.s32 	%r4, %r3, %r2, %r1;
	mov.u32 	%r5, %tid.x;
	add.s32 	%r6, %r4, %r5;
	shl.b32 	%r7, %r6, 4;
	mul.wide.u32 	%rd4, %r7, 8;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.v2.f32 	{%f1, %f2}, [%rd5];
	cvt.f64.f32	%fd1, %f1;
	add.f64 	%fd2, %fd1, 0d0000000000000000;
	cvt.f64.f32	%fd3, %f2;
	add.f64 	%fd4, %fd3, 0d0000000000000000;
	add.s32 	%r8, %r7, 1;
	mul.wide.u32 	%rd6, %r8, 8;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.v2.f32 	{%f5, %f6}, [%rd7];
	cvt.f64.f32	%fd5, %f5;
	add.f64 	%fd6, %fd2, %fd5;
	cvt.f64.f32	%fd7, %f6;
	add.f64 	%fd8, %fd4, %fd7;
	add.s32 	%r9, %r7, 2;
	mul.wide.u32 	%rd8, %r9, 8;
	add.s64 	%rd9, %rd1, %rd8;
	ld.global.v2.f32 	{%f9, %f10}, [%rd9];
	cvt.f64.f32	%fd9, %f9;
	add.f64 	%fd10, %fd6, %fd9;
	cvt.f64.f32	%fd11, %f10;
	add.f64 	%fd12, %fd8, %fd11;
	add.s32 	%r10, %r7, 3;
	mul.wide.u32 	%rd10, %r10, 8;
	add.s64 	%rd11, %rd1, %rd10;
	ld.global.v2.f32 	{%f13, %f14}, [%rd11];
	cvt.f64.f32	%fd13, %f13;
	add.f64 	%fd14, %fd10, %fd13;
	cvt.f64.f32	%fd15, %f14;
	add.f64 	%fd16, %fd12, %fd15;
	add.s32 	%r11, %r7, 4;
	mul.wide.u32 	%rd12, %r11, 8;
	add.s64 	%rd13, %rd1, %rd12;
	ld.global.v2.f32 	{%f17, %f18}, [%rd13];
	cvt.f64.f32	%fd17, %f17;
	add.f64 	%fd18, %fd14, %fd17;
	cvt.f64.f32	%fd19, %f18;
	add.f64 	%fd20, %fd16, %fd19;
	add.s32 	%r12, %r7, 5;
	mul.wide.u32 	%rd14, %r12, 8;
	add.s64 	%rd15, %rd1, %rd14;
	ld.global.v2.f32 	{%f21, %f22}, [%rd15];
	cvt.f64.f32	%fd21, %f21;
	add.f64 	%fd22, %fd18, %fd21;
	cvt.f64.f32	%fd23, %f22;
	add.f64 	%fd24, %fd20, %fd23;
	add.s32 	%r13, %r7, 6;
	mul.wide.u32 	%rd16, %r13, 8;
	add.s64 	%rd17, %rd1, %rd16;
	ld.global.v2.f32 	{%f25, %f26}, [%rd17];
	cvt.f64.f32	%fd25, %f25;
	add.f64 	%fd26, %fd22, %fd25;
	cvt.f64.f32	%fd27, %f26;
	add.f64 	%fd28, %fd24, %fd27;
	add.s32 	%r14, %r7, 7;
	mul.wide.u32 	%rd18, %r14, 8;
	add.s64 	%rd19, %rd1, %rd18;
	ld.global.v2.f32 	{%f29, %f30}, [%rd19];
	cvt.f64.f32	%fd29, %f29;
	add.f64 	%fd30, %fd26, %fd29;
	cvt.f64.f32	%fd31, %f30;
	add.f64 	%fd32, %fd28, %fd31;
	add.s32 	%r15, %r7, 8;
	mul.wide.u32 	%rd20, %r15, 8;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.v2.f32 	{%f33, %f34}, [%rd21];
	cvt.f64.f32	%fd33, %f33;
	add.f64 	%fd34, %fd30, %fd33;
	cvt.f64.f32	%fd35, %f34;
	add.f64 	%fd36, %fd32, %fd35;
	add.s32 	%r16, %r7, 9;
	mul.wide.u32 	%rd22, %r16, 8;
	add.s64 	%rd23, %rd1, %rd22;
	ld.global.v2.f32 	{%f37, %f38}, [%rd23];
	cvt.f64.f32	%fd37, %f37;
	add.f64 	%fd38, %fd34, %fd37;
	cvt.f64.f32	%fd39, %f38;
	add.f64 	%fd40, %fd36, %fd39;
	add.s32 	%r17, %r7, 10;
	mul.wide.u32 	%rd24, %r17, 8;
	add.s64 	%rd25, %rd1, %rd24;
	ld.global.v2.f32 	{%f41, %f42}, [%rd25];
	cvt.f64.f32	%fd41, %f41;
	add.f64 	%fd42, %fd38, %fd41;
	cvt.f64.f32	%fd43, %f42;
	add.f64 	%fd44, %fd40, %fd43;
	add.s32 	%r18, %r7, 11;
	mul.wide.u32 	%rd26, %r18, 8;
	add.s64 	%rd27, %rd1, %rd26;
	ld.global.v2.f32 	{%f45, %f46}, [%rd27];
	cvt.f64.f32	%fd45, %f45;
	add.f64 	%fd46, %fd42, %fd45;
	cvt.f64.f32	%fd47, %f46;
	add.f64 	%fd48, %fd44, %fd47;
	add.s32 	%r19, %r7, 12;
	mul.wide.u32 	%rd28, %r19, 8;
	add.s64 	%rd29, %rd1, %rd28;
	ld.global.v2.f32 	{%f49, %f50}, [%rd29];
	cvt.f64.f32	%fd49, %f49;
	add.f64 	%fd50, %fd46, %fd49;
	cvt.f64.f32	%fd51, %f50;
	add.f64 	%fd52, %fd48, %fd51;
	add.s32 	%r20, %r7, 13;
	mul.wide.u32 	%rd30, %r20, 8;
	add.s64 	%rd31, %rd1, %rd30;
	ld.global.v2.f32 	{%f53, %f54}, [%rd31];
	cvt.f64.f32	%fd53, %f53;
	add.f64 	%fd54, %fd50, %fd53;
	cvt.f64.f32	%fd55, %f54;
	add.f64 	%fd56, %fd52, %fd55;
	add.s32 	%r21, %r7, 14;
	mul.wide.u32 	%rd32, %r21, 8;
	add.s64 	%rd33, %rd1, %rd32;
	ld.global.v2.f32 	{%f57, %f58}, [%rd33];
	cvt.f64.f32	%fd57, %f57;
	add.f64 	%fd58, %fd54, %fd57;
	cvt.f64.f32	%fd59, %f58;
	add.f64 	%fd60, %fd56, %fd59;
	add.s32 	%r22, %r7, 15;
	mul.wide.u32 	%rd34, %r22, 8;
	add.s64 	%rd35, %rd1, %rd34;
	ld.global.v2.f32 	{%f61, %f62}, [%rd35];
	cvt.f64.f32	%fd61, %f61;
	add.f64 	%fd62, %fd58, %fd61;
	cvt.f64.f32	%fd63, %f62;
	add.f64 	%fd64, %fd60, %fd63;
	mul.wide.u32 	%rd36, %r6, 8;
	add.s64 	%rd37, %rd2, %rd36;
	st.global.f64 	[%rd37], %fd62;
	add.s64 	%rd38, %rd3, %rd36;
	st.global.f64 	[%rd38], %fd64;
	ret;
}

	// .globl	float2_sumcomponents_N
.entry float2_sumcomponents_N(
	.param .u64 .ptr .global .align 8 float2_sumcomponents_N_param_0,
	.param .u32 float2_sumcomponents_N_param_1,
	.param .u32 float2_sumcomponents_N_param_2,
	.param .u32 float2_sumcomponents_N_param_3,
	.param .u64 .ptr .global .align 8 float2_sumcomponents_N_param_4,
	.param .u64 .ptr .global .align 8 float2_sumcomponents_N_param_5
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<20>;
	.reg .f64 	%fd<15>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [float2_sumcomponents_N_param_0];
	ld.param.u32 	%r8, [float2_sumcomponents_N_param_1];
	ld.param.u32 	%r9, [float2_sumcomponents_N_param_2];
	ld.param.u32 	%r10, [float2_sumcomponents_N_param_3];
	ld.param.u64 	%rd2, [float2_sumcomponents_N_param_4];
	ld.param.u64 	%rd3, [float2_sumcomponents_N_param_5];
	mov.b32	%r11, %envreg3;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %ntid.x;
	mad.lo.s32 	%r1, %r12, %r13, %r11;
	mov.u32 	%r2, %tid.x;
	setp.eq.s32	%p1, %r10, 0;
	mov.f64 	%fd14, 0d0000000000000000;
	mov.f64 	%fd13, %fd14;
	@%p1 bra 	BB31_3;

	add.s32 	%r15, %r1, %r2;
	mad.lo.s32 	%r18, %r10, %r15, %r8;
	mov.u32 	%r19, 0;
	mov.f64 	%fd14, 0d0000000000000000;
	mov.f64 	%fd13, %fd14;

BB31_2:
	mul.wide.u32 	%rd4, %r18, 8;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.v2.f32 	{%f1, %f2}, [%rd5];
	cvt.f64.f32	%fd11, %f1;
	add.f64 	%fd14, %fd14, %fd11;
	cvt.f64.f32	%fd12, %f2;
	add.f64 	%fd13, %fd13, %fd12;
	add.s32 	%r18, %r18, 1;
	add.s32 	%r19, %r19, 1;
	setp.lt.u32	%p2, %r19, %r10;
	@%p2 bra 	BB31_2;

BB31_3:
	add.s32 	%r16, %r1, %r2;
	add.s32 	%r17, %r16, %r9;
	mul.wide.u32 	%rd6, %r17, 8;
	add.s64 	%rd7, %rd2, %rd6;
	st.global.f64 	[%rd7], %fd14;
	add.s64 	%rd8, %rd3, %rd6;
	st.global.f64 	[%rd8], %fd13;
	ret;
}

	// .globl	float2_sumcomponents_weighted_16
.entry float2_sumcomponents_weighted_16(
	.param .u64 .ptr .global .align 8 float2_sumcomponents_weighted_16_param_0,
	.param .u64 .ptr .global .align 4 float2_sumcomponents_weighted_16_param_1,
	.param .u64 .ptr .global .align 8 float2_sumcomponents_weighted_16_param_2,
	.param .u64 .ptr .global .align 8 float2_sumcomponents_weighted_16_param_3
)
{
	.reg .f32 	%f<113>;
	.reg .b32 	%r<23>;
	.reg .f64 	%fd<65>;
	.reg .b64 	%rd<72>;


	ld.param.u64 	%rd1, [float2_sumcomponents_weighted_16_param_0];
	ld.param.u64 	%rd2, [float2_sumcomponents_weighted_16_param_1];
	ld.param.u64 	%rd3, [float2_sumcomponents_weighted_16_param_2];
	ld.param.u64 	%rd4, [float2_sumcomponents_weighted_16_param_3];
	mov.b32	%r1, %envreg3;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mad.lo.s32 	%r4, %r3, %r2, %r1;
	mov.u32 	%r5, %tid.x;
	add.s32 	%r6, %r4, %r5;
	shl.b32 	%r7, %r6, 4;
	mul.wide.u32 	%rd5, %r7, 8;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.v2.f32 	{%f1, %f2}, [%rd6];
	mul.wide.u32 	%rd7, %r7, 4;
	add.s64 	%rd8, %rd2, %rd7;
	ld.global.f32 	%f4, [%rd8];
	mul.f32 	%f5, %f1, %f4;
	cvt.f64.f32	%fd1, %f5;
	add.f64 	%fd2, %fd1, 0d0000000000000000;
	mul.f32 	%f7, %f4, %f2;
	cvt.f64.f32	%fd3, %f7;
	add.f64 	%fd4, %fd3, 0d0000000000000000;
	add.s32 	%r8, %r7, 1;
	mul.wide.u32 	%rd9, %r8, 8;
	add.s64 	%rd10, %rd1, %rd9;
	ld.global.v2.f32 	{%f8, %f9}, [%rd10];
	mul.wide.u32 	%rd11, %r8, 4;
	add.s64 	%rd12, %rd2, %rd11;
	ld.global.f32 	%f11, [%rd12];
	mul.f32 	%f12, %f8, %f11;
	cvt.f64.f32	%fd5, %f12;
	add.f64 	%fd6, %fd2, %fd5;
	mul.f32 	%f14, %f11, %f9;
	cvt.f64.f32	%fd7, %f14;
	add.f64 	%fd8, %fd4, %fd7;
	add.s32 	%r9, %r7, 2;
	mul.wide.u32 	%rd13, %r9, 8;
	add.s64 	%rd14, %rd1, %rd13;
	ld.global.v2.f32 	{%f15, %f16}, [%rd14];
	mul.wide.u32 	%rd15, %r9, 4;
	add.s64 	%rd16, %rd2, %rd15;
	ld.global.f32 	%f18, [%rd16];
	mul.f32 	%f19, %f15, %f18;
	cvt.f64.f32	%fd9, %f19;
	add.f64 	%fd10, %fd6, %fd9;
	mul.f32 	%f21, %f18, %f16;
	cvt.f64.f32	%fd11, %f21;
	add.f64 	%fd12, %fd8, %fd11;
	add.s32 	%r10, %r7, 3;
	mul.wide.u32 	%rd17, %r10, 8;
	add.s64 	%rd18, %rd1, %rd17;
	ld.global.v2.f32 	{%f22, %f23}, [%rd18];
	mul.wide.u32 	%rd19, %r10, 4;
	add.s64 	%rd20, %rd2, %rd19;
	ld.global.f32 	%f25, [%rd20];
	mul.f32 	%f26, %f22, %f25;
	cvt.f64.f32	%fd13, %f26;
	add.f64 	%fd14, %fd10, %fd13;
	mul.f32 	%f28, %f25, %f23;
	cvt.f64.f32	%fd15, %f28;
	add.f64 	%fd16, %fd12, %fd15;
	add.s32 	%r11, %r7, 4;
	mul.wide.u32 	%rd21, %r11, 8;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.v2.f32 	{%f29, %f30}, [%rd22];
	mul.wide.u32 	%rd23, %r11, 4;
	add.s64 	%rd24, %rd2, %rd23;
	ld.global.f32 	%f32, [%rd24];
	mul.f32 	%f33, %f29, %f32;
	cvt.f64.f32	%fd17, %f33;
	add.f64 	%fd18, %fd14, %fd17;
	mul.f32 	%f35, %f32, %f30;
	cvt.f64.f32	%fd19, %f35;
	add.f64 	%fd20, %fd16, %fd19;
	add.s32 	%r12, %r7, 5;
	mul.wide.u32 	%rd25, %r12, 8;
	add.s64 	%rd26, %rd1, %rd25;
	ld.global.v2.f32 	{%f36, %f37}, [%rd26];
	mul.wide.u32 	%rd27, %r12, 4;
	add.s64 	%rd28, %rd2, %rd27;
	ld.global.f32 	%f39, [%rd28];
	mul.f32 	%f40, %f36, %f39;
	cvt.f64.f32	%fd21, %f40;
	add.f64 	%fd22, %fd18, %fd21;
	mul.f32 	%f42, %f39, %f37;
	cvt.f64.f32	%fd23, %f42;
	add.f64 	%fd24, %fd20, %fd23;
	add.s32 	%r13, %r7, 6;
	mul.wide.u32 	%rd29, %r13, 8;
	add.s64 	%rd30, %rd1, %rd29;
	ld.global.v2.f32 	{%f43, %f44}, [%rd30];
	mul.wide.u32 	%rd31, %r13, 4;
	add.s64 	%rd32, %rd2, %rd31;
	ld.global.f32 	%f46, [%rd32];
	mul.f32 	%f47, %f43, %f46;
	cvt.f64.f32	%fd25, %f47;
	add.f64 	%fd26, %fd22, %fd25;
	mul.f32 	%f49, %f46, %f44;
	cvt.f64.f32	%fd27, %f49;
	add.f64 	%fd28, %fd24, %fd27;
	add.s32 	%r14, %r7, 7;
	mul.wide.u32 	%rd33, %r14, 8;
	add.s64 	%rd34, %rd1, %rd33;
	ld.global.v2.f32 	{%f50, %f51}, [%rd34];
	mul.wide.u32 	%rd35, %r14, 4;
	add.s64 	%rd36, %rd2, %rd35;
	ld.global.f32 	%f53, [%rd36];
	mul.f32 	%f54, %f50, %f53;
	cvt.f64.f32	%fd29, %f54;
	add.f64 	%fd30, %fd26, %fd29;
	mul.f32 	%f56, %f53, %f51;
	cvt.f64.f32	%fd31, %f56;
	add.f64 	%fd32, %fd28, %fd31;
	add.s32 	%r15, %r7, 8;
	mul.wide.u32 	%rd37, %r15, 8;
	add.s64 	%rd38, %rd1, %rd37;
	ld.global.v2.f32 	{%f57, %f58}, [%rd38];
	mul.wide.u32 	%rd39, %r15, 4;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.f32 	%f60, [%rd40];
	mul.f32 	%f61, %f57, %f60;
	cvt.f64.f32	%fd33, %f61;
	add.f64 	%fd34, %fd30, %fd33;
	mul.f32 	%f63, %f60, %f58;
	cvt.f64.f32	%fd35, %f63;
	add.f64 	%fd36, %fd32, %fd35;
	add.s32 	%r16, %r7, 9;
	mul.wide.u32 	%rd41, %r16, 8;
	add.s64 	%rd42, %rd1, %rd41;
	ld.global.v2.f32 	{%f64, %f65}, [%rd42];
	mul.wide.u32 	%rd43, %r16, 4;
	add.s64 	%rd44, %rd2, %rd43;
	ld.global.f32 	%f67, [%rd44];
	mul.f32 	%f68, %f64, %f67;
	cvt.f64.f32	%fd37, %f68;
	add.f64 	%fd38, %fd34, %fd37;
	mul.f32 	%f70, %f67, %f65;
	cvt.f64.f32	%fd39, %f70;
	add.f64 	%fd40, %fd36, %fd39;
	add.s32 	%r17, %r7, 10;
	mul.wide.u32 	%rd45, %r17, 8;
	add.s64 	%rd46, %rd1, %rd45;
	ld.global.v2.f32 	{%f71, %f72}, [%rd46];
	mul.wide.u32 	%rd47, %r17, 4;
	add.s64 	%rd48, %rd2, %rd47;
	ld.global.f32 	%f74, [%rd48];
	mul.f32 	%f75, %f71, %f74;
	cvt.f64.f32	%fd41, %f75;
	add.f64 	%fd42, %fd38, %fd41;
	mul.f32 	%f77, %f74, %f72;
	cvt.f64.f32	%fd43, %f77;
	add.f64 	%fd44, %fd40, %fd43;
	add.s32 	%r18, %r7, 11;
	mul.wide.u32 	%rd49, %r18, 8;
	add.s64 	%rd50, %rd1, %rd49;
	ld.global.v2.f32 	{%f78, %f79}, [%rd50];
	mul.wide.u32 	%rd51, %r18, 4;
	add.s64 	%rd52, %rd2, %rd51;
	ld.global.f32 	%f81, [%rd52];
	mul.f32 	%f82, %f78, %f81;
	cvt.f64.f32	%fd45, %f82;
	add.f64 	%fd46, %fd42, %fd45;
	mul.f32 	%f84, %f81, %f79;
	cvt.f64.f32	%fd47, %f84;
	add.f64 	%fd48, %fd44, %fd47;
	add.s32 	%r19, %r7, 12;
	mul.wide.u32 	%rd53, %r19, 8;
	add.s64 	%rd54, %rd1, %rd53;
	ld.global.v2.f32 	{%f85, %f86}, [%rd54];
	mul.wide.u32 	%rd55, %r19, 4;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.f32 	%f88, [%rd56];
	mul.f32 	%f89, %f85, %f88;
	cvt.f64.f32	%fd49, %f89;
	add.f64 	%fd50, %fd46, %fd49;
	mul.f32 	%f91, %f88, %f86;
	cvt.f64.f32	%fd51, %f91;
	add.f64 	%fd52, %fd48, %fd51;
	add.s32 	%r20, %r7, 13;
	mul.wide.u32 	%rd57, %r20, 8;
	add.s64 	%rd58, %rd1, %rd57;
	ld.global.v2.f32 	{%f92, %f93}, [%rd58];
	mul.wide.u32 	%rd59, %r20, 4;
	add.s64 	%rd60, %rd2, %rd59;
	ld.global.f32 	%f95, [%rd60];
	mul.f32 	%f96, %f92, %f95;
	cvt.f64.f32	%fd53, %f96;
	add.f64 	%fd54, %fd50, %fd53;
	mul.f32 	%f98, %f95, %f93;
	cvt.f64.f32	%fd55, %f98;
	add.f64 	%fd56, %fd52, %fd55;
	add.s32 	%r21, %r7, 14;
	mul.wide.u32 	%rd61, %r21, 8;
	add.s64 	%rd62, %rd1, %rd61;
	ld.global.v2.f32 	{%f99, %f100}, [%rd62];
	mul.wide.u32 	%rd63, %r21, 4;
	add.s64 	%rd64, %rd2, %rd63;
	ld.global.f32 	%f102, [%rd64];
	mul.f32 	%f103, %f99, %f102;
	cvt.f64.f32	%fd57, %f103;
	add.f64 	%fd58, %fd54, %fd57;
	mul.f32 	%f105, %f102, %f100;
	cvt.f64.f32	%fd59, %f105;
	add.f64 	%fd60, %fd56, %fd59;
	add.s32 	%r22, %r7, 15;
	mul.wide.u32 	%rd65, %r22, 8;
	add.s64 	%rd66, %rd1, %rd65;
	ld.global.v2.f32 	{%f106, %f107}, [%rd66];
	mul.wide.u32 	%rd67, %r22, 4;
	add.s64 	%rd68, %rd2, %rd67;
	ld.global.f32 	%f109, [%rd68];
	mul.f32 	%f110, %f106, %f109;
	cvt.f64.f32	%fd61, %f110;
	add.f64 	%fd62, %fd58, %fd61;
	mul.f32 	%f112, %f109, %f107;
	cvt.f64.f32	%fd63, %f112;
	add.f64 	%fd64, %fd60, %fd63;
	mul.wide.u32 	%rd69, %r6, 8;
	add.s64 	%rd70, %rd3, %rd69;
	st.global.f64 	[%rd70], %fd62;
	add.s64 	%rd71, %rd4, %rd69;
	st.global.f64 	[%rd71], %fd64;
	ret;
}

	// .globl	float2_sumcomponents_weighted_N
.entry float2_sumcomponents_weighted_N(
	.param .u64 .ptr .global .align 8 float2_sumcomponents_weighted_N_param_0,
	.param .u64 .ptr .global .align 4 float2_sumcomponents_weighted_N_param_1,
	.param .u32 float2_sumcomponents_weighted_N_param_2,
	.param .u32 float2_sumcomponents_weighted_N_param_3,
	.param .u32 float2_sumcomponents_weighted_N_param_4,
	.param .u64 .ptr .global .align 8 float2_sumcomponents_weighted_N_param_5,
	.param .u64 .ptr .global .align 8 float2_sumcomponents_weighted_N_param_6
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<8>;
	.reg .b32 	%r<24>;
	.reg .f64 	%fd<15>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [float2_sumcomponents_weighted_N_param_0];
	ld.param.u64 	%rd2, [float2_sumcomponents_weighted_N_param_1];
	ld.param.u32 	%r11, [float2_sumcomponents_weighted_N_param_2];
	ld.param.u32 	%r12, [float2_sumcomponents_weighted_N_param_3];
	ld.param.u32 	%r13, [float2_sumcomponents_weighted_N_param_4];
	ld.param.u64 	%rd3, [float2_sumcomponents_weighted_N_param_5];
	ld.param.u64 	%rd4, [float2_sumcomponents_weighted_N_param_6];
	mov.b32	%r14, %envreg3;
	mov.u32 	%r15, %ctaid.x;
	mov.u32 	%r16, %ntid.x;
	mad.lo.s32 	%r1, %r15, %r16, %r14;
	mov.u32 	%r2, %tid.x;
	setp.eq.s32	%p1, %r13, 0;
	mov.f64 	%fd14, 0d0000000000000000;
	mov.f64 	%fd13, %fd14;
	@%p1 bra 	BB33_3;

	add.s32 	%r18, %r1, %r2;
	mul.lo.s32 	%r21, %r13, %r18;
	add.s32 	%r22, %r11, %r21;
	mov.u32 	%r23, 0;
	mov.f64 	%fd14, 0d0000000000000000;
	mov.f64 	%fd13, %fd14;

BB33_2:
	mul.wide.u32 	%rd5, %r21, 8;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.v2.f32 	{%f1, %f2}, [%rd6];
	mul.wide.u32 	%rd7, %r22, 4;
	add.s64 	%rd8, %rd2, %rd7;
	ld.global.f32 	%f4, [%rd8];
	mul.f32 	%f5, %f1, %f4;
	cvt.f64.f32	%fd11, %f5;
	add.f64 	%fd14, %fd14, %fd11;
	mul.f32 	%f7, %f4, %f2;
	cvt.f64.f32	%fd12, %f7;
	add.f64 	%fd13, %fd13, %fd12;
	add.s32 	%r22, %r22, 1;
	add.s32 	%r21, %r21, 1;
	add.s32 	%r23, %r23, 1;
	setp.lt.u32	%p2, %r23, %r13;
	@%p2 bra 	BB33_2;

BB33_3:
	add.s32 	%r19, %r1, %r2;
	add.s32 	%r20, %r19, %r12;
	mul.wide.u32 	%rd9, %r20, 8;
	add.s64 	%rd10, %rd3, %rd9;
	st.global.f64 	[%rd10], %fd14;
	add.s64 	%rd11, %rd4, %rd9;
	st.global.f64 	[%rd11], %fd13;
	ret;
}

	// .globl	float2_sum2d_16
.entry float2_sum2d_16(
	.param .u64 .ptr .global .align 8 float2_sum2d_16_param_0,
	.param .u32 float2_sum2d_16_param_1,
	.param .u64 .ptr .global .align 8 float2_sum2d_16_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<65>;
	.reg .b32 	%r<70>;
	.reg .f64 	%fd<65>;
	.reg .b64 	%rd<37>;


	ld.param.u64 	%rd1, [float2_sum2d_16_param_0];
	ld.param.u32 	%r28, [float2_sum2d_16_param_1];
	ld.param.u64 	%rd2, [float2_sum2d_16_param_2];
	setp.eq.s32	%p1, %r28, 0;
	@%p1 bra 	BB34_3;

	mov.b32	%r30, %envreg3;
	mov.u32 	%r31, %ntid.x;
	mov.u32 	%r32, %ctaid.x;
	mov.u32 	%r33, %tid.x;
	add.s32 	%r34, %r30, %r33;
	mad.lo.s32 	%r35, %r32, %r31, %r34;
	shl.b32 	%r36, %r35, 4;
	add.s32 	%r37, %r36, 15;
	mul.lo.s32 	%r68, %r28, %r37;
	add.s32 	%r38, %r36, 14;
	mul.lo.s32 	%r67, %r28, %r38;
	add.s32 	%r39, %r36, 13;
	mul.lo.s32 	%r66, %r28, %r39;
	add.s32 	%r40, %r36, 12;
	mul.lo.s32 	%r65, %r28, %r40;
	add.s32 	%r41, %r36, 11;
	mul.lo.s32 	%r5, %r28, %r41;
	add.s32 	%r42, %r36, 10;
	mul.lo.s32 	%r6, %r28, %r42;
	add.s32 	%r43, %r36, 9;
	mul.lo.s32 	%r7, %r28, %r43;
	add.s32 	%r44, %r36, 8;
	mul.lo.s32 	%r8, %r28, %r44;
	add.s32 	%r45, %r36, 7;
	mul.lo.s32 	%r9, %r28, %r45;
	add.s32 	%r46, %r36, 6;
	mul.lo.s32 	%r10, %r28, %r46;
	add.s32 	%r47, %r36, 5;
	mul.lo.s32 	%r11, %r28, %r47;
	add.s32 	%r48, %r36, 4;
	mul.lo.s32 	%r12, %r28, %r48;
	add.s32 	%r49, %r36, 3;
	mul.lo.s32 	%r13, %r28, %r49;
	add.s32 	%r50, %r36, 2;
	mul.lo.s32 	%r14, %r28, %r50;
	add.s32 	%r51, %r36, 1;
	mul.lo.s32 	%r15, %r28, %r51;
	mul.lo.s32 	%r16, %r28, %r35;
	shl.b32 	%r17, %r16, 4;
	mov.u32 	%r69, 0;

BB34_2:
	add.s32 	%r52, %r17, %r69;
	mul.wide.u32 	%rd3, %r52, 8;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.v2.f32 	{%f1, %f2}, [%rd4];
	cvt.f64.f32	%fd1, %f1;
	add.f64 	%fd2, %fd1, 0d0000000000000000;
	cvt.f64.f32	%fd3, %f2;
	add.f64 	%fd4, %fd3, %fd2;
	add.s32 	%r53, %r15, %r69;
	mul.wide.u32 	%rd5, %r53, 8;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.v2.f32 	{%f5, %f6}, [%rd6];
	cvt.f64.f32	%fd5, %f5;
	add.f64 	%fd6, %fd4, %fd5;
	cvt.f64.f32	%fd7, %f6;
	add.f64 	%fd8, %fd7, %fd6;
	add.s32 	%r54, %r14, %r69;
	mul.wide.u32 	%rd7, %r54, 8;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.v2.f32 	{%f9, %f10}, [%rd8];
	cvt.f64.f32	%fd9, %f9;
	add.f64 	%fd10, %fd8, %fd9;
	cvt.f64.f32	%fd11, %f10;
	add.f64 	%fd12, %fd11, %fd10;
	add.s32 	%r55, %r13, %r69;
	mul.wide.u32 	%rd9, %r55, 8;
	add.s64 	%rd10, %rd1, %rd9;
	ld.global.v2.f32 	{%f13, %f14}, [%rd10];
	cvt.f64.f32	%fd13, %f13;
	add.f64 	%fd14, %fd12, %fd13;
	cvt.f64.f32	%fd15, %f14;
	add.f64 	%fd16, %fd15, %fd14;
	add.s32 	%r56, %r12, %r69;
	mul.wide.u32 	%rd11, %r56, 8;
	add.s64 	%rd12, %rd1, %rd11;
	ld.global.v2.f32 	{%f17, %f18}, [%rd12];
	cvt.f64.f32	%fd17, %f17;
	add.f64 	%fd18, %fd16, %fd17;
	cvt.f64.f32	%fd19, %f18;
	add.f64 	%fd20, %fd19, %fd18;
	add.s32 	%r57, %r11, %r69;
	mul.wide.u32 	%rd13, %r57, 8;
	add.s64 	%rd14, %rd1, %rd13;
	ld.global.v2.f32 	{%f21, %f22}, [%rd14];
	cvt.f64.f32	%fd21, %f21;
	add.f64 	%fd22, %fd20, %fd21;
	cvt.f64.f32	%fd23, %f22;
	add.f64 	%fd24, %fd23, %fd22;
	add.s32 	%r58, %r10, %r69;
	mul.wide.u32 	%rd15, %r58, 8;
	add.s64 	%rd16, %rd1, %rd15;
	ld.global.v2.f32 	{%f25, %f26}, [%rd16];
	cvt.f64.f32	%fd25, %f25;
	add.f64 	%fd26, %fd24, %fd25;
	cvt.f64.f32	%fd27, %f26;
	add.f64 	%fd28, %fd27, %fd26;
	add.s32 	%r59, %r9, %r69;
	mul.wide.u32 	%rd17, %r59, 8;
	add.s64 	%rd18, %rd1, %rd17;
	ld.global.v2.f32 	{%f29, %f30}, [%rd18];
	cvt.f64.f32	%fd29, %f29;
	add.f64 	%fd30, %fd28, %fd29;
	cvt.f64.f32	%fd31, %f30;
	add.f64 	%fd32, %fd31, %fd30;
	add.s32 	%r60, %r8, %r69;
	mul.wide.u32 	%rd19, %r60, 8;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.v2.f32 	{%f33, %f34}, [%rd20];
	cvt.f64.f32	%fd33, %f33;
	add.f64 	%fd34, %fd32, %fd33;
	cvt.f64.f32	%fd35, %f34;
	add.f64 	%fd36, %fd35, %fd34;
	add.s32 	%r61, %r7, %r69;
	mul.wide.u32 	%rd21, %r61, 8;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.v2.f32 	{%f37, %f38}, [%rd22];
	cvt.f64.f32	%fd37, %f37;
	add.f64 	%fd38, %fd36, %fd37;
	cvt.f64.f32	%fd39, %f38;
	add.f64 	%fd40, %fd39, %fd38;
	add.s32 	%r62, %r6, %r69;
	mul.wide.u32 	%rd23, %r62, 8;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.v2.f32 	{%f41, %f42}, [%rd24];
	cvt.f64.f32	%fd41, %f41;
	add.f64 	%fd42, %fd40, %fd41;
	cvt.f64.f32	%fd43, %f42;
	add.f64 	%fd44, %fd43, %fd42;
	add.s32 	%r63, %r5, %r69;
	mul.wide.u32 	%rd25, %r63, 8;
	add.s64 	%rd26, %rd1, %rd25;
	ld.global.v2.f32 	{%f45, %f46}, [%rd26];
	cvt.f64.f32	%fd45, %f45;
	add.f64 	%fd46, %fd44, %fd45;
	cvt.f64.f32	%fd47, %f46;
	add.f64 	%fd48, %fd47, %fd46;
	mul.wide.u32 	%rd27, %r65, 8;
	add.s64 	%rd28, %rd1, %rd27;
	ld.global.v2.f32 	{%f49, %f50}, [%rd28];
	cvt.f64.f32	%fd49, %f49;
	add.f64 	%fd50, %fd48, %fd49;
	cvt.f64.f32	%fd51, %f50;
	add.f64 	%fd52, %fd51, %fd50;
	mul.wide.u32 	%rd29, %r66, 8;
	add.s64 	%rd30, %rd1, %rd29;
	ld.global.v2.f32 	{%f53, %f54}, [%rd30];
	cvt.f64.f32	%fd53, %f53;
	add.f64 	%fd54, %fd52, %fd53;
	cvt.f64.f32	%fd55, %f54;
	add.f64 	%fd56, %fd55, %fd54;
	mul.wide.u32 	%rd31, %r67, 8;
	add.s64 	%rd32, %rd1, %rd31;
	ld.global.v2.f32 	{%f57, %f58}, [%rd32];
	cvt.f64.f32	%fd57, %f57;
	add.f64 	%fd58, %fd56, %fd57;
	cvt.f64.f32	%fd59, %f58;
	add.f64 	%fd60, %fd59, %fd58;
	mul.wide.u32 	%rd33, %r68, 8;
	add.s64 	%rd34, %rd1, %rd33;
	ld.global.v2.f32 	{%f61, %f62}, [%rd34];
	cvt.f64.f32	%fd61, %f61;
	add.f64 	%fd62, %fd60, %fd61;
	cvt.f64.f32	%fd63, %f62;
	add.f64 	%fd64, %fd63, %fd62;
	add.s32 	%r64, %r16, %r69;
	mul.wide.u32 	%rd35, %r64, 8;
	add.s64 	%rd36, %rd2, %rd35;
	st.global.f64 	[%rd36], %fd64;
	add.s32 	%r68, %r68, 1;
	add.s32 	%r67, %r67, 1;
	add.s32 	%r66, %r66, 1;
	add.s32 	%r65, %r65, 1;
	add.s32 	%r69, %r69, 1;
	setp.lt.u32	%p2, %r69, %r28;
	@%p2 bra 	BB34_2;

BB34_3:
	ret;
}

	// .globl	float2_sum2d_N
.entry float2_sum2d_N(
	.param .u64 .ptr .global .align 8 float2_sum2d_N_param_0,
	.param .u32 float2_sum2d_N_param_1,
	.param .u32 float2_sum2d_N_param_2,
	.param .u32 float2_sum2d_N_param_3,
	.param .u32 float2_sum2d_N_param_4,
	.param .u64 .ptr .global .align 8 float2_sum2d_N_param_5
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<28>;
	.reg .f64 	%fd<12>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [float2_sum2d_N_param_0];
	ld.param.u32 	%r10, [float2_sum2d_N_param_1];
	ld.param.u32 	%r11, [float2_sum2d_N_param_2];
	ld.param.u32 	%r12, [float2_sum2d_N_param_3];
	ld.param.u32 	%r13, [float2_sum2d_N_param_4];
	ld.param.u64 	%rd2, [float2_sum2d_N_param_5];
	setp.eq.s32	%p1, %r10, 0;
	@%p1 bra 	BB35_5;

	mov.b32	%r15, %envreg3;
	mov.u32 	%r16, %ctaid.x;
	mov.u32 	%r17, %ntid.x;
	mad.lo.s32 	%r18, %r16, %r17, %r15;
	mov.u32 	%r19, %tid.x;
	add.s32 	%r20, %r18, %r19;
	mul.lo.s32 	%r21, %r20, %r10;
	add.s32 	%r1, %r21, %r12;
	mad.lo.s32 	%r2, %r21, %r13, %r11;
	mov.u32 	%r14, 0;
	mov.u32 	%r27, %r14;

BB35_2:
	add.s32 	%r24, %r2, %r27;
	setp.eq.s32	%p2, %r13, 0;
	mov.f64 	%fd11, 0d0000000000000000;
	mov.f64 	%fd10, %fd11;
	mov.u32 	%r26, %r14;
	@%p2 bra 	BB35_4;

BB35_3:
	mov.u32 	%r6, %r26;
	mul.wide.u32 	%rd3, %r24, 8;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.v2.f32 	{%f1, %f2}, [%rd4];
	cvt.f64.f32	%fd6, %f1;
	add.f64 	%fd7, %fd11, %fd6;
	cvt.f64.f32	%fd8, %f2;
	add.f64 	%fd11, %fd8, %fd7;
	add.s32 	%r24, %r24, %r10;
	add.s32 	%r8, %r6, 1;
	setp.lt.u32	%p3, %r8, %r13;
	mov.u32 	%r26, %r8;
	mov.f64 	%fd10, %fd11;
	@%p3 bra 	BB35_3;

BB35_4:
	add.s32 	%r23, %r1, %r27;
	mul.wide.u32 	%rd5, %r23, 8;
	add.s64 	%rd6, %rd2, %rd5;
	st.global.f64 	[%rd6], %fd10;
	add.s32 	%r27, %r27, 1;
	setp.lt.u32	%p4, %r27, %r10;
	@%p4 bra 	BB35_2;

BB35_5:
	ret;
}

	// .globl	float2_sum2d_16_weighted
.entry float2_sum2d_16_weighted(
	.param .u64 .ptr .global .align 16 float2_sum2d_16_weighted_param_0,
	.param .u64 .ptr .global .align 8 float2_sum2d_16_weighted_param_1,
	.param .u32 float2_sum2d_16_weighted_param_2,
	.param .u64 .ptr .global .align 8 float2_sum2d_16_weighted_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<65>;
	.reg .b32 	%r<70>;
	.reg .f64 	%fd<65>;
	.reg .b64 	%rd<37>;


	ld.param.u64 	%rd1, [float2_sum2d_16_weighted_param_1];
	ld.param.u32 	%r28, [float2_sum2d_16_weighted_param_2];
	ld.param.u64 	%rd2, [float2_sum2d_16_weighted_param_3];
	setp.eq.s32	%p1, %r28, 0;
	@%p1 bra 	BB36_3;

	mov.b32	%r30, %envreg3;
	mov.u32 	%r31, %ntid.x;
	mov.u32 	%r32, %ctaid.x;
	mov.u32 	%r33, %tid.x;
	add.s32 	%r34, %r30, %r33;
	mad.lo.s32 	%r35, %r32, %r31, %r34;
	shl.b32 	%r36, %r35, 4;
	add.s32 	%r37, %r36, 15;
	mul.lo.s32 	%r68, %r28, %r37;
	add.s32 	%r38, %r36, 14;
	mul.lo.s32 	%r67, %r28, %r38;
	add.s32 	%r39, %r36, 13;
	mul.lo.s32 	%r66, %r28, %r39;
	add.s32 	%r40, %r36, 12;
	mul.lo.s32 	%r65, %r28, %r40;
	add.s32 	%r41, %r36, 11;
	mul.lo.s32 	%r5, %r28, %r41;
	add.s32 	%r42, %r36, 10;
	mul.lo.s32 	%r6, %r28, %r42;
	add.s32 	%r43, %r36, 9;
	mul.lo.s32 	%r7, %r28, %r43;
	add.s32 	%r44, %r36, 8;
	mul.lo.s32 	%r8, %r28, %r44;
	add.s32 	%r45, %r36, 7;
	mul.lo.s32 	%r9, %r28, %r45;
	add.s32 	%r46, %r36, 6;
	mul.lo.s32 	%r10, %r28, %r46;
	add.s32 	%r47, %r36, 5;
	mul.lo.s32 	%r11, %r28, %r47;
	add.s32 	%r48, %r36, 4;
	mul.lo.s32 	%r12, %r28, %r48;
	add.s32 	%r49, %r36, 3;
	mul.lo.s32 	%r13, %r28, %r49;
	add.s32 	%r50, %r36, 2;
	mul.lo.s32 	%r14, %r28, %r50;
	add.s32 	%r51, %r36, 1;
	mul.lo.s32 	%r15, %r28, %r51;
	mul.lo.s32 	%r16, %r28, %r35;
	shl.b32 	%r17, %r16, 4;
	mov.u32 	%r69, 0;

BB36_2:
	add.s32 	%r52, %r17, %r69;
	mul.wide.u32 	%rd3, %r52, 8;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.v2.f32 	{%f1, %f2}, [%rd4];
	cvt.f64.f32	%fd1, %f1;
	add.f64 	%fd2, %fd1, 0d0000000000000000;
	cvt.f64.f32	%fd3, %f2;
	add.f64 	%fd4, %fd3, %fd2;
	add.s32 	%r53, %r15, %r69;
	mul.wide.u32 	%rd5, %r53, 8;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.v2.f32 	{%f5, %f6}, [%rd6];
	cvt.f64.f32	%fd5, %f5;
	add.f64 	%fd6, %fd4, %fd5;
	cvt.f64.f32	%fd7, %f6;
	add.f64 	%fd8, %fd7, %fd6;
	add.s32 	%r54, %r14, %r69;
	mul.wide.u32 	%rd7, %r54, 8;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.v2.f32 	{%f9, %f10}, [%rd8];
	cvt.f64.f32	%fd9, %f9;
	add.f64 	%fd10, %fd8, %fd9;
	cvt.f64.f32	%fd11, %f10;
	add.f64 	%fd12, %fd11, %fd10;
	add.s32 	%r55, %r13, %r69;
	mul.wide.u32 	%rd9, %r55, 8;
	add.s64 	%rd10, %rd1, %rd9;
	ld.global.v2.f32 	{%f13, %f14}, [%rd10];
	cvt.f64.f32	%fd13, %f13;
	add.f64 	%fd14, %fd12, %fd13;
	cvt.f64.f32	%fd15, %f14;
	add.f64 	%fd16, %fd15, %fd14;
	add.s32 	%r56, %r12, %r69;
	mul.wide.u32 	%rd11, %r56, 8;
	add.s64 	%rd12, %rd1, %rd11;
	ld.global.v2.f32 	{%f17, %f18}, [%rd12];
	cvt.f64.f32	%fd17, %f17;
	add.f64 	%fd18, %fd16, %fd17;
	cvt.f64.f32	%fd19, %f18;
	add.f64 	%fd20, %fd19, %fd18;
	add.s32 	%r57, %r11, %r69;
	mul.wide.u32 	%rd13, %r57, 8;
	add.s64 	%rd14, %rd1, %rd13;
	ld.global.v2.f32 	{%f21, %f22}, [%rd14];
	cvt.f64.f32	%fd21, %f21;
	add.f64 	%fd22, %fd20, %fd21;
	cvt.f64.f32	%fd23, %f22;
	add.f64 	%fd24, %fd23, %fd22;
	add.s32 	%r58, %r10, %r69;
	mul.wide.u32 	%rd15, %r58, 8;
	add.s64 	%rd16, %rd1, %rd15;
	ld.global.v2.f32 	{%f25, %f26}, [%rd16];
	cvt.f64.f32	%fd25, %f25;
	add.f64 	%fd26, %fd24, %fd25;
	cvt.f64.f32	%fd27, %f26;
	add.f64 	%fd28, %fd27, %fd26;
	add.s32 	%r59, %r9, %r69;
	mul.wide.u32 	%rd17, %r59, 8;
	add.s64 	%rd18, %rd1, %rd17;
	ld.global.v2.f32 	{%f29, %f30}, [%rd18];
	cvt.f64.f32	%fd29, %f29;
	add.f64 	%fd30, %fd28, %fd29;
	cvt.f64.f32	%fd31, %f30;
	add.f64 	%fd32, %fd31, %fd30;
	add.s32 	%r60, %r8, %r69;
	mul.wide.u32 	%rd19, %r60, 8;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.v2.f32 	{%f33, %f34}, [%rd20];
	cvt.f64.f32	%fd33, %f33;
	add.f64 	%fd34, %fd32, %fd33;
	cvt.f64.f32	%fd35, %f34;
	add.f64 	%fd36, %fd35, %fd34;
	add.s32 	%r61, %r7, %r69;
	mul.wide.u32 	%rd21, %r61, 8;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.v2.f32 	{%f37, %f38}, [%rd22];
	cvt.f64.f32	%fd37, %f37;
	add.f64 	%fd38, %fd36, %fd37;
	cvt.f64.f32	%fd39, %f38;
	add.f64 	%fd40, %fd39, %fd38;
	add.s32 	%r62, %r6, %r69;
	mul.wide.u32 	%rd23, %r62, 8;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.v2.f32 	{%f41, %f42}, [%rd24];
	cvt.f64.f32	%fd41, %f41;
	add.f64 	%fd42, %fd40, %fd41;
	cvt.f64.f32	%fd43, %f42;
	add.f64 	%fd44, %fd43, %fd42;
	add.s32 	%r63, %r5, %r69;
	mul.wide.u32 	%rd25, %r63, 8;
	add.s64 	%rd26, %rd1, %rd25;
	ld.global.v2.f32 	{%f45, %f46}, [%rd26];
	cvt.f64.f32	%fd45, %f45;
	add.f64 	%fd46, %fd44, %fd45;
	cvt.f64.f32	%fd47, %f46;
	add.f64 	%fd48, %fd47, %fd46;
	mul.wide.u32 	%rd27, %r65, 8;
	add.s64 	%rd28, %rd1, %rd27;
	ld.global.v2.f32 	{%f49, %f50}, [%rd28];
	cvt.f64.f32	%fd49, %f49;
	add.f64 	%fd50, %fd48, %fd49;
	cvt.f64.f32	%fd51, %f50;
	add.f64 	%fd52, %fd51, %fd50;
	mul.wide.u32 	%rd29, %r66, 8;
	add.s64 	%rd30, %rd1, %rd29;
	ld.global.v2.f32 	{%f53, %f54}, [%rd30];
	cvt.f64.f32	%fd53, %f53;
	add.f64 	%fd54, %fd52, %fd53;
	cvt.f64.f32	%fd55, %f54;
	add.f64 	%fd56, %fd55, %fd54;
	mul.wide.u32 	%rd31, %r67, 8;
	add.s64 	%rd32, %rd1, %rd31;
	ld.global.v2.f32 	{%f57, %f58}, [%rd32];
	cvt.f64.f32	%fd57, %f57;
	add.f64 	%fd58, %fd56, %fd57;
	cvt.f64.f32	%fd59, %f58;
	add.f64 	%fd60, %fd59, %fd58;
	mul.wide.u32 	%rd33, %r68, 8;
	add.s64 	%rd34, %rd1, %rd33;
	ld.global.v2.f32 	{%f61, %f62}, [%rd34];
	cvt.f64.f32	%fd61, %f61;
	add.f64 	%fd62, %fd60, %fd61;
	cvt.f64.f32	%fd63, %f62;
	add.f64 	%fd64, %fd63, %fd62;
	add.s32 	%r64, %r16, %r69;
	mul.wide.u32 	%rd35, %r64, 8;
	add.s64 	%rd36, %rd2, %rd35;
	st.global.f64 	[%rd36], %fd64;
	add.s32 	%r68, %r68, 1;
	add.s32 	%r67, %r67, 1;
	add.s32 	%r66, %r66, 1;
	add.s32 	%r65, %r65, 1;
	add.s32 	%r69, %r69, 1;
	setp.lt.u32	%p2, %r69, %r28;
	@%p2 bra 	BB36_2;

BB36_3:
	ret;
}

	// .globl	float2_sum2d_N_weighted
.entry float2_sum2d_N_weighted(
	.param .u64 .ptr .global .align 16 float2_sum2d_N_weighted_param_0,
	.param .u64 .ptr .global .align 8 float2_sum2d_N_weighted_param_1,
	.param .u32 float2_sum2d_N_weighted_param_2,
	.param .u32 float2_sum2d_N_weighted_param_3,
	.param .u32 float2_sum2d_N_weighted_param_4,
	.param .u32 float2_sum2d_N_weighted_param_5,
	.param .u64 .ptr .global .align 8 float2_sum2d_N_weighted_param_6
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<28>;
	.reg .f64 	%fd<12>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [float2_sum2d_N_weighted_param_1];
	ld.param.u32 	%r10, [float2_sum2d_N_weighted_param_2];
	ld.param.u32 	%r11, [float2_sum2d_N_weighted_param_3];
	ld.param.u32 	%r12, [float2_sum2d_N_weighted_param_4];
	ld.param.u32 	%r13, [float2_sum2d_N_weighted_param_5];
	ld.param.u64 	%rd2, [float2_sum2d_N_weighted_param_6];
	setp.eq.s32	%p1, %r10, 0;
	@%p1 bra 	BB37_5;

	mov.b32	%r15, %envreg3;
	mov.u32 	%r16, %ctaid.x;
	mov.u32 	%r17, %ntid.x;
	mad.lo.s32 	%r18, %r16, %r17, %r15;
	mov.u32 	%r19, %tid.x;
	add.s32 	%r20, %r18, %r19;
	mul.lo.s32 	%r21, %r20, %r10;
	add.s32 	%r1, %r21, %r12;
	mad.lo.s32 	%r2, %r21, %r13, %r11;
	mov.u32 	%r14, 0;
	mov.u32 	%r27, %r14;

BB37_2:
	add.s32 	%r24, %r2, %r27;
	setp.eq.s32	%p2, %r13, 0;
	mov.f64 	%fd11, 0d0000000000000000;
	mov.f64 	%fd10, %fd11;
	mov.u32 	%r26, %r14;
	@%p2 bra 	BB37_4;

BB37_3:
	mov.u32 	%r6, %r26;
	mul.wide.u32 	%rd3, %r24, 8;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.v2.f32 	{%f1, %f2}, [%rd4];
	cvt.f64.f32	%fd6, %f1;
	add.f64 	%fd7, %fd11, %fd6;
	cvt.f64.f32	%fd8, %f2;
	add.f64 	%fd11, %fd8, %fd7;
	add.s32 	%r24, %r24, %r10;
	add.s32 	%r8, %r6, 1;
	setp.lt.u32	%p3, %r8, %r13;
	mov.u32 	%r26, %r8;
	mov.f64 	%fd10, %fd11;
	@%p3 bra 	BB37_3;

BB37_4:
	add.s32 	%r23, %r1, %r27;
	mul.wide.u32 	%rd5, %r23, 8;
	add.s64 	%rd6, %rd2, %rd5;
	st.global.f64 	[%rd6], %fd10;
	add.s32 	%r27, %r27, 1;
	setp.lt.u32	%p4, %r27, %r10;
	@%p4 bra 	BB37_2;

BB37_5:
	ret;
}

	// .globl	float4_sum_16
.entry float4_sum_16(
	.param .u64 .ptr .global .align 16 float4_sum_16_param_0,
	.param .u64 .ptr .global .align 8 float4_sum_16_param_1
)
{
	.reg .f32 	%f<129>;
	.reg .b32 	%r<23>;
	.reg .f64 	%fd<129>;
	.reg .b64 	%rd<37>;


	ld.param.u64 	%rd1, [float4_sum_16_param_0];
	ld.param.u64 	%rd2, [float4_sum_16_param_1];
	mov.b32	%r1, %envreg3;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mad.lo.s32 	%r4, %r3, %r2, %r1;
	mov.u32 	%r5, %tid.x;
	add.s32 	%r6, %r4, %r5;
	shl.b32 	%r7, %r6, 4;
	mul.wide.u32 	%rd3, %r7, 16;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd4];
	cvt.f64.f32	%fd1, %f1;
	add.f64 	%fd2, %fd1, 0d0000000000000000;
	cvt.f64.f32	%fd3, %f2;
	add.f64 	%fd4, %fd3, %fd2;
	cvt.f64.f32	%fd5, %f3;
	add.f64 	%fd6, %fd5, %fd4;
	cvt.f64.f32	%fd7, %f4;
	add.f64 	%fd8, %fd7, %fd6;
	add.s32 	%r8, %r7, 1;
	mul.wide.u32 	%rd5, %r8, 16;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.v4.f32 	{%f9, %f10, %f11, %f12}, [%rd6];
	cvt.f64.f32	%fd9, %f9;
	add.f64 	%fd10, %fd8, %fd9;
	cvt.f64.f32	%fd11, %f10;
	add.f64 	%fd12, %fd11, %fd10;
	cvt.f64.f32	%fd13, %f11;
	add.f64 	%fd14, %fd13, %fd12;
	cvt.f64.f32	%fd15, %f12;
	add.f64 	%fd16, %fd15, %fd14;
	add.s32 	%r9, %r7, 2;
	mul.wide.u32 	%rd7, %r9, 16;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.v4.f32 	{%f17, %f18, %f19, %f20}, [%rd8];
	cvt.f64.f32	%fd17, %f17;
	add.f64 	%fd18, %fd16, %fd17;
	cvt.f64.f32	%fd19, %f18;
	add.f64 	%fd20, %fd19, %fd18;
	cvt.f64.f32	%fd21, %f19;
	add.f64 	%fd22, %fd21, %fd20;
	cvt.f64.f32	%fd23, %f20;
	add.f64 	%fd24, %fd23, %fd22;
	add.s32 	%r10, %r7, 3;
	mul.wide.u32 	%rd9, %r10, 16;
	add.s64 	%rd10, %rd1, %rd9;
	ld.global.v4.f32 	{%f25, %f26, %f27, %f28}, [%rd10];
	cvt.f64.f32	%fd25, %f25;
	add.f64 	%fd26, %fd24, %fd25;
	cvt.f64.f32	%fd27, %f26;
	add.f64 	%fd28, %fd27, %fd26;
	cvt.f64.f32	%fd29, %f27;
	add.f64 	%fd30, %fd29, %fd28;
	cvt.f64.f32	%fd31, %f28;
	add.f64 	%fd32, %fd31, %fd30;
	add.s32 	%r11, %r7, 4;
	mul.wide.u32 	%rd11, %r11, 16;
	add.s64 	%rd12, %rd1, %rd11;
	ld.global.v4.f32 	{%f33, %f34, %f35, %f36}, [%rd12];
	cvt.f64.f32	%fd33, %f33;
	add.f64 	%fd34, %fd32, %fd33;
	cvt.f64.f32	%fd35, %f34;
	add.f64 	%fd36, %fd35, %fd34;
	cvt.f64.f32	%fd37, %f35;
	add.f64 	%fd38, %fd37, %fd36;
	cvt.f64.f32	%fd39, %f36;
	add.f64 	%fd40, %fd39, %fd38;
	add.s32 	%r12, %r7, 5;
	mul.wide.u32 	%rd13, %r12, 16;
	add.s64 	%rd14, %rd1, %rd13;
	ld.global.v4.f32 	{%f41, %f42, %f43, %f44}, [%rd14];
	cvt.f64.f32	%fd41, %f41;
	add.f64 	%fd42, %fd40, %fd41;
	cvt.f64.f32	%fd43, %f42;
	add.f64 	%fd44, %fd43, %fd42;
	cvt.f64.f32	%fd45, %f43;
	add.f64 	%fd46, %fd45, %fd44;
	cvt.f64.f32	%fd47, %f44;
	add.f64 	%fd48, %fd47, %fd46;
	add.s32 	%r13, %r7, 6;
	mul.wide.u32 	%rd15, %r13, 16;
	add.s64 	%rd16, %rd1, %rd15;
	ld.global.v4.f32 	{%f49, %f50, %f51, %f52}, [%rd16];
	cvt.f64.f32	%fd49, %f49;
	add.f64 	%fd50, %fd48, %fd49;
	cvt.f64.f32	%fd51, %f50;
	add.f64 	%fd52, %fd51, %fd50;
	cvt.f64.f32	%fd53, %f51;
	add.f64 	%fd54, %fd53, %fd52;
	cvt.f64.f32	%fd55, %f52;
	add.f64 	%fd56, %fd55, %fd54;
	add.s32 	%r14, %r7, 7;
	mul.wide.u32 	%rd17, %r14, 16;
	add.s64 	%rd18, %rd1, %rd17;
	ld.global.v4.f32 	{%f57, %f58, %f59, %f60}, [%rd18];
	cvt.f64.f32	%fd57, %f57;
	add.f64 	%fd58, %fd56, %fd57;
	cvt.f64.f32	%fd59, %f58;
	add.f64 	%fd60, %fd59, %fd58;
	cvt.f64.f32	%fd61, %f59;
	add.f64 	%fd62, %fd61, %fd60;
	cvt.f64.f32	%fd63, %f60;
	add.f64 	%fd64, %fd63, %fd62;
	add.s32 	%r15, %r7, 8;
	mul.wide.u32 	%rd19, %r15, 16;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.v4.f32 	{%f65, %f66, %f67, %f68}, [%rd20];
	cvt.f64.f32	%fd65, %f65;
	add.f64 	%fd66, %fd64, %fd65;
	cvt.f64.f32	%fd67, %f66;
	add.f64 	%fd68, %fd67, %fd66;
	cvt.f64.f32	%fd69, %f67;
	add.f64 	%fd70, %fd69, %fd68;
	cvt.f64.f32	%fd71, %f68;
	add.f64 	%fd72, %fd71, %fd70;
	add.s32 	%r16, %r7, 9;
	mul.wide.u32 	%rd21, %r16, 16;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.v4.f32 	{%f73, %f74, %f75, %f76}, [%rd22];
	cvt.f64.f32	%fd73, %f73;
	add.f64 	%fd74, %fd72, %fd73;
	cvt.f64.f32	%fd75, %f74;
	add.f64 	%fd76, %fd75, %fd74;
	cvt.f64.f32	%fd77, %f75;
	add.f64 	%fd78, %fd77, %fd76;
	cvt.f64.f32	%fd79, %f76;
	add.f64 	%fd80, %fd79, %fd78;
	add.s32 	%r17, %r7, 10;
	mul.wide.u32 	%rd23, %r17, 16;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.v4.f32 	{%f81, %f82, %f83, %f84}, [%rd24];
	cvt.f64.f32	%fd81, %f81;
	add.f64 	%fd82, %fd80, %fd81;
	cvt.f64.f32	%fd83, %f82;
	add.f64 	%fd84, %fd83, %fd82;
	cvt.f64.f32	%fd85, %f83;
	add.f64 	%fd86, %fd85, %fd84;
	cvt.f64.f32	%fd87, %f84;
	add.f64 	%fd88, %fd87, %fd86;
	add.s32 	%r18, %r7, 11;
	mul.wide.u32 	%rd25, %r18, 16;
	add.s64 	%rd26, %rd1, %rd25;
	ld.global.v4.f32 	{%f89, %f90, %f91, %f92}, [%rd26];
	cvt.f64.f32	%fd89, %f89;
	add.f64 	%fd90, %fd88, %fd89;
	cvt.f64.f32	%fd91, %f90;
	add.f64 	%fd92, %fd91, %fd90;
	cvt.f64.f32	%fd93, %f91;
	add.f64 	%fd94, %fd93, %fd92;
	cvt.f64.f32	%fd95, %f92;
	add.f64 	%fd96, %fd95, %fd94;
	add.s32 	%r19, %r7, 12;
	mul.wide.u32 	%rd27, %r19, 16;
	add.s64 	%rd28, %rd1, %rd27;
	ld.global.v4.f32 	{%f97, %f98, %f99, %f100}, [%rd28];
	cvt.f64.f32	%fd97, %f97;
	add.f64 	%fd98, %fd96, %fd97;
	cvt.f64.f32	%fd99, %f98;
	add.f64 	%fd100, %fd99, %fd98;
	cvt.f64.f32	%fd101, %f99;
	add.f64 	%fd102, %fd101, %fd100;
	cvt.f64.f32	%fd103, %f100;
	add.f64 	%fd104, %fd103, %fd102;
	add.s32 	%r20, %r7, 13;
	mul.wide.u32 	%rd29, %r20, 16;
	add.s64 	%rd30, %rd1, %rd29;
	ld.global.v4.f32 	{%f105, %f106, %f107, %f108}, [%rd30];
	cvt.f64.f32	%fd105, %f105;
	add.f64 	%fd106, %fd104, %fd105;
	cvt.f64.f32	%fd107, %f106;
	add.f64 	%fd108, %fd107, %fd106;
	cvt.f64.f32	%fd109, %f107;
	add.f64 	%fd110, %fd109, %fd108;
	cvt.f64.f32	%fd111, %f108;
	add.f64 	%fd112, %fd111, %fd110;
	add.s32 	%r21, %r7, 14;
	mul.wide.u32 	%rd31, %r21, 16;
	add.s64 	%rd32, %rd1, %rd31;
	ld.global.v4.f32 	{%f113, %f114, %f115, %f116}, [%rd32];
	cvt.f64.f32	%fd113, %f113;
	add.f64 	%fd114, %fd112, %fd113;
	cvt.f64.f32	%fd115, %f114;
	add.f64 	%fd116, %fd115, %fd114;
	cvt.f64.f32	%fd117, %f115;
	add.f64 	%fd118, %fd117, %fd116;
	cvt.f64.f32	%fd119, %f116;
	add.f64 	%fd120, %fd119, %fd118;
	add.s32 	%r22, %r7, 15;
	mul.wide.u32 	%rd33, %r22, 16;
	add.s64 	%rd34, %rd1, %rd33;
	ld.global.v4.f32 	{%f121, %f122, %f123, %f124}, [%rd34];
	cvt.f64.f32	%fd121, %f121;
	add.f64 	%fd122, %fd120, %fd121;
	cvt.f64.f32	%fd123, %f122;
	add.f64 	%fd124, %fd123, %fd122;
	cvt.f64.f32	%fd125, %f123;
	add.f64 	%fd126, %fd125, %fd124;
	cvt.f64.f32	%fd127, %f124;
	add.f64 	%fd128, %fd127, %fd126;
	mul.wide.u32 	%rd35, %r6, 8;
	add.s64 	%rd36, %rd2, %rd35;
	st.global.f64 	[%rd36], %fd128;
	ret;
}

	// .globl	float4_sum_N
.entry float4_sum_N(
	.param .u64 .ptr .global .align 16 float4_sum_N_param_0,
	.param .u32 float4_sum_N_param_1,
	.param .u32 float4_sum_N_param_2,
	.param .u32 float4_sum_N_param_3,
	.param .u64 .ptr .global .align 8 float4_sum_N_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<20>;
	.reg .f64 	%fd<14>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [float4_sum_N_param_0];
	ld.param.u32 	%r8, [float4_sum_N_param_1];
	ld.param.u32 	%r9, [float4_sum_N_param_2];
	ld.param.u32 	%r10, [float4_sum_N_param_3];
	ld.param.u64 	%rd2, [float4_sum_N_param_4];
	mov.b32	%r11, %envreg3;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %ntid.x;
	mad.lo.s32 	%r1, %r12, %r13, %r11;
	mov.u32 	%r2, %tid.x;
	setp.eq.s32	%p1, %r10, 0;
	mov.f64 	%fd13, 0d0000000000000000;
	@%p1 bra 	BB39_3;

	add.s32 	%r15, %r1, %r2;
	mad.lo.s32 	%r18, %r10, %r15, %r8;
	mov.u32 	%r19, 0;
	mov.f64 	%fd13, 0d0000000000000000;

BB39_2:
	mul.wide.u32 	%rd3, %r18, 16;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd4];
	cvt.f64.f32	%fd6, %f1;
	add.f64 	%fd7, %fd13, %fd6;
	cvt.f64.f32	%fd8, %f2;
	add.f64 	%fd9, %fd8, %fd7;
	cvt.f64.f32	%fd10, %f3;
	add.f64 	%fd11, %fd10, %fd9;
	cvt.f64.f32	%fd12, %f4;
	add.f64 	%fd13, %fd12, %fd11;
	add.s32 	%r18, %r18, 1;
	add.s32 	%r19, %r19, 1;
	setp.lt.u32	%p2, %r19, %r10;
	@%p2 bra 	BB39_2;

BB39_3:
	add.s32 	%r16, %r1, %r2;
	add.s32 	%r17, %r16, %r9;
	mul.wide.u32 	%rd5, %r17, 8;
	add.s64 	%rd6, %rd2, %rd5;
	st.global.f64 	[%rd6], %fd13;
	ret;
}

	// .globl	float4_sumcomponents_16
.entry float4_sumcomponents_16(
	.param .u64 .ptr .global .align 16 float4_sumcomponents_16_param_0,
	.param .u64 .ptr .global .align 8 float4_sumcomponents_16_param_1,
	.param .u64 .ptr .global .align 8 float4_sumcomponents_16_param_2,
	.param .u64 .ptr .global .align 8 float4_sumcomponents_16_param_3,
	.param .u64 .ptr .global .align 8 float4_sumcomponents_16_param_4
)
{
	.reg .f32 	%f<129>;
	.reg .b32 	%r<23>;
	.reg .f64 	%fd<129>;
	.reg .b64 	%rd<43>;


	ld.param.u64 	%rd1, [float4_sumcomponents_16_param_0];
	ld.param.u64 	%rd2, [float4_sumcomponents_16_param_1];
	ld.param.u64 	%rd3, [float4_sumcomponents_16_param_2];
	ld.param.u64 	%rd4, [float4_sumcomponents_16_param_3];
	ld.param.u64 	%rd5, [float4_sumcomponents_16_param_4];
	mov.b32	%r1, %envreg3;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mad.lo.s32 	%r4, %r3, %r2, %r1;
	mov.u32 	%r5, %tid.x;
	add.s32 	%r6, %r4, %r5;
	shl.b32 	%r7, %r6, 4;
	mul.wide.u32 	%rd6, %r7, 16;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd7];
	cvt.f64.f32	%fd1, %f1;
	add.f64 	%fd2, %fd1, 0d0000000000000000;
	cvt.f64.f32	%fd3, %f2;
	add.f64 	%fd4, %fd3, 0d0000000000000000;
	cvt.f64.f32	%fd5, %f3;
	add.f64 	%fd6, %fd5, 0d0000000000000000;
	cvt.f64.f32	%fd7, %f4;
	add.f64 	%fd8, %fd7, 0d0000000000000000;
	add.s32 	%r8, %r7, 1;
	mul.wide.u32 	%rd8, %r8, 16;
	add.s64 	%rd9, %rd1, %rd8;
	ld.global.v4.f32 	{%f9, %f10, %f11, %f12}, [%rd9];
	cvt.f64.f32	%fd9, %f9;
	add.f64 	%fd10, %fd2, %fd9;
	cvt.f64.f32	%fd11, %f10;
	add.f64 	%fd12, %fd4, %fd11;
	cvt.f64.f32	%fd13, %f11;
	add.f64 	%fd14, %fd6, %fd13;
	cvt.f64.f32	%fd15, %f12;
	add.f64 	%fd16, %fd8, %fd15;
	add.s32 	%r9, %r7, 2;
	mul.wide.u32 	%rd10, %r9, 16;
	add.s64 	%rd11, %rd1, %rd10;
	ld.global.v4.f32 	{%f17, %f18, %f19, %f20}, [%rd11];
	cvt.f64.f32	%fd17, %f17;
	add.f64 	%fd18, %fd10, %fd17;
	cvt.f64.f32	%fd19, %f18;
	add.f64 	%fd20, %fd12, %fd19;
	cvt.f64.f32	%fd21, %f19;
	add.f64 	%fd22, %fd14, %fd21;
	cvt.f64.f32	%fd23, %f20;
	add.f64 	%fd24, %fd16, %fd23;
	add.s32 	%r10, %r7, 3;
	mul.wide.u32 	%rd12, %r10, 16;
	add.s64 	%rd13, %rd1, %rd12;
	ld.global.v4.f32 	{%f25, %f26, %f27, %f28}, [%rd13];
	cvt.f64.f32	%fd25, %f25;
	add.f64 	%fd26, %fd18, %fd25;
	cvt.f64.f32	%fd27, %f26;
	add.f64 	%fd28, %fd20, %fd27;
	cvt.f64.f32	%fd29, %f27;
	add.f64 	%fd30, %fd22, %fd29;
	cvt.f64.f32	%fd31, %f28;
	add.f64 	%fd32, %fd24, %fd31;
	add.s32 	%r11, %r7, 4;
	mul.wide.u32 	%rd14, %r11, 16;
	add.s64 	%rd15, %rd1, %rd14;
	ld.global.v4.f32 	{%f33, %f34, %f35, %f36}, [%rd15];
	cvt.f64.f32	%fd33, %f33;
	add.f64 	%fd34, %fd26, %fd33;
	cvt.f64.f32	%fd35, %f34;
	add.f64 	%fd36, %fd28, %fd35;
	cvt.f64.f32	%fd37, %f35;
	add.f64 	%fd38, %fd30, %fd37;
	cvt.f64.f32	%fd39, %f36;
	add.f64 	%fd40, %fd32, %fd39;
	add.s32 	%r12, %r7, 5;
	mul.wide.u32 	%rd16, %r12, 16;
	add.s64 	%rd17, %rd1, %rd16;
	ld.global.v4.f32 	{%f41, %f42, %f43, %f44}, [%rd17];
	cvt.f64.f32	%fd41, %f41;
	add.f64 	%fd42, %fd34, %fd41;
	cvt.f64.f32	%fd43, %f42;
	add.f64 	%fd44, %fd36, %fd43;
	cvt.f64.f32	%fd45, %f43;
	add.f64 	%fd46, %fd38, %fd45;
	cvt.f64.f32	%fd47, %f44;
	add.f64 	%fd48, %fd40, %fd47;
	add.s32 	%r13, %r7, 6;
	mul.wide.u32 	%rd18, %r13, 16;
	add.s64 	%rd19, %rd1, %rd18;
	ld.global.v4.f32 	{%f49, %f50, %f51, %f52}, [%rd19];
	cvt.f64.f32	%fd49, %f49;
	add.f64 	%fd50, %fd42, %fd49;
	cvt.f64.f32	%fd51, %f50;
	add.f64 	%fd52, %fd44, %fd51;
	cvt.f64.f32	%fd53, %f51;
	add.f64 	%fd54, %fd46, %fd53;
	cvt.f64.f32	%fd55, %f52;
	add.f64 	%fd56, %fd48, %fd55;
	add.s32 	%r14, %r7, 7;
	mul.wide.u32 	%rd20, %r14, 16;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.v4.f32 	{%f57, %f58, %f59, %f60}, [%rd21];
	cvt.f64.f32	%fd57, %f57;
	add.f64 	%fd58, %fd50, %fd57;
	cvt.f64.f32	%fd59, %f58;
	add.f64 	%fd60, %fd52, %fd59;
	cvt.f64.f32	%fd61, %f59;
	add.f64 	%fd62, %fd54, %fd61;
	cvt.f64.f32	%fd63, %f60;
	add.f64 	%fd64, %fd56, %fd63;
	add.s32 	%r15, %r7, 8;
	mul.wide.u32 	%rd22, %r15, 16;
	add.s64 	%rd23, %rd1, %rd22;
	ld.global.v4.f32 	{%f65, %f66, %f67, %f68}, [%rd23];
	cvt.f64.f32	%fd65, %f65;
	add.f64 	%fd66, %fd58, %fd65;
	cvt.f64.f32	%fd67, %f66;
	add.f64 	%fd68, %fd60, %fd67;
	cvt.f64.f32	%fd69, %f67;
	add.f64 	%fd70, %fd62, %fd69;
	cvt.f64.f32	%fd71, %f68;
	add.f64 	%fd72, %fd64, %fd71;
	add.s32 	%r16, %r7, 9;
	mul.wide.u32 	%rd24, %r16, 16;
	add.s64 	%rd25, %rd1, %rd24;
	ld.global.v4.f32 	{%f73, %f74, %f75, %f76}, [%rd25];
	cvt.f64.f32	%fd73, %f73;
	add.f64 	%fd74, %fd66, %fd73;
	cvt.f64.f32	%fd75, %f74;
	add.f64 	%fd76, %fd68, %fd75;
	cvt.f64.f32	%fd77, %f75;
	add.f64 	%fd78, %fd70, %fd77;
	cvt.f64.f32	%fd79, %f76;
	add.f64 	%fd80, %fd72, %fd79;
	add.s32 	%r17, %r7, 10;
	mul.wide.u32 	%rd26, %r17, 16;
	add.s64 	%rd27, %rd1, %rd26;
	ld.global.v4.f32 	{%f81, %f82, %f83, %f84}, [%rd27];
	cvt.f64.f32	%fd81, %f81;
	add.f64 	%fd82, %fd74, %fd81;
	cvt.f64.f32	%fd83, %f82;
	add.f64 	%fd84, %fd76, %fd83;
	cvt.f64.f32	%fd85, %f83;
	add.f64 	%fd86, %fd78, %fd85;
	cvt.f64.f32	%fd87, %f84;
	add.f64 	%fd88, %fd80, %fd87;
	add.s32 	%r18, %r7, 11;
	mul.wide.u32 	%rd28, %r18, 16;
	add.s64 	%rd29, %rd1, %rd28;
	ld.global.v4.f32 	{%f89, %f90, %f91, %f92}, [%rd29];
	cvt.f64.f32	%fd89, %f89;
	add.f64 	%fd90, %fd82, %fd89;
	cvt.f64.f32	%fd91, %f90;
	add.f64 	%fd92, %fd84, %fd91;
	cvt.f64.f32	%fd93, %f91;
	add.f64 	%fd94, %fd86, %fd93;
	cvt.f64.f32	%fd95, %f92;
	add.f64 	%fd96, %fd88, %fd95;
	add.s32 	%r19, %r7, 12;
	mul.wide.u32 	%rd30, %r19, 16;
	add.s64 	%rd31, %rd1, %rd30;
	ld.global.v4.f32 	{%f97, %f98, %f99, %f100}, [%rd31];
	cvt.f64.f32	%fd97, %f97;
	add.f64 	%fd98, %fd90, %fd97;
	cvt.f64.f32	%fd99, %f98;
	add.f64 	%fd100, %fd92, %fd99;
	cvt.f64.f32	%fd101, %f99;
	add.f64 	%fd102, %fd94, %fd101;
	cvt.f64.f32	%fd103, %f100;
	add.f64 	%fd104, %fd96, %fd103;
	add.s32 	%r20, %r7, 13;
	mul.wide.u32 	%rd32, %r20, 16;
	add.s64 	%rd33, %rd1, %rd32;
	ld.global.v4.f32 	{%f105, %f106, %f107, %f108}, [%rd33];
	cvt.f64.f32	%fd105, %f105;
	add.f64 	%fd106, %fd98, %fd105;
	cvt.f64.f32	%fd107, %f106;
	add.f64 	%fd108, %fd100, %fd107;
	cvt.f64.f32	%fd109, %f107;
	add.f64 	%fd110, %fd102, %fd109;
	cvt.f64.f32	%fd111, %f108;
	add.f64 	%fd112, %fd104, %fd111;
	add.s32 	%r21, %r7, 14;
	mul.wide.u32 	%rd34, %r21, 16;
	add.s64 	%rd35, %rd1, %rd34;
	ld.global.v4.f32 	{%f113, %f114, %f115, %f116}, [%rd35];
	cvt.f64.f32	%fd113, %f113;
	add.f64 	%fd114, %fd106, %fd113;
	cvt.f64.f32	%fd115, %f114;
	add.f64 	%fd116, %fd108, %fd115;
	cvt.f64.f32	%fd117, %f115;
	add.f64 	%fd118, %fd110, %fd117;
	cvt.f64.f32	%fd119, %f116;
	add.f64 	%fd120, %fd112, %fd119;
	add.s32 	%r22, %r7, 15;
	mul.wide.u32 	%rd36, %r22, 16;
	add.s64 	%rd37, %rd1, %rd36;
	ld.global.v4.f32 	{%f121, %f122, %f123, %f124}, [%rd37];
	cvt.f64.f32	%fd121, %f121;
	add.f64 	%fd122, %fd114, %fd121;
	cvt.f64.f32	%fd123, %f122;
	add.f64 	%fd124, %fd116, %fd123;
	cvt.f64.f32	%fd125, %f123;
	add.f64 	%fd126, %fd118, %fd125;
	cvt.f64.f32	%fd127, %f124;
	add.f64 	%fd128, %fd120, %fd127;
	mul.wide.u32 	%rd38, %r6, 8;
	add.s64 	%rd39, %rd2, %rd38;
	st.global.f64 	[%rd39], %fd122;
	add.s64 	%rd40, %rd3, %rd38;
	st.global.f64 	[%rd40], %fd124;
	add.s64 	%rd41, %rd4, %rd38;
	st.global.f64 	[%rd41], %fd126;
	add.s64 	%rd42, %rd5, %rd38;
	st.global.f64 	[%rd42], %fd128;
	ret;
}

	// .globl	float4_sumcomponents_N
.entry float4_sumcomponents_N(
	.param .u64 .ptr .global .align 16 float4_sumcomponents_N_param_0,
	.param .u32 float4_sumcomponents_N_param_1,
	.param .u32 float4_sumcomponents_N_param_2,
	.param .u32 float4_sumcomponents_N_param_3,
	.param .u64 .ptr .global .align 8 float4_sumcomponents_N_param_4,
	.param .u64 .ptr .global .align 8 float4_sumcomponents_N_param_5,
	.param .u64 .ptr .global .align 8 float4_sumcomponents_N_param_6,
	.param .u64 .ptr .global .align 8 float4_sumcomponents_N_param_7
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<20>;
	.reg .f64 	%fd<29>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd1, [float4_sumcomponents_N_param_0];
	ld.param.u32 	%r8, [float4_sumcomponents_N_param_1];
	ld.param.u32 	%r9, [float4_sumcomponents_N_param_2];
	ld.param.u32 	%r10, [float4_sumcomponents_N_param_3];
	ld.param.u64 	%rd2, [float4_sumcomponents_N_param_4];
	ld.param.u64 	%rd3, [float4_sumcomponents_N_param_5];
	ld.param.u64 	%rd4, [float4_sumcomponents_N_param_6];
	ld.param.u64 	%rd5, [float4_sumcomponents_N_param_7];
	mov.b32	%r11, %envreg3;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %ntid.x;
	mad.lo.s32 	%r1, %r12, %r13, %r11;
	mov.u32 	%r2, %tid.x;
	setp.eq.s32	%p1, %r10, 0;
	mov.f64 	%fd28, 0d0000000000000000;
	mov.f64 	%fd27, %fd28;
	mov.f64 	%fd26, %fd28;
	mov.f64 	%fd25, %fd28;
	@%p1 bra 	BB41_3;

	add.s32 	%r15, %r1, %r2;
	mad.lo.s32 	%r18, %r10, %r15, %r8;
	mov.u32 	%r19, 0;
	mov.f64 	%fd28, 0d0000000000000000;
	mov.f64 	%fd27, %fd28;
	mov.f64 	%fd26, %fd28;
	mov.f64 	%fd25, %fd28;

BB41_2:
	mul.wide.u32 	%rd6, %r18, 16;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd7];
	cvt.f64.f32	%fd21, %f1;
	add.f64 	%fd28, %fd28, %fd21;
	cvt.f64.f32	%fd22, %f2;
	add.f64 	%fd27, %fd27, %fd22;
	cvt.f64.f32	%fd23, %f3;
	add.f64 	%fd26, %fd26, %fd23;
	cvt.f64.f32	%fd24, %f4;
	add.f64 	%fd25, %fd25, %fd24;
	add.s32 	%r18, %r18, 1;
	add.s32 	%r19, %r19, 1;
	setp.lt.u32	%p2, %r19, %r10;
	@%p2 bra 	BB41_2;

BB41_3:
	add.s32 	%r16, %r1, %r2;
	add.s32 	%r17, %r16, %r9;
	mul.wide.u32 	%rd8, %r17, 8;
	add.s64 	%rd9, %rd2, %rd8;
	st.global.f64 	[%rd9], %fd28;
	add.s64 	%rd10, %rd3, %rd8;
	st.global.f64 	[%rd10], %fd27;
	add.s64 	%rd11, %rd4, %rd8;
	st.global.f64 	[%rd11], %fd26;
	add.s64 	%rd12, %rd5, %rd8;
	st.global.f64 	[%rd12], %fd25;
	ret;
}

	// .globl	float4_sumcomponents_weighted_16
.entry float4_sumcomponents_weighted_16(
	.param .u64 .ptr .global .align 16 float4_sumcomponents_weighted_16_param_0,
	.param .u64 .ptr .global .align 4 float4_sumcomponents_weighted_16_param_1,
	.param .u64 .ptr .global .align 8 float4_sumcomponents_weighted_16_param_2,
	.param .u64 .ptr .global .align 8 float4_sumcomponents_weighted_16_param_3,
	.param .u64 .ptr .global .align 8 float4_sumcomponents_weighted_16_param_4,
	.param .u64 .ptr .global .align 8 float4_sumcomponents_weighted_16_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<53>;
	.reg .b32 	%r<21>;
	.reg .f64 	%fd<45>;
	.reg .b64 	%rd<28>;


	ld.param.u64 	%rd1, [float4_sumcomponents_weighted_16_param_0];
	ld.param.u64 	%rd2, [float4_sumcomponents_weighted_16_param_1];
	ld.param.u64 	%rd3, [float4_sumcomponents_weighted_16_param_2];
	ld.param.u64 	%rd4, [float4_sumcomponents_weighted_16_param_3];
	ld.param.u64 	%rd5, [float4_sumcomponents_weighted_16_param_4];
	ld.param.u64 	%rd6, [float4_sumcomponents_weighted_16_param_5];
	mov.b32	%r8, %envreg3;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %ntid.x;
	mul.lo.s32 	%r11, %r9, %r10;
	add.s32 	%r12, %r11, %r8;
	mov.u32 	%r13, %tid.x;
	add.s32 	%r1, %r12, %r13;
	shl.b32 	%r14, %r8, 4;
	mad.lo.s32 	%r15, %r11, 16, %r14;
	mad.lo.s32 	%r19, %r13, 16, %r15;
	mov.f64 	%fd44, 0d0000000000000000;
	mov.f64 	%fd43, %fd44;
	mov.f64 	%fd42, %fd44;
	mov.f64 	%fd41, %fd44;
	mov.u32 	%r20, -16;

BB42_1:
	mul.wide.u32 	%rd7, %r19, 16;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd8];
	mul.wide.u32 	%rd9, %r19, 4;
	add.s64 	%rd10, %rd2, %rd9;
	ld.global.f32 	%f6, [%rd10];
	mul.f32 	%f7, %f1, %f6;
	cvt.f64.f32	%fd13, %f7;
	add.f64 	%fd14, %fd44, %fd13;
	mul.f32 	%f9, %f6, %f2;
	cvt.f64.f32	%fd15, %f9;
	add.f64 	%fd16, %fd43, %fd15;
	mul.f32 	%f11, %f3, %f6;
	cvt.f64.f32	%fd17, %f11;
	add.f64 	%fd18, %fd42, %fd17;
	mul.f32 	%f13, %f6, %f4;
	cvt.f64.f32	%fd19, %f13;
	add.f64 	%fd20, %fd41, %fd19;
	add.s32 	%r16, %r19, 1;
	mul.wide.u32 	%rd11, %r16, 16;
	add.s64 	%rd12, %rd1, %rd11;
	ld.global.v4.f32 	{%f14, %f15, %f16, %f17}, [%rd12];
	mul.wide.u32 	%rd13, %r16, 4;
	add.s64 	%rd14, %rd2, %rd13;
	ld.global.f32 	%f19, [%rd14];
	mul.f32 	%f20, %f14, %f19;
	cvt.f64.f32	%fd21, %f20;
	add.f64 	%fd22, %fd14, %fd21;
	mul.f32 	%f22, %f19, %f15;
	cvt.f64.f32	%fd23, %f22;
	add.f64 	%fd24, %fd16, %fd23;
	mul.f32 	%f24, %f16, %f19;
	cvt.f64.f32	%fd25, %f24;
	add.f64 	%fd26, %fd18, %fd25;
	mul.f32 	%f26, %f19, %f17;
	cvt.f64.f32	%fd27, %f26;
	add.f64 	%fd28, %fd20, %fd27;
	add.s32 	%r17, %r19, 2;
	mul.wide.u32 	%rd15, %r17, 16;
	add.s64 	%rd16, %rd1, %rd15;
	ld.global.v4.f32 	{%f27, %f28, %f29, %f30}, [%rd16];
	mul.wide.u32 	%rd17, %r17, 4;
	add.s64 	%rd18, %rd2, %rd17;
	ld.global.f32 	%f32, [%rd18];
	mul.f32 	%f33, %f27, %f32;
	cvt.f64.f32	%fd29, %f33;
	add.f64 	%fd30, %fd22, %fd29;
	mul.f32 	%f35, %f32, %f28;
	cvt.f64.f32	%fd31, %f35;
	add.f64 	%fd32, %fd24, %fd31;
	mul.f32 	%f37, %f29, %f32;
	cvt.f64.f32	%fd33, %f37;
	add.f64 	%fd34, %fd26, %fd33;
	mul.f32 	%f39, %f32, %f30;
	cvt.f64.f32	%fd35, %f39;
	add.f64 	%fd36, %fd28, %fd35;
	add.s32 	%r18, %r19, 3;
	mul.wide.u32 	%rd19, %r18, 16;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.v4.f32 	{%f40, %f41, %f42, %f43}, [%rd20];
	mul.wide.u32 	%rd21, %r18, 4;
	add.s64 	%rd22, %rd2, %rd21;
	ld.global.f32 	%f45, [%rd22];
	mul.f32 	%f46, %f40, %f45;
	cvt.f64.f32	%fd37, %f46;
	add.f64 	%fd44, %fd30, %fd37;
	mul.f32 	%f48, %f45, %f41;
	cvt.f64.f32	%fd38, %f48;
	add.f64 	%fd43, %fd32, %fd38;
	mul.f32 	%f50, %f42, %f45;
	cvt.f64.f32	%fd39, %f50;
	add.f64 	%fd42, %fd34, %fd39;
	mul.f32 	%f52, %f45, %f43;
	cvt.f64.f32	%fd40, %f52;
	add.f64 	%fd41, %fd36, %fd40;
	add.s32 	%r19, %r19, 4;
	add.s32 	%r20, %r20, 4;
	setp.ne.s32	%p1, %r20, 0;
	@%p1 bra 	BB42_1;

	mul.wide.u32 	%rd23, %r1, 8;
	add.s64 	%rd24, %rd3, %rd23;
	st.global.f64 	[%rd24], %fd44;
	add.s64 	%rd25, %rd4, %rd23;
	st.global.f64 	[%rd25], %fd43;
	add.s64 	%rd26, %rd5, %rd23;
	st.global.f64 	[%rd26], %fd42;
	add.s64 	%rd27, %rd6, %rd23;
	st.global.f64 	[%rd27], %fd41;
	ret;
}

	// .globl	float4_sumcomponents_weighted_N
.entry float4_sumcomponents_weighted_N(
	.param .u64 .ptr .global .align 16 float4_sumcomponents_weighted_N_param_0,
	.param .u64 .ptr .global .align 4 float4_sumcomponents_weighted_N_param_1,
	.param .u32 float4_sumcomponents_weighted_N_param_2,
	.param .u32 float4_sumcomponents_weighted_N_param_3,
	.param .u32 float4_sumcomponents_weighted_N_param_4,
	.param .u64 .ptr .global .align 8 float4_sumcomponents_weighted_N_param_5,
	.param .u64 .ptr .global .align 8 float4_sumcomponents_weighted_N_param_6,
	.param .u64 .ptr .global .align 8 float4_sumcomponents_weighted_N_param_7,
	.param .u64 .ptr .global .align 8 float4_sumcomponents_weighted_N_param_8
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<14>;
	.reg .b32 	%r<24>;
	.reg .f64 	%fd<29>;
	.reg .b64 	%rd<16>;


	ld.param.u64 	%rd1, [float4_sumcomponents_weighted_N_param_0];
	ld.param.u64 	%rd2, [float4_sumcomponents_weighted_N_param_1];
	ld.param.u32 	%r11, [float4_sumcomponents_weighted_N_param_2];
	ld.param.u32 	%r12, [float4_sumcomponents_weighted_N_param_3];
	ld.param.u32 	%r13, [float4_sumcomponents_weighted_N_param_4];
	ld.param.u64 	%rd3, [float4_sumcomponents_weighted_N_param_5];
	ld.param.u64 	%rd4, [float4_sumcomponents_weighted_N_param_6];
	ld.param.u64 	%rd5, [float4_sumcomponents_weighted_N_param_7];
	ld.param.u64 	%rd6, [float4_sumcomponents_weighted_N_param_8];
	mov.b32	%r14, %envreg3;
	mov.u32 	%r15, %ctaid.x;
	mov.u32 	%r16, %ntid.x;
	mad.lo.s32 	%r1, %r15, %r16, %r14;
	mov.u32 	%r2, %tid.x;
	setp.eq.s32	%p1, %r13, 0;
	mov.f64 	%fd28, 0d0000000000000000;
	mov.f64 	%fd27, %fd28;
	mov.f64 	%fd26, %fd28;
	mov.f64 	%fd25, %fd28;
	@%p1 bra 	BB43_3;

	add.s32 	%r18, %r1, %r2;
	mul.lo.s32 	%r21, %r13, %r18;
	add.s32 	%r22, %r11, %r21;
	mov.u32 	%r23, 0;
	mov.f64 	%fd28, 0d0000000000000000;
	mov.f64 	%fd27, %fd28;
	mov.f64 	%fd26, %fd28;
	mov.f64 	%fd25, %fd28;

BB43_2:
	mul.wide.u32 	%rd7, %r21, 16;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd8];
	mul.wide.u32 	%rd9, %r22, 4;
	add.s64 	%rd10, %rd2, %rd9;
	ld.global.f32 	%f6, [%rd10];
	mul.f32 	%f7, %f1, %f6;
	cvt.f64.f32	%fd21, %f7;
	add.f64 	%fd28, %fd28, %fd21;
	mul.f32 	%f9, %f6, %f2;
	cvt.f64.f32	%fd22, %f9;
	add.f64 	%fd27, %fd27, %fd22;
	mul.f32 	%f11, %f3, %f6;
	cvt.f64.f32	%fd23, %f11;
	add.f64 	%fd26, %fd26, %fd23;
	mul.f32 	%f13, %f6, %f4;
	cvt.f64.f32	%fd24, %f13;
	add.f64 	%fd25, %fd25, %fd24;
	add.s32 	%r22, %r22, 1;
	add.s32 	%r21, %r21, 1;
	add.s32 	%r23, %r23, 1;
	setp.lt.u32	%p2, %r23, %r13;
	@%p2 bra 	BB43_2;

BB43_3:
	add.s32 	%r19, %r1, %r2;
	add.s32 	%r20, %r19, %r12;
	mul.wide.u32 	%rd11, %r20, 8;
	add.s64 	%rd12, %rd3, %rd11;
	st.global.f64 	[%rd12], %fd28;
	add.s64 	%rd13, %rd4, %rd11;
	st.global.f64 	[%rd13], %fd27;
	add.s64 	%rd14, %rd5, %rd11;
	st.global.f64 	[%rd14], %fd26;
	add.s64 	%rd15, %rd6, %rd11;
	st.global.f64 	[%rd15], %fd25;
	ret;
}

	// .globl	float4_sum2d_16
.entry float4_sum2d_16(
	.param .u64 .ptr .global .align 16 float4_sum2d_16_param_0,
	.param .u32 float4_sum2d_16_param_1,
	.param .u64 .ptr .global .align 8 float4_sum2d_16_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<129>;
	.reg .b32 	%r<70>;
	.reg .f64 	%fd<129>;
	.reg .b64 	%rd<37>;


	ld.param.u64 	%rd1, [float4_sum2d_16_param_0];
	ld.param.u32 	%r28, [float4_sum2d_16_param_1];
	ld.param.u64 	%rd2, [float4_sum2d_16_param_2];
	setp.eq.s32	%p1, %r28, 0;
	@%p1 bra 	BB44_3;

	mov.b32	%r30, %envreg3;
	mov.u32 	%r31, %ntid.x;
	mov.u32 	%r32, %ctaid.x;
	mov.u32 	%r33, %tid.x;
	add.s32 	%r34, %r30, %r33;
	mad.lo.s32 	%r35, %r32, %r31, %r34;
	shl.b32 	%r36, %r35, 4;
	add.s32 	%r37, %r36, 15;
	mul.lo.s32 	%r68, %r28, %r37;
	add.s32 	%r38, %r36, 14;
	mul.lo.s32 	%r67, %r28, %r38;
	add.s32 	%r39, %r36, 13;
	mul.lo.s32 	%r66, %r28, %r39;
	add.s32 	%r40, %r36, 12;
	mul.lo.s32 	%r65, %r28, %r40;
	add.s32 	%r41, %r36, 11;
	mul.lo.s32 	%r5, %r28, %r41;
	add.s32 	%r42, %r36, 10;
	mul.lo.s32 	%r6, %r28, %r42;
	add.s32 	%r43, %r36, 9;
	mul.lo.s32 	%r7, %r28, %r43;
	add.s32 	%r44, %r36, 8;
	mul.lo.s32 	%r8, %r28, %r44;
	add.s32 	%r45, %r36, 7;
	mul.lo.s32 	%r9, %r28, %r45;
	add.s32 	%r46, %r36, 6;
	mul.lo.s32 	%r10, %r28, %r46;
	add.s32 	%r47, %r36, 5;
	mul.lo.s32 	%r11, %r28, %r47;
	add.s32 	%r48, %r36, 4;
	mul.lo.s32 	%r12, %r28, %r48;
	add.s32 	%r49, %r36, 3;
	mul.lo.s32 	%r13, %r28, %r49;
	add.s32 	%r50, %r36, 2;
	mul.lo.s32 	%r14, %r28, %r50;
	add.s32 	%r51, %r36, 1;
	mul.lo.s32 	%r15, %r28, %r51;
	mul.lo.s32 	%r16, %r28, %r35;
	shl.b32 	%r17, %r16, 4;
	mov.u32 	%r69, 0;

BB44_2:
	add.s32 	%r52, %r17, %r69;
	mul.wide.u32 	%rd3, %r52, 16;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd4];
	cvt.f64.f32	%fd1, %f1;
	add.f64 	%fd2, %fd1, 0d0000000000000000;
	cvt.f64.f32	%fd3, %f2;
	add.f64 	%fd4, %fd3, %fd2;
	cvt.f64.f32	%fd5, %f3;
	add.f64 	%fd6, %fd5, %fd4;
	cvt.f64.f32	%fd7, %f4;
	add.f64 	%fd8, %fd7, %fd6;
	add.s32 	%r53, %r15, %r69;
	mul.wide.u32 	%rd5, %r53, 16;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.v4.f32 	{%f9, %f10, %f11, %f12}, [%rd6];
	cvt.f64.f32	%fd9, %f9;
	add.f64 	%fd10, %fd8, %fd9;
	cvt.f64.f32	%fd11, %f10;
	add.f64 	%fd12, %fd11, %fd10;
	cvt.f64.f32	%fd13, %f11;
	add.f64 	%fd14, %fd13, %fd12;
	cvt.f64.f32	%fd15, %f12;
	add.f64 	%fd16, %fd15, %fd14;
	add.s32 	%r54, %r14, %r69;
	mul.wide.u32 	%rd7, %r54, 16;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.v4.f32 	{%f17, %f18, %f19, %f20}, [%rd8];
	cvt.f64.f32	%fd17, %f17;
	add.f64 	%fd18, %fd16, %fd17;
	cvt.f64.f32	%fd19, %f18;
	add.f64 	%fd20, %fd19, %fd18;
	cvt.f64.f32	%fd21, %f19;
	add.f64 	%fd22, %fd21, %fd20;
	cvt.f64.f32	%fd23, %f20;
	add.f64 	%fd24, %fd23, %fd22;
	add.s32 	%r55, %r13, %r69;
	mul.wide.u32 	%rd9, %r55, 16;
	add.s64 	%rd10, %rd1, %rd9;
	ld.global.v4.f32 	{%f25, %f26, %f27, %f28}, [%rd10];
	cvt.f64.f32	%fd25, %f25;
	add.f64 	%fd26, %fd24, %fd25;
	cvt.f64.f32	%fd27, %f26;
	add.f64 	%fd28, %fd27, %fd26;
	cvt.f64.f32	%fd29, %f27;
	add.f64 	%fd30, %fd29, %fd28;
	cvt.f64.f32	%fd31, %f28;
	add.f64 	%fd32, %fd31, %fd30;
	add.s32 	%r56, %r12, %r69;
	mul.wide.u32 	%rd11, %r56, 16;
	add.s64 	%rd12, %rd1, %rd11;
	ld.global.v4.f32 	{%f33, %f34, %f35, %f36}, [%rd12];
	cvt.f64.f32	%fd33, %f33;
	add.f64 	%fd34, %fd32, %fd33;
	cvt.f64.f32	%fd35, %f34;
	add.f64 	%fd36, %fd35, %fd34;
	cvt.f64.f32	%fd37, %f35;
	add.f64 	%fd38, %fd37, %fd36;
	cvt.f64.f32	%fd39, %f36;
	add.f64 	%fd40, %fd39, %fd38;
	add.s32 	%r57, %r11, %r69;
	mul.wide.u32 	%rd13, %r57, 16;
	add.s64 	%rd14, %rd1, %rd13;
	ld.global.v4.f32 	{%f41, %f42, %f43, %f44}, [%rd14];
	cvt.f64.f32	%fd41, %f41;
	add.f64 	%fd42, %fd40, %fd41;
	cvt.f64.f32	%fd43, %f42;
	add.f64 	%fd44, %fd43, %fd42;
	cvt.f64.f32	%fd45, %f43;
	add.f64 	%fd46, %fd45, %fd44;
	cvt.f64.f32	%fd47, %f44;
	add.f64 	%fd48, %fd47, %fd46;
	add.s32 	%r58, %r10, %r69;
	mul.wide.u32 	%rd15, %r58, 16;
	add.s64 	%rd16, %rd1, %rd15;
	ld.global.v4.f32 	{%f49, %f50, %f51, %f52}, [%rd16];
	cvt.f64.f32	%fd49, %f49;
	add.f64 	%fd50, %fd48, %fd49;
	cvt.f64.f32	%fd51, %f50;
	add.f64 	%fd52, %fd51, %fd50;
	cvt.f64.f32	%fd53, %f51;
	add.f64 	%fd54, %fd53, %fd52;
	cvt.f64.f32	%fd55, %f52;
	add.f64 	%fd56, %fd55, %fd54;
	add.s32 	%r59, %r9, %r69;
	mul.wide.u32 	%rd17, %r59, 16;
	add.s64 	%rd18, %rd1, %rd17;
	ld.global.v4.f32 	{%f57, %f58, %f59, %f60}, [%rd18];
	cvt.f64.f32	%fd57, %f57;
	add.f64 	%fd58, %fd56, %fd57;
	cvt.f64.f32	%fd59, %f58;
	add.f64 	%fd60, %fd59, %fd58;
	cvt.f64.f32	%fd61, %f59;
	add.f64 	%fd62, %fd61, %fd60;
	cvt.f64.f32	%fd63, %f60;
	add.f64 	%fd64, %fd63, %fd62;
	add.s32 	%r60, %r8, %r69;
	mul.wide.u32 	%rd19, %r60, 16;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.v4.f32 	{%f65, %f66, %f67, %f68}, [%rd20];
	cvt.f64.f32	%fd65, %f65;
	add.f64 	%fd66, %fd64, %fd65;
	cvt.f64.f32	%fd67, %f66;
	add.f64 	%fd68, %fd67, %fd66;
	cvt.f64.f32	%fd69, %f67;
	add.f64 	%fd70, %fd69, %fd68;
	cvt.f64.f32	%fd71, %f68;
	add.f64 	%fd72, %fd71, %fd70;
	add.s32 	%r61, %r7, %r69;
	mul.wide.u32 	%rd21, %r61, 16;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.v4.f32 	{%f73, %f74, %f75, %f76}, [%rd22];
	cvt.f64.f32	%fd73, %f73;
	add.f64 	%fd74, %fd72, %fd73;
	cvt.f64.f32	%fd75, %f74;
	add.f64 	%fd76, %fd75, %fd74;
	cvt.f64.f32	%fd77, %f75;
	add.f64 	%fd78, %fd77, %fd76;
	cvt.f64.f32	%fd79, %f76;
	add.f64 	%fd80, %fd79, %fd78;
	add.s32 	%r62, %r6, %r69;
	mul.wide.u32 	%rd23, %r62, 16;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.v4.f32 	{%f81, %f82, %f83, %f84}, [%rd24];
	cvt.f64.f32	%fd81, %f81;
	add.f64 	%fd82, %fd80, %fd81;
	cvt.f64.f32	%fd83, %f82;
	add.f64 	%fd84, %fd83, %fd82;
	cvt.f64.f32	%fd85, %f83;
	add.f64 	%fd86, %fd85, %fd84;
	cvt.f64.f32	%fd87, %f84;
	add.f64 	%fd88, %fd87, %fd86;
	add.s32 	%r63, %r5, %r69;
	mul.wide.u32 	%rd25, %r63, 16;
	add.s64 	%rd26, %rd1, %rd25;
	ld.global.v4.f32 	{%f89, %f90, %f91, %f92}, [%rd26];
	cvt.f64.f32	%fd89, %f89;
	add.f64 	%fd90, %fd88, %fd89;
	cvt.f64.f32	%fd91, %f90;
	add.f64 	%fd92, %fd91, %fd90;
	cvt.f64.f32	%fd93, %f91;
	add.f64 	%fd94, %fd93, %fd92;
	cvt.f64.f32	%fd95, %f92;
	add.f64 	%fd96, %fd95, %fd94;
	mul.wide.u32 	%rd27, %r65, 16;
	add.s64 	%rd28, %rd1, %rd27;
	ld.global.v4.f32 	{%f97, %f98, %f99, %f100}, [%rd28];
	cvt.f64.f32	%fd97, %f97;
	add.f64 	%fd98, %fd96, %fd97;
	cvt.f64.f32	%fd99, %f98;
	add.f64 	%fd100, %fd99, %fd98;
	cvt.f64.f32	%fd101, %f99;
	add.f64 	%fd102, %fd101, %fd100;
	cvt.f64.f32	%fd103, %f100;
	add.f64 	%fd104, %fd103, %fd102;
	mul.wide.u32 	%rd29, %r66, 16;
	add.s64 	%rd30, %rd1, %rd29;
	ld.global.v4.f32 	{%f105, %f106, %f107, %f108}, [%rd30];
	cvt.f64.f32	%fd105, %f105;
	add.f64 	%fd106, %fd104, %fd105;
	cvt.f64.f32	%fd107, %f106;
	add.f64 	%fd108, %fd107, %fd106;
	cvt.f64.f32	%fd109, %f107;
	add.f64 	%fd110, %fd109, %fd108;
	cvt.f64.f32	%fd111, %f108;
	add.f64 	%fd112, %fd111, %fd110;
	mul.wide.u32 	%rd31, %r67, 16;
	add.s64 	%rd32, %rd1, %rd31;
	ld.global.v4.f32 	{%f113, %f114, %f115, %f116}, [%rd32];
	cvt.f64.f32	%fd113, %f113;
	add.f64 	%fd114, %fd112, %fd113;
	cvt.f64.f32	%fd115, %f114;
	add.f64 	%fd116, %fd115, %fd114;
	cvt.f64.f32	%fd117, %f115;
	add.f64 	%fd118, %fd117, %fd116;
	cvt.f64.f32	%fd119, %f116;
	add.f64 	%fd120, %fd119, %fd118;
	mul.wide.u32 	%rd33, %r68, 16;
	add.s64 	%rd34, %rd1, %rd33;
	ld.global.v4.f32 	{%f121, %f122, %f123, %f124}, [%rd34];
	cvt.f64.f32	%fd121, %f121;
	add.f64 	%fd122, %fd120, %fd121;
	cvt.f64.f32	%fd123, %f122;
	add.f64 	%fd124, %fd123, %fd122;
	cvt.f64.f32	%fd125, %f123;
	add.f64 	%fd126, %fd125, %fd124;
	cvt.f64.f32	%fd127, %f124;
	add.f64 	%fd128, %fd127, %fd126;
	add.s32 	%r64, %r16, %r69;
	mul.wide.u32 	%rd35, %r64, 8;
	add.s64 	%rd36, %rd2, %rd35;
	st.global.f64 	[%rd36], %fd128;
	add.s32 	%r68, %r68, 1;
	add.s32 	%r67, %r67, 1;
	add.s32 	%r66, %r66, 1;
	add.s32 	%r65, %r65, 1;
	add.s32 	%r69, %r69, 1;
	setp.lt.u32	%p2, %r69, %r28;
	@%p2 bra 	BB44_2;

BB44_3:
	ret;
}

	// .globl	float4_sum2d_N
.entry float4_sum2d_N(
	.param .u64 .ptr .global .align 16 float4_sum2d_N_param_0,
	.param .u32 float4_sum2d_N_param_1,
	.param .u32 float4_sum2d_N_param_2,
	.param .u32 float4_sum2d_N_param_3,
	.param .u32 float4_sum2d_N_param_4,
	.param .u64 .ptr .global .align 8 float4_sum2d_N_param_5
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<28>;
	.reg .f64 	%fd<16>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [float4_sum2d_N_param_0];
	ld.param.u32 	%r10, [float4_sum2d_N_param_1];
	ld.param.u32 	%r11, [float4_sum2d_N_param_2];
	ld.param.u32 	%r12, [float4_sum2d_N_param_3];
	ld.param.u32 	%r13, [float4_sum2d_N_param_4];
	ld.param.u64 	%rd2, [float4_sum2d_N_param_5];
	setp.eq.s32	%p1, %r10, 0;
	@%p1 bra 	BB45_5;

	mov.b32	%r15, %envreg3;
	mov.u32 	%r16, %ctaid.x;
	mov.u32 	%r17, %ntid.x;
	mad.lo.s32 	%r18, %r16, %r17, %r15;
	mov.u32 	%r19, %tid.x;
	add.s32 	%r20, %r18, %r19;
	mul.lo.s32 	%r21, %r20, %r10;
	add.s32 	%r1, %r21, %r12;
	mad.lo.s32 	%r2, %r21, %r13, %r11;
	mov.u32 	%r14, 0;
	mov.u32 	%r27, %r14;

BB45_2:
	add.s32 	%r24, %r2, %r27;
	setp.eq.s32	%p2, %r13, 0;
	mov.f64 	%fd15, 0d0000000000000000;
	mov.f64 	%fd14, %fd15;
	mov.u32 	%r26, %r14;
	@%p2 bra 	BB45_4;

BB45_3:
	mov.u32 	%r6, %r26;
	mul.wide.u32 	%rd3, %r24, 16;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd4];
	cvt.f64.f32	%fd6, %f1;
	add.f64 	%fd7, %fd15, %fd6;
	cvt.f64.f32	%fd8, %f2;
	add.f64 	%fd9, %fd8, %fd7;
	cvt.f64.f32	%fd10, %f3;
	add.f64 	%fd11, %fd10, %fd9;
	cvt.f64.f32	%fd12, %f4;
	add.f64 	%fd15, %fd12, %fd11;
	add.s32 	%r24, %r24, %r10;
	add.s32 	%r8, %r6, 1;
	setp.lt.u32	%p3, %r8, %r13;
	mov.u32 	%r26, %r8;
	mov.f64 	%fd14, %fd15;
	@%p3 bra 	BB45_3;

BB45_4:
	add.s32 	%r23, %r1, %r27;
	mul.wide.u32 	%rd5, %r23, 8;
	add.s64 	%rd6, %rd2, %rd5;
	st.global.f64 	[%rd6], %fd14;
	add.s32 	%r27, %r27, 1;
	setp.lt.u32	%p4, %r27, %r10;
	@%p4 bra 	BB45_2;

BB45_5:
	ret;
}

	// .globl	float4_sum2d_16_weighted
.entry float4_sum2d_16_weighted(
	.param .u64 .ptr .global .align 16 float4_sum2d_16_weighted_param_0,
	.param .u64 .ptr .global .align 16 float4_sum2d_16_weighted_param_1,
	.param .u32 float4_sum2d_16_weighted_param_2,
	.param .u64 .ptr .global .align 8 float4_sum2d_16_weighted_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<129>;
	.reg .b32 	%r<70>;
	.reg .f64 	%fd<129>;
	.reg .b64 	%rd<37>;


	ld.param.u64 	%rd1, [float4_sum2d_16_weighted_param_1];
	ld.param.u32 	%r28, [float4_sum2d_16_weighted_param_2];
	ld.param.u64 	%rd2, [float4_sum2d_16_weighted_param_3];
	setp.eq.s32	%p1, %r28, 0;
	@%p1 bra 	BB46_3;

	mov.b32	%r30, %envreg3;
	mov.u32 	%r31, %ntid.x;
	mov.u32 	%r32, %ctaid.x;
	mov.u32 	%r33, %tid.x;
	add.s32 	%r34, %r30, %r33;
	mad.lo.s32 	%r35, %r32, %r31, %r34;
	shl.b32 	%r36, %r35, 4;
	add.s32 	%r37, %r36, 15;
	mul.lo.s32 	%r68, %r28, %r37;
	add.s32 	%r38, %r36, 14;
	mul.lo.s32 	%r67, %r28, %r38;
	add.s32 	%r39, %r36, 13;
	mul.lo.s32 	%r66, %r28, %r39;
	add.s32 	%r40, %r36, 12;
	mul.lo.s32 	%r65, %r28, %r40;
	add.s32 	%r41, %r36, 11;
	mul.lo.s32 	%r5, %r28, %r41;
	add.s32 	%r42, %r36, 10;
	mul.lo.s32 	%r6, %r28, %r42;
	add.s32 	%r43, %r36, 9;
	mul.lo.s32 	%r7, %r28, %r43;
	add.s32 	%r44, %r36, 8;
	mul.lo.s32 	%r8, %r28, %r44;
	add.s32 	%r45, %r36, 7;
	mul.lo.s32 	%r9, %r28, %r45;
	add.s32 	%r46, %r36, 6;
	mul.lo.s32 	%r10, %r28, %r46;
	add.s32 	%r47, %r36, 5;
	mul.lo.s32 	%r11, %r28, %r47;
	add.s32 	%r48, %r36, 4;
	mul.lo.s32 	%r12, %r28, %r48;
	add.s32 	%r49, %r36, 3;
	mul.lo.s32 	%r13, %r28, %r49;
	add.s32 	%r50, %r36, 2;
	mul.lo.s32 	%r14, %r28, %r50;
	add.s32 	%r51, %r36, 1;
	mul.lo.s32 	%r15, %r28, %r51;
	mul.lo.s32 	%r16, %r28, %r35;
	shl.b32 	%r17, %r16, 4;
	mov.u32 	%r69, 0;

BB46_2:
	add.s32 	%r52, %r17, %r69;
	mul.wide.u32 	%rd3, %r52, 16;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd4];
	cvt.f64.f32	%fd1, %f1;
	add.f64 	%fd2, %fd1, 0d0000000000000000;
	cvt.f64.f32	%fd3, %f2;
	add.f64 	%fd4, %fd3, %fd2;
	cvt.f64.f32	%fd5, %f3;
	add.f64 	%fd6, %fd5, %fd4;
	cvt.f64.f32	%fd7, %f4;
	add.f64 	%fd8, %fd7, %fd6;
	add.s32 	%r53, %r15, %r69;
	mul.wide.u32 	%rd5, %r53, 16;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.v4.f32 	{%f9, %f10, %f11, %f12}, [%rd6];
	cvt.f64.f32	%fd9, %f9;
	add.f64 	%fd10, %fd8, %fd9;
	cvt.f64.f32	%fd11, %f10;
	add.f64 	%fd12, %fd11, %fd10;
	cvt.f64.f32	%fd13, %f11;
	add.f64 	%fd14, %fd13, %fd12;
	cvt.f64.f32	%fd15, %f12;
	add.f64 	%fd16, %fd15, %fd14;
	add.s32 	%r54, %r14, %r69;
	mul.wide.u32 	%rd7, %r54, 16;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.v4.f32 	{%f17, %f18, %f19, %f20}, [%rd8];
	cvt.f64.f32	%fd17, %f17;
	add.f64 	%fd18, %fd16, %fd17;
	cvt.f64.f32	%fd19, %f18;
	add.f64 	%fd20, %fd19, %fd18;
	cvt.f64.f32	%fd21, %f19;
	add.f64 	%fd22, %fd21, %fd20;
	cvt.f64.f32	%fd23, %f20;
	add.f64 	%fd24, %fd23, %fd22;
	add.s32 	%r55, %r13, %r69;
	mul.wide.u32 	%rd9, %r55, 16;
	add.s64 	%rd10, %rd1, %rd9;
	ld.global.v4.f32 	{%f25, %f26, %f27, %f28}, [%rd10];
	cvt.f64.f32	%fd25, %f25;
	add.f64 	%fd26, %fd24, %fd25;
	cvt.f64.f32	%fd27, %f26;
	add.f64 	%fd28, %fd27, %fd26;
	cvt.f64.f32	%fd29, %f27;
	add.f64 	%fd30, %fd29, %fd28;
	cvt.f64.f32	%fd31, %f28;
	add.f64 	%fd32, %fd31, %fd30;
	add.s32 	%r56, %r12, %r69;
	mul.wide.u32 	%rd11, %r56, 16;
	add.s64 	%rd12, %rd1, %rd11;
	ld.global.v4.f32 	{%f33, %f34, %f35, %f36}, [%rd12];
	cvt.f64.f32	%fd33, %f33;
	add.f64 	%fd34, %fd32, %fd33;
	cvt.f64.f32	%fd35, %f34;
	add.f64 	%fd36, %fd35, %fd34;
	cvt.f64.f32	%fd37, %f35;
	add.f64 	%fd38, %fd37, %fd36;
	cvt.f64.f32	%fd39, %f36;
	add.f64 	%fd40, %fd39, %fd38;
	add.s32 	%r57, %r11, %r69;
	mul.wide.u32 	%rd13, %r57, 16;
	add.s64 	%rd14, %rd1, %rd13;
	ld.global.v4.f32 	{%f41, %f42, %f43, %f44}, [%rd14];
	cvt.f64.f32	%fd41, %f41;
	add.f64 	%fd42, %fd40, %fd41;
	cvt.f64.f32	%fd43, %f42;
	add.f64 	%fd44, %fd43, %fd42;
	cvt.f64.f32	%fd45, %f43;
	add.f64 	%fd46, %fd45, %fd44;
	cvt.f64.f32	%fd47, %f44;
	add.f64 	%fd48, %fd47, %fd46;
	add.s32 	%r58, %r10, %r69;
	mul.wide.u32 	%rd15, %r58, 16;
	add.s64 	%rd16, %rd1, %rd15;
	ld.global.v4.f32 	{%f49, %f50, %f51, %f52}, [%rd16];
	cvt.f64.f32	%fd49, %f49;
	add.f64 	%fd50, %fd48, %fd49;
	cvt.f64.f32	%fd51, %f50;
	add.f64 	%fd52, %fd51, %fd50;
	cvt.f64.f32	%fd53, %f51;
	add.f64 	%fd54, %fd53, %fd52;
	cvt.f64.f32	%fd55, %f52;
	add.f64 	%fd56, %fd55, %fd54;
	add.s32 	%r59, %r9, %r69;
	mul.wide.u32 	%rd17, %r59, 16;
	add.s64 	%rd18, %rd1, %rd17;
	ld.global.v4.f32 	{%f57, %f58, %f59, %f60}, [%rd18];
	cvt.f64.f32	%fd57, %f57;
	add.f64 	%fd58, %fd56, %fd57;
	cvt.f64.f32	%fd59, %f58;
	add.f64 	%fd60, %fd59, %fd58;
	cvt.f64.f32	%fd61, %f59;
	add.f64 	%fd62, %fd61, %fd60;
	cvt.f64.f32	%fd63, %f60;
	add.f64 	%fd64, %fd63, %fd62;
	add.s32 	%r60, %r8, %r69;
	mul.wide.u32 	%rd19, %r60, 16;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.v4.f32 	{%f65, %f66, %f67, %f68}, [%rd20];
	cvt.f64.f32	%fd65, %f65;
	add.f64 	%fd66, %fd64, %fd65;
	cvt.f64.f32	%fd67, %f66;
	add.f64 	%fd68, %fd67, %fd66;
	cvt.f64.f32	%fd69, %f67;
	add.f64 	%fd70, %fd69, %fd68;
	cvt.f64.f32	%fd71, %f68;
	add.f64 	%fd72, %fd71, %fd70;
	add.s32 	%r61, %r7, %r69;
	mul.wide.u32 	%rd21, %r61, 16;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.v4.f32 	{%f73, %f74, %f75, %f76}, [%rd22];
	cvt.f64.f32	%fd73, %f73;
	add.f64 	%fd74, %fd72, %fd73;
	cvt.f64.f32	%fd75, %f74;
	add.f64 	%fd76, %fd75, %fd74;
	cvt.f64.f32	%fd77, %f75;
	add.f64 	%fd78, %fd77, %fd76;
	cvt.f64.f32	%fd79, %f76;
	add.f64 	%fd80, %fd79, %fd78;
	add.s32 	%r62, %r6, %r69;
	mul.wide.u32 	%rd23, %r62, 16;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.v4.f32 	{%f81, %f82, %f83, %f84}, [%rd24];
	cvt.f64.f32	%fd81, %f81;
	add.f64 	%fd82, %fd80, %fd81;
	cvt.f64.f32	%fd83, %f82;
	add.f64 	%fd84, %fd83, %fd82;
	cvt.f64.f32	%fd85, %f83;
	add.f64 	%fd86, %fd85, %fd84;
	cvt.f64.f32	%fd87, %f84;
	add.f64 	%fd88, %fd87, %fd86;
	add.s32 	%r63, %r5, %r69;
	mul.wide.u32 	%rd25, %r63, 16;
	add.s64 	%rd26, %rd1, %rd25;
	ld.global.v4.f32 	{%f89, %f90, %f91, %f92}, [%rd26];
	cvt.f64.f32	%fd89, %f89;
	add.f64 	%fd90, %fd88, %fd89;
	cvt.f64.f32	%fd91, %f90;
	add.f64 	%fd92, %fd91, %fd90;
	cvt.f64.f32	%fd93, %f91;
	add.f64 	%fd94, %fd93, %fd92;
	cvt.f64.f32	%fd95, %f92;
	add.f64 	%fd96, %fd95, %fd94;
	mul.wide.u32 	%rd27, %r65, 16;
	add.s64 	%rd28, %rd1, %rd27;
	ld.global.v4.f32 	{%f97, %f98, %f99, %f100}, [%rd28];
	cvt.f64.f32	%fd97, %f97;
	add.f64 	%fd98, %fd96, %fd97;
	cvt.f64.f32	%fd99, %f98;
	add.f64 	%fd100, %fd99, %fd98;
	cvt.f64.f32	%fd101, %f99;
	add.f64 	%fd102, %fd101, %fd100;
	cvt.f64.f32	%fd103, %f100;
	add.f64 	%fd104, %fd103, %fd102;
	mul.wide.u32 	%rd29, %r66, 16;
	add.s64 	%rd30, %rd1, %rd29;
	ld.global.v4.f32 	{%f105, %f106, %f107, %f108}, [%rd30];
	cvt.f64.f32	%fd105, %f105;
	add.f64 	%fd106, %fd104, %fd105;
	cvt.f64.f32	%fd107, %f106;
	add.f64 	%fd108, %fd107, %fd106;
	cvt.f64.f32	%fd109, %f107;
	add.f64 	%fd110, %fd109, %fd108;
	cvt.f64.f32	%fd111, %f108;
	add.f64 	%fd112, %fd111, %fd110;
	mul.wide.u32 	%rd31, %r67, 16;
	add.s64 	%rd32, %rd1, %rd31;
	ld.global.v4.f32 	{%f113, %f114, %f115, %f116}, [%rd32];
	cvt.f64.f32	%fd113, %f113;
	add.f64 	%fd114, %fd112, %fd113;
	cvt.f64.f32	%fd115, %f114;
	add.f64 	%fd116, %fd115, %fd114;
	cvt.f64.f32	%fd117, %f115;
	add.f64 	%fd118, %fd117, %fd116;
	cvt.f64.f32	%fd119, %f116;
	add.f64 	%fd120, %fd119, %fd118;
	mul.wide.u32 	%rd33, %r68, 16;
	add.s64 	%rd34, %rd1, %rd33;
	ld.global.v4.f32 	{%f121, %f122, %f123, %f124}, [%rd34];
	cvt.f64.f32	%fd121, %f121;
	add.f64 	%fd122, %fd120, %fd121;
	cvt.f64.f32	%fd123, %f122;
	add.f64 	%fd124, %fd123, %fd122;
	cvt.f64.f32	%fd125, %f123;
	add.f64 	%fd126, %fd125, %fd124;
	cvt.f64.f32	%fd127, %f124;
	add.f64 	%fd128, %fd127, %fd126;
	add.s32 	%r64, %r16, %r69;
	mul.wide.u32 	%rd35, %r64, 8;
	add.s64 	%rd36, %rd2, %rd35;
	st.global.f64 	[%rd36], %fd128;
	add.s32 	%r68, %r68, 1;
	add.s32 	%r67, %r67, 1;
	add.s32 	%r66, %r66, 1;
	add.s32 	%r65, %r65, 1;
	add.s32 	%r69, %r69, 1;
	setp.lt.u32	%p2, %r69, %r28;
	@%p2 bra 	BB46_2;

BB46_3:
	ret;
}

	// .globl	float4_sum2d_N_weighted
.entry float4_sum2d_N_weighted(
	.param .u64 .ptr .global .align 16 float4_sum2d_N_weighted_param_0,
	.param .u64 .ptr .global .align 16 float4_sum2d_N_weighted_param_1,
	.param .u32 float4_sum2d_N_weighted_param_2,
	.param .u32 float4_sum2d_N_weighted_param_3,
	.param .u32 float4_sum2d_N_weighted_param_4,
	.param .u32 float4_sum2d_N_weighted_param_5,
	.param .u64 .ptr .global .align 8 float4_sum2d_N_weighted_param_6
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<28>;
	.reg .f64 	%fd<16>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [float4_sum2d_N_weighted_param_1];
	ld.param.u32 	%r10, [float4_sum2d_N_weighted_param_2];
	ld.param.u32 	%r11, [float4_sum2d_N_weighted_param_3];
	ld.param.u32 	%r12, [float4_sum2d_N_weighted_param_4];
	ld.param.u32 	%r13, [float4_sum2d_N_weighted_param_5];
	ld.param.u64 	%rd2, [float4_sum2d_N_weighted_param_6];
	setp.eq.s32	%p1, %r10, 0;
	@%p1 bra 	BB47_5;

	mov.b32	%r15, %envreg3;
	mov.u32 	%r16, %ctaid.x;
	mov.u32 	%r17, %ntid.x;
	mad.lo.s32 	%r18, %r16, %r17, %r15;
	mov.u32 	%r19, %tid.x;
	add.s32 	%r20, %r18, %r19;
	mul.lo.s32 	%r21, %r20, %r10;
	add.s32 	%r1, %r21, %r12;
	mad.lo.s32 	%r2, %r21, %r13, %r11;
	mov.u32 	%r14, 0;
	mov.u32 	%r27, %r14;

BB47_2:
	add.s32 	%r24, %r2, %r27;
	setp.eq.s32	%p2, %r13, 0;
	mov.f64 	%fd15, 0d0000000000000000;
	mov.f64 	%fd14, %fd15;
	mov.u32 	%r26, %r14;
	@%p2 bra 	BB47_4;

BB47_3:
	mov.u32 	%r6, %r26;
	mul.wide.u32 	%rd3, %r24, 16;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd4];
	cvt.f64.f32	%fd6, %f1;
	add.f64 	%fd7, %fd15, %fd6;
	cvt.f64.f32	%fd8, %f2;
	add.f64 	%fd9, %fd8, %fd7;
	cvt.f64.f32	%fd10, %f3;
	add.f64 	%fd11, %fd10, %fd9;
	cvt.f64.f32	%fd12, %f4;
	add.f64 	%fd15, %fd12, %fd11;
	add.s32 	%r24, %r24, %r10;
	add.s32 	%r8, %r6, 1;
	setp.lt.u32	%p3, %r8, %r13;
	mov.u32 	%r26, %r8;
	mov.f64 	%fd14, %fd15;
	@%p3 bra 	BB47_3;

BB47_4:
	add.s32 	%r23, %r1, %r27;
	mul.wide.u32 	%rd5, %r23, 8;
	add.s64 	%rd6, %rd2, %rd5;
	st.global.f64 	[%rd6], %fd14;
	add.s32 	%r27, %r27, 1;
	setp.lt.u32	%p4, %r27, %r10;
	@%p4 bra 	BB47_2;

BB47_5:
	ret;
}

	// .globl	float44_sum_16
.entry float44_sum_16(
	.param .u64 .ptr .global .align 16 float44_sum_16_param_0,
	.param .u64 .ptr .global .align 8 float44_sum_16_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<65>;
	.reg .b32 	%r<19>;
	.reg .f64 	%fd<68>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [float44_sum_16_param_0];
	ld.param.u64 	%rd2, [float44_sum_16_param_1];
	mov.b32	%r8, %envreg3;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %ntid.x;
	mul.lo.s32 	%r11, %r9, %r10;
	add.s32 	%r12, %r11, %r8;
	mov.u32 	%r13, %tid.x;
	add.s32 	%r1, %r12, %r13;
	shl.b32 	%r14, %r8, 4;
	mad.lo.s32 	%r15, %r11, 16, %r14;
	mad.lo.s32 	%r17, %r13, 16, %r15;
	mov.f64 	%fd67, 0d0000000000000000;
	mov.u32 	%r18, -16;

BB48_1:
	mul.wide.u32 	%rd3, %r17, 64;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd4];
	cvt.f64.f32	%fd4, %f1;
	add.f64 	%fd5, %fd67, %fd4;
	cvt.f64.f32	%fd6, %f2;
	add.f64 	%fd7, %fd6, %fd5;
	cvt.f64.f32	%fd8, %f3;
	add.f64 	%fd9, %fd8, %fd7;
	cvt.f64.f32	%fd10, %f4;
	add.f64 	%fd11, %fd10, %fd9;
	ld.global.v4.f32 	{%f9, %f10, %f11, %f12}, [%rd4+16];
	cvt.f64.f32	%fd12, %f9;
	add.f64 	%fd13, %fd12, %fd11;
	cvt.f64.f32	%fd14, %f10;
	add.f64 	%fd15, %fd14, %fd13;
	cvt.f64.f32	%fd16, %f11;
	add.f64 	%fd17, %fd16, %fd15;
	cvt.f64.f32	%fd18, %f12;
	add.f64 	%fd19, %fd18, %fd17;
	ld.global.v4.f32 	{%f17, %f18, %f19, %f20}, [%rd4+32];
	cvt.f64.f32	%fd20, %f17;
	add.f64 	%fd21, %fd20, %fd19;
	cvt.f64.f32	%fd22, %f18;
	add.f64 	%fd23, %fd22, %fd21;
	cvt.f64.f32	%fd24, %f19;
	add.f64 	%fd25, %fd24, %fd23;
	cvt.f64.f32	%fd26, %f20;
	add.f64 	%fd27, %fd26, %fd25;
	ld.global.v4.f32 	{%f25, %f26, %f27, %f28}, [%rd4+48];
	cvt.f64.f32	%fd28, %f25;
	add.f64 	%fd29, %fd28, %fd27;
	cvt.f64.f32	%fd30, %f26;
	add.f64 	%fd31, %fd30, %fd29;
	cvt.f64.f32	%fd32, %f27;
	add.f64 	%fd33, %fd32, %fd31;
	cvt.f64.f32	%fd34, %f28;
	add.f64 	%fd35, %fd34, %fd33;
	add.s32 	%r16, %r17, 1;
	mul.wide.u32 	%rd5, %r16, 64;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.v4.f32 	{%f33, %f34, %f35, %f36}, [%rd6];
	cvt.f64.f32	%fd36, %f33;
	add.f64 	%fd37, %fd35, %fd36;
	cvt.f64.f32	%fd38, %f34;
	add.f64 	%fd39, %fd38, %fd37;
	cvt.f64.f32	%fd40, %f35;
	add.f64 	%fd41, %fd40, %fd39;
	cvt.f64.f32	%fd42, %f36;
	add.f64 	%fd43, %fd42, %fd41;
	ld.global.v4.f32 	{%f41, %f42, %f43, %f44}, [%rd6+16];
	cvt.f64.f32	%fd44, %f41;
	add.f64 	%fd45, %fd44, %fd43;
	cvt.f64.f32	%fd46, %f42;
	add.f64 	%fd47, %fd46, %fd45;
	cvt.f64.f32	%fd48, %f43;
	add.f64 	%fd49, %fd48, %fd47;
	cvt.f64.f32	%fd50, %f44;
	add.f64 	%fd51, %fd50, %fd49;
	ld.global.v4.f32 	{%f49, %f50, %f51, %f52}, [%rd6+32];
	cvt.f64.f32	%fd52, %f49;
	add.f64 	%fd53, %fd52, %fd51;
	cvt.f64.f32	%fd54, %f50;
	add.f64 	%fd55, %fd54, %fd53;
	cvt.f64.f32	%fd56, %f51;
	add.f64 	%fd57, %fd56, %fd55;
	cvt.f64.f32	%fd58, %f52;
	add.f64 	%fd59, %fd58, %fd57;
	ld.global.v4.f32 	{%f57, %f58, %f59, %f60}, [%rd6+48];
	cvt.f64.f32	%fd60, %f57;
	add.f64 	%fd61, %fd60, %fd59;
	cvt.f64.f32	%fd62, %f58;
	add.f64 	%fd63, %fd62, %fd61;
	cvt.f64.f32	%fd64, %f59;
	add.f64 	%fd65, %fd64, %fd63;
	cvt.f64.f32	%fd66, %f60;
	add.f64 	%fd67, %fd66, %fd65;
	add.s32 	%r17, %r17, 2;
	add.s32 	%r18, %r18, 2;
	setp.ne.s32	%p1, %r18, 0;
	@%p1 bra 	BB48_1;

	mul.wide.u32 	%rd7, %r1, 8;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.f64 	[%rd8], %fd67;
	ret;
}

	// .globl	float44_sum_N
.entry float44_sum_N(
	.param .u64 .ptr .global .align 16 float44_sum_N_param_0,
	.param .u32 float44_sum_N_param_1,
	.param .u32 float44_sum_N_param_2,
	.param .u32 float44_sum_N_param_3,
	.param .u64 .ptr .global .align 8 float44_sum_N_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<33>;
	.reg .b32 	%r<20>;
	.reg .f64 	%fd<38>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [float44_sum_N_param_0];
	ld.param.u32 	%r8, [float44_sum_N_param_1];
	ld.param.u32 	%r9, [float44_sum_N_param_2];
	ld.param.u32 	%r10, [float44_sum_N_param_3];
	ld.param.u64 	%rd2, [float44_sum_N_param_4];
	mov.b32	%r11, %envreg3;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %ntid.x;
	mad.lo.s32 	%r1, %r12, %r13, %r11;
	mov.u32 	%r2, %tid.x;
	setp.eq.s32	%p1, %r10, 0;
	mov.f64 	%fd37, 0d0000000000000000;
	@%p1 bra 	BB49_3;

	add.s32 	%r15, %r1, %r2;
	mad.lo.s32 	%r18, %r10, %r15, %r8;
	mov.u32 	%r19, 0;
	mov.f64 	%fd37, 0d0000000000000000;

BB49_2:
	mul.wide.u32 	%rd3, %r18, 64;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd4];
	cvt.f64.f32	%fd6, %f1;
	add.f64 	%fd7, %fd37, %fd6;
	cvt.f64.f32	%fd8, %f2;
	add.f64 	%fd9, %fd8, %fd7;
	cvt.f64.f32	%fd10, %f3;
	add.f64 	%fd11, %fd10, %fd9;
	cvt.f64.f32	%fd12, %f4;
	add.f64 	%fd13, %fd12, %fd11;
	ld.global.v4.f32 	{%f9, %f10, %f11, %f12}, [%rd4+16];
	cvt.f64.f32	%fd14, %f9;
	add.f64 	%fd15, %fd14, %fd13;
	cvt.f64.f32	%fd16, %f10;
	add.f64 	%fd17, %fd16, %fd15;
	cvt.f64.f32	%fd18, %f11;
	add.f64 	%fd19, %fd18, %fd17;
	cvt.f64.f32	%fd20, %f12;
	add.f64 	%fd21, %fd20, %fd19;
	ld.global.v4.f32 	{%f17, %f18, %f19, %f20}, [%rd4+32];
	cvt.f64.f32	%fd22, %f17;
	add.f64 	%fd23, %fd22, %fd21;
	cvt.f64.f32	%fd24, %f18;
	add.f64 	%fd25, %fd24, %fd23;
	cvt.f64.f32	%fd26, %f19;
	add.f64 	%fd27, %fd26, %fd25;
	cvt.f64.f32	%fd28, %f20;
	add.f64 	%fd29, %fd28, %fd27;
	ld.global.v4.f32 	{%f25, %f26, %f27, %f28}, [%rd4+48];
	cvt.f64.f32	%fd30, %f25;
	add.f64 	%fd31, %fd30, %fd29;
	cvt.f64.f32	%fd32, %f26;
	add.f64 	%fd33, %fd32, %fd31;
	cvt.f64.f32	%fd34, %f27;
	add.f64 	%fd35, %fd34, %fd33;
	cvt.f64.f32	%fd36, %f28;
	add.f64 	%fd37, %fd36, %fd35;
	add.s32 	%r18, %r18, 1;
	add.s32 	%r19, %r19, 1;
	setp.lt.u32	%p2, %r19, %r10;
	@%p2 bra 	BB49_2;

BB49_3:
	add.s32 	%r16, %r1, %r2;
	add.s32 	%r17, %r16, %r9;
	mul.wide.u32 	%rd5, %r17, 8;
	add.s64 	%rd6, %rd2, %rd5;
	st.global.f64 	[%rd6], %fd37;
	ret;
}

	// .globl	float44_sum2d_16
.entry float44_sum2d_16(
	.param .u64 .ptr .global .align 16 float44_sum2d_16_param_0,
	.param .u32 float44_sum2d_16_param_1,
	.param .u64 .ptr .global .align 8 float44_sum2d_16_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<65>;
	.reg .b32 	%r<31>;
	.reg .f64 	%fd<68>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [float44_sum2d_16_param_0];
	ld.param.u32 	%r15, [float44_sum2d_16_param_1];
	ld.param.u64 	%rd2, [float44_sum2d_16_param_2];
	setp.eq.s32	%p1, %r15, 0;
	@%p1 bra 	BB50_5;

	mov.b32	%r17, %envreg3;
	mov.u32 	%r18, %ctaid.x;
	mov.u32 	%r19, %ntid.x;
	mad.lo.s32 	%r20, %r18, %r19, %r17;
	mov.u32 	%r21, %tid.x;
	add.s32 	%r22, %r20, %r21;
	mul.lo.s32 	%r1, %r22, %r15;
	shl.b32 	%r23, %r22, 4;
	add.s32 	%r24, %r23, 1;
	mul.lo.s32 	%r2, %r15, %r24;
	shl.b32 	%r3, %r15, 1;
	mul.lo.s32 	%r4, %r23, %r15;
	mov.u32 	%r27, 0;

BB50_2:
	add.s32 	%r29, %r2, %r27;
	add.s32 	%r28, %r4, %r27;
	mov.f64 	%fd67, 0d0000000000000000;
	mov.u32 	%r30, -16;

BB50_3:
	mul.wide.u32 	%rd3, %r28, 64;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd4];
	cvt.f64.f32	%fd4, %f1;
	add.f64 	%fd5, %fd67, %fd4;
	cvt.f64.f32	%fd6, %f2;
	add.f64 	%fd7, %fd6, %fd5;
	cvt.f64.f32	%fd8, %f3;
	add.f64 	%fd9, %fd8, %fd7;
	cvt.f64.f32	%fd10, %f4;
	add.f64 	%fd11, %fd10, %fd9;
	ld.global.v4.f32 	{%f9, %f10, %f11, %f12}, [%rd4+16];
	cvt.f64.f32	%fd12, %f9;
	add.f64 	%fd13, %fd12, %fd11;
	cvt.f64.f32	%fd14, %f10;
	add.f64 	%fd15, %fd14, %fd13;
	cvt.f64.f32	%fd16, %f11;
	add.f64 	%fd17, %fd16, %fd15;
	cvt.f64.f32	%fd18, %f12;
	add.f64 	%fd19, %fd18, %fd17;
	ld.global.v4.f32 	{%f17, %f18, %f19, %f20}, [%rd4+32];
	cvt.f64.f32	%fd20, %f17;
	add.f64 	%fd21, %fd20, %fd19;
	cvt.f64.f32	%fd22, %f18;
	add.f64 	%fd23, %fd22, %fd21;
	cvt.f64.f32	%fd24, %f19;
	add.f64 	%fd25, %fd24, %fd23;
	cvt.f64.f32	%fd26, %f20;
	add.f64 	%fd27, %fd26, %fd25;
	ld.global.v4.f32 	{%f25, %f26, %f27, %f28}, [%rd4+48];
	cvt.f64.f32	%fd28, %f25;
	add.f64 	%fd29, %fd28, %fd27;
	cvt.f64.f32	%fd30, %f26;
	add.f64 	%fd31, %fd30, %fd29;
	cvt.f64.f32	%fd32, %f27;
	add.f64 	%fd33, %fd32, %fd31;
	cvt.f64.f32	%fd34, %f28;
	add.f64 	%fd35, %fd34, %fd33;
	mul.wide.u32 	%rd5, %r29, 64;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.v4.f32 	{%f33, %f34, %f35, %f36}, [%rd6];
	cvt.f64.f32	%fd36, %f33;
	add.f64 	%fd37, %fd35, %fd36;
	cvt.f64.f32	%fd38, %f34;
	add.f64 	%fd39, %fd38, %fd37;
	cvt.f64.f32	%fd40, %f35;
	add.f64 	%fd41, %fd40, %fd39;
	cvt.f64.f32	%fd42, %f36;
	add.f64 	%fd43, %fd42, %fd41;
	ld.global.v4.f32 	{%f41, %f42, %f43, %f44}, [%rd6+16];
	cvt.f64.f32	%fd44, %f41;
	add.f64 	%fd45, %fd44, %fd43;
	cvt.f64.f32	%fd46, %f42;
	add.f64 	%fd47, %fd46, %fd45;
	cvt.f64.f32	%fd48, %f43;
	add.f64 	%fd49, %fd48, %fd47;
	cvt.f64.f32	%fd50, %f44;
	add.f64 	%fd51, %fd50, %fd49;
	ld.global.v4.f32 	{%f49, %f50, %f51, %f52}, [%rd6+32];
	cvt.f64.f32	%fd52, %f49;
	add.f64 	%fd53, %fd52, %fd51;
	cvt.f64.f32	%fd54, %f50;
	add.f64 	%fd55, %fd54, %fd53;
	cvt.f64.f32	%fd56, %f51;
	add.f64 	%fd57, %fd56, %fd55;
	cvt.f64.f32	%fd58, %f52;
	add.f64 	%fd59, %fd58, %fd57;
	ld.global.v4.f32 	{%f57, %f58, %f59, %f60}, [%rd6+48];
	cvt.f64.f32	%fd60, %f57;
	add.f64 	%fd61, %fd60, %fd59;
	cvt.f64.f32	%fd62, %f58;
	add.f64 	%fd63, %fd62, %fd61;
	cvt.f64.f32	%fd64, %f59;
	add.f64 	%fd65, %fd64, %fd63;
	cvt.f64.f32	%fd66, %f60;
	add.f64 	%fd67, %fd66, %fd65;
	add.s32 	%r29, %r29, %r3;
	add.s32 	%r28, %r28, %r3;
	add.s32 	%r30, %r30, 2;
	setp.ne.s32	%p2, %r30, 0;
	@%p2 bra 	BB50_3;

	add.s32 	%r26, %r27, %r1;
	mul.wide.u32 	%rd7, %r26, 8;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.f64 	[%rd8], %fd67;
	add.s32 	%r27, %r27, 1;
	setp.lt.u32	%p3, %r27, %r15;
	@%p3 bra 	BB50_2;

BB50_5:
	ret;
}

	// .globl	float44_sum2d_N
.entry float44_sum2d_N(
	.param .u64 .ptr .global .align 16 float44_sum2d_N_param_0,
	.param .u32 float44_sum2d_N_param_1,
	.param .u32 float44_sum2d_N_param_2,
	.param .u32 float44_sum2d_N_param_3,
	.param .u32 float44_sum2d_N_param_4,
	.param .u64 .ptr .global .align 8 float44_sum2d_N_param_5
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<33>;
	.reg .b32 	%r<28>;
	.reg .f64 	%fd<40>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [float44_sum2d_N_param_0];
	ld.param.u32 	%r10, [float44_sum2d_N_param_1];
	ld.param.u32 	%r11, [float44_sum2d_N_param_2];
	ld.param.u32 	%r12, [float44_sum2d_N_param_3];
	ld.param.u32 	%r13, [float44_sum2d_N_param_4];
	ld.param.u64 	%rd2, [float44_sum2d_N_param_5];
	setp.eq.s32	%p1, %r10, 0;
	@%p1 bra 	BB51_5;

	mov.b32	%r15, %envreg3;
	mov.u32 	%r16, %ctaid.x;
	mov.u32 	%r17, %ntid.x;
	mad.lo.s32 	%r18, %r16, %r17, %r15;
	mov.u32 	%r19, %tid.x;
	add.s32 	%r20, %r18, %r19;
	mul.lo.s32 	%r21, %r20, %r10;
	add.s32 	%r1, %r21, %r12;
	mad.lo.s32 	%r2, %r21, %r13, %r11;
	mov.u32 	%r14, 0;
	mov.u32 	%r27, %r14;

BB51_2:
	add.s32 	%r24, %r2, %r27;
	setp.eq.s32	%p2, %r13, 0;
	mov.f64 	%fd39, 0d0000000000000000;
	mov.f64 	%fd38, %fd39;
	mov.u32 	%r26, %r14;
	@%p2 bra 	BB51_4;

BB51_3:
	mov.u32 	%r6, %r26;
	mul.wide.u32 	%rd3, %r24, 64;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd4];
	cvt.f64.f32	%fd6, %f1;
	add.f64 	%fd7, %fd39, %fd6;
	cvt.f64.f32	%fd8, %f2;
	add.f64 	%fd9, %fd8, %fd7;
	cvt.f64.f32	%fd10, %f3;
	add.f64 	%fd11, %fd10, %fd9;
	cvt.f64.f32	%fd12, %f4;
	add.f64 	%fd13, %fd12, %fd11;
	ld.global.v4.f32 	{%f9, %f10, %f11, %f12}, [%rd4+16];
	cvt.f64.f32	%fd14, %f9;
	add.f64 	%fd15, %fd14, %fd13;
	cvt.f64.f32	%fd16, %f10;
	add.f64 	%fd17, %fd16, %fd15;
	cvt.f64.f32	%fd18, %f11;
	add.f64 	%fd19, %fd18, %fd17;
	cvt.f64.f32	%fd20, %f12;
	add.f64 	%fd21, %fd20, %fd19;
	ld.global.v4.f32 	{%f17, %f18, %f19, %f20}, [%rd4+32];
	cvt.f64.f32	%fd22, %f17;
	add.f64 	%fd23, %fd22, %fd21;
	cvt.f64.f32	%fd24, %f18;
	add.f64 	%fd25, %fd24, %fd23;
	cvt.f64.f32	%fd26, %f19;
	add.f64 	%fd27, %fd26, %fd25;
	cvt.f64.f32	%fd28, %f20;
	add.f64 	%fd29, %fd28, %fd27;
	ld.global.v4.f32 	{%f25, %f26, %f27, %f28}, [%rd4+48];
	cvt.f64.f32	%fd30, %f25;
	add.f64 	%fd31, %fd30, %fd29;
	cvt.f64.f32	%fd32, %f26;
	add.f64 	%fd33, %fd32, %fd31;
	cvt.f64.f32	%fd34, %f27;
	add.f64 	%fd35, %fd34, %fd33;
	cvt.f64.f32	%fd36, %f28;
	add.f64 	%fd39, %fd36, %fd35;
	add.s32 	%r24, %r24, %r10;
	add.s32 	%r8, %r6, 1;
	setp.lt.u32	%p3, %r8, %r13;
	mov.u32 	%r26, %r8;
	mov.f64 	%fd38, %fd39;
	@%p3 bra 	BB51_3;

BB51_4:
	add.s32 	%r23, %r1, %r27;
	mul.wide.u32 	%rd5, %r23, 8;
	add.s64 	%rd6, %rd2, %rd5;
	st.global.f64 	[%rd6], %fd38;
	add.s32 	%r27, %r27, 1;
	setp.lt.u32	%p4, %r27, %r10;
	@%p4 bra 	BB51_2;

BB51_5:
	ret;
}

	// .globl	float44_sum2d_16_weighted
.entry float44_sum2d_16_weighted(
	.param .u64 .ptr .global .align 16 float44_sum2d_16_weighted_param_0,
	.param .u64 .ptr .global .align 16 float44_sum2d_16_weighted_param_1,
	.param .u32 float44_sum2d_16_weighted_param_2,
	.param .u64 .ptr .global .align 8 float44_sum2d_16_weighted_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<57>;
	.reg .b32 	%r<30>;
	.reg .f64 	%fd<36>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd1, [float44_sum2d_16_weighted_param_0];
	ld.param.u64 	%rd2, [float44_sum2d_16_weighted_param_1];
	ld.param.u32 	%r13, [float44_sum2d_16_weighted_param_2];
	ld.param.u64 	%rd3, [float44_sum2d_16_weighted_param_3];
	setp.eq.s32	%p1, %r13, 0;
	@%p1 bra 	BB52_5;

	mov.b32	%r15, %envreg3;
	mov.u32 	%r16, %ctaid.x;
	mov.u32 	%r17, %ntid.x;
	mul.lo.s32 	%r18, %r16, %r17;
	add.s32 	%r19, %r18, %r15;
	mov.u32 	%r20, %tid.x;
	add.s32 	%r21, %r19, %r20;
	mul.lo.s32 	%r1, %r21, %r13;
	shl.b32 	%r22, %r15, 4;
	mad.lo.s32 	%r23, %r18, 16, %r22;
	mad.lo.s32 	%r2, %r20, 16, %r23;
	shl.b32 	%r3, %r1, 4;
	mov.u32 	%r26, 0;

BB52_2:
	add.s32 	%r27, %r3, %r26;
	mov.f64 	%fd35, 0d0000000000000000;
	mov.u32 	%r29, -16;
	mov.u32 	%r28, %r2;

BB52_3:
	mov.u32 	%r7, %r28;
	mul.wide.u32 	%rd4, %r27, 64;
	add.s64 	%rd5, %rd2, %rd4;
	ld.global.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd5];
	mul.wide.u32 	%rd6, %r7, 16;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.v4.f32 	{%f6, %f7, %f8, %f9}, [%rd7];
	mul.f32 	%f11, %f1, %f6;
	cvt.f64.f32	%fd4, %f11;
	add.f64 	%fd5, %fd35, %fd4;
	mul.f32 	%f14, %f2, %f7;
	cvt.f64.f32	%fd6, %f14;
	add.f64 	%fd7, %fd5, %fd6;
	mul.f32 	%f17, %f3, %f8;
	cvt.f64.f32	%fd8, %f17;
	add.f64 	%fd9, %fd7, %fd8;
	mul.f32 	%f20, %f4, %f9;
	cvt.f64.f32	%fd10, %f20;
	add.f64 	%fd11, %fd9, %fd10;
	ld.global.v4.f32 	{%f21, %f22, %f23, %f24}, [%rd5+16];
	mul.f32 	%f26, %f21, %f6;
	cvt.f64.f32	%fd12, %f26;
	add.f64 	%fd13, %fd11, %fd12;
	mul.f32 	%f28, %f22, %f7;
	cvt.f64.f32	%fd14, %f28;
	add.f64 	%fd15, %fd13, %fd14;
	mul.f32 	%f30, %f23, %f8;
	cvt.f64.f32	%fd16, %f30;
	add.f64 	%fd17, %fd15, %fd16;
	mul.f32 	%f32, %f24, %f9;
	cvt.f64.f32	%fd18, %f32;
	add.f64 	%fd19, %fd17, %fd18;
	ld.global.v4.f32 	{%f33, %f34, %f35, %f36}, [%rd5+32];
	mul.f32 	%f38, %f33, %f6;
	cvt.f64.f32	%fd20, %f38;
	add.f64 	%fd21, %fd19, %fd20;
	mul.f32 	%f40, %f34, %f7;
	cvt.f64.f32	%fd22, %f40;
	add.f64 	%fd23, %fd21, %fd22;
	mul.f32 	%f42, %f35, %f8;
	cvt.f64.f32	%fd24, %f42;
	add.f64 	%fd25, %fd23, %fd24;
	mul.f32 	%f44, %f36, %f9;
	cvt.f64.f32	%fd26, %f44;
	add.f64 	%fd27, %fd25, %fd26;
	ld.global.v4.f32 	{%f45, %f46, %f47, %f48}, [%rd5+48];
	mul.f32 	%f50, %f45, %f6;
	cvt.f64.f32	%fd28, %f50;
	add.f64 	%fd29, %fd27, %fd28;
	mul.f32 	%f52, %f46, %f7;
	cvt.f64.f32	%fd30, %f52;
	add.f64 	%fd31, %fd29, %fd30;
	mul.f32 	%f54, %f47, %f8;
	cvt.f64.f32	%fd32, %f54;
	add.f64 	%fd33, %fd31, %fd32;
	mul.f32 	%f56, %f48, %f9;
	cvt.f64.f32	%fd34, %f56;
	add.f64 	%fd35, %fd33, %fd34;
	add.s32 	%r9, %r7, 1;
	add.s32 	%r27, %r27, %r13;
	add.s32 	%r29, %r29, 1;
	setp.ne.s32	%p2, %r29, 0;
	mov.u32 	%r28, %r9;
	@%p2 bra 	BB52_3;

	add.s32 	%r25, %r26, %r1;
	mul.wide.u32 	%rd8, %r25, 8;
	add.s64 	%rd9, %rd3, %rd8;
	st.global.f64 	[%rd9], %fd35;
	add.s32 	%r26, %r26, 1;
	setp.lt.u32	%p3, %r26, %r13;
	@%p3 bra 	BB52_2;

BB52_5:
	ret;
}

	// .globl	float44_sum2d_N_weighted
.entry float44_sum2d_N_weighted(
	.param .u64 .ptr .global .align 16 float44_sum2d_N_weighted_param_0,
	.param .u64 .ptr .global .align 16 float44_sum2d_N_weighted_param_1,
	.param .u32 float44_sum2d_N_weighted_param_2,
	.param .u32 float44_sum2d_N_weighted_param_3,
	.param .u32 float44_sum2d_N_weighted_param_4,
	.param .u32 float44_sum2d_N_weighted_param_5,
	.param .u64 .ptr .global .align 8 float44_sum2d_N_weighted_param_6
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<57>;
	.reg .b32 	%r<31>;
	.reg .f64 	%fd<40>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd1, [float44_sum2d_N_weighted_param_0];
	ld.param.u64 	%rd2, [float44_sum2d_N_weighted_param_1];
	ld.param.u32 	%r13, [float44_sum2d_N_weighted_param_2];
	ld.param.u32 	%r14, [float44_sum2d_N_weighted_param_3];
	ld.param.u32 	%r15, [float44_sum2d_N_weighted_param_4];
	ld.param.u32 	%r16, [float44_sum2d_N_weighted_param_5];
	ld.param.u64 	%rd3, [float44_sum2d_N_weighted_param_6];
	setp.eq.s32	%p1, %r13, 0;
	@%p1 bra 	BB53_5;

	mov.b32	%r18, %envreg3;
	mov.u32 	%r19, %ctaid.x;
	mov.u32 	%r20, %ntid.x;
	mad.lo.s32 	%r21, %r19, %r20, %r18;
	mov.u32 	%r22, %tid.x;
	add.s32 	%r23, %r21, %r22;
	mad.lo.s32 	%r1, %r23, %r13, %r15;
	mul.lo.s32 	%r2, %r16, %r23;
	mad.lo.s32 	%r3, %r2, %r13, %r14;
	mov.u32 	%r17, 0;
	mov.u32 	%r30, %r17;

BB53_2:
	add.s32 	%r26, %r3, %r30;
	setp.eq.s32	%p2, %r16, 0;
	mov.f64 	%fd39, 0d0000000000000000;
	mov.f64 	%fd38, %fd39;
	mov.u32 	%r27, %r2;
	mov.u32 	%r29, %r17;
	@%p2 bra 	BB53_4;

BB53_3:
	mov.u32 	%r8, %r29;
	mov.u32 	%r7, %r27;
	mul.wide.u32 	%rd4, %r26, 64;
	add.s64 	%rd5, %rd2, %rd4;
	ld.global.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd5];
	mul.wide.u32 	%rd6, %r7, 16;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.v4.f32 	{%f6, %f7, %f8, %f9}, [%rd7];
	mul.f32 	%f11, %f1, %f6;
	cvt.f64.f32	%fd6, %f11;
	add.f64 	%fd7, %fd39, %fd6;
	mul.f32 	%f14, %f2, %f7;
	cvt.f64.f32	%fd8, %f14;
	add.f64 	%fd9, %fd7, %fd8;
	mul.f32 	%f17, %f3, %f8;
	cvt.f64.f32	%fd10, %f17;
	add.f64 	%fd11, %fd9, %fd10;
	mul.f32 	%f20, %f4, %f9;
	cvt.f64.f32	%fd12, %f20;
	add.f64 	%fd13, %fd11, %fd12;
	ld.global.v4.f32 	{%f21, %f22, %f23, %f24}, [%rd5+16];
	mul.f32 	%f26, %f21, %f6;
	cvt.f64.f32	%fd14, %f26;
	add.f64 	%fd15, %fd13, %fd14;
	mul.f32 	%f28, %f22, %f7;
	cvt.f64.f32	%fd16, %f28;
	add.f64 	%fd17, %fd15, %fd16;
	mul.f32 	%f30, %f23, %f8;
	cvt.f64.f32	%fd18, %f30;
	add.f64 	%fd19, %fd17, %fd18;
	mul.f32 	%f32, %f24, %f9;
	cvt.f64.f32	%fd20, %f32;
	add.f64 	%fd21, %fd19, %fd20;
	ld.global.v4.f32 	{%f33, %f34, %f35, %f36}, [%rd5+32];
	mul.f32 	%f38, %f33, %f6;
	cvt.f64.f32	%fd22, %f38;
	add.f64 	%fd23, %fd21, %fd22;
	mul.f32 	%f40, %f34, %f7;
	cvt.f64.f32	%fd24, %f40;
	add.f64 	%fd25, %fd23, %fd24;
	mul.f32 	%f42, %f35, %f8;
	cvt.f64.f32	%fd26, %f42;
	add.f64 	%fd27, %fd25, %fd26;
	mul.f32 	%f44, %f36, %f9;
	cvt.f64.f32	%fd28, %f44;
	add.f64 	%fd29, %fd27, %fd28;
	ld.global.v4.f32 	{%f45, %f46, %f47, %f48}, [%rd5+48];
	mul.f32 	%f50, %f45, %f6;
	cvt.f64.f32	%fd30, %f50;
	add.f64 	%fd31, %fd29, %fd30;
	mul.f32 	%f52, %f46, %f7;
	cvt.f64.f32	%fd32, %f52;
	add.f64 	%fd33, %fd31, %fd32;
	mul.f32 	%f54, %f47, %f8;
	cvt.f64.f32	%fd34, %f54;
	add.f64 	%fd35, %fd33, %fd34;
	mul.f32 	%f56, %f48, %f9;
	cvt.f64.f32	%fd36, %f56;
	add.f64 	%fd39, %fd35, %fd36;
	add.s32 	%r9, %r7, 1;
	add.s32 	%r26, %r26, %r13;
	add.s32 	%r11, %r8, 1;
	setp.lt.u32	%p3, %r11, %r16;
	mov.u32 	%r27, %r9;
	mov.u32 	%r29, %r11;
	mov.f64 	%fd38, %fd39;
	@%p3 bra 	BB53_3;

BB53_4:
	add.s32 	%r25, %r1, %r30;
	mul.wide.u32 	%rd8, %r25, 8;
	add.s64 	%rd9, %rd3, %rd8;
	st.global.f64 	[%rd9], %fd38;
	add.s32 	%r30, %r30, 1;
	setp.lt.u32	%p4, %r30, %r13;
	@%p4 bra 	BB53_2;

BB53_5:
	ret;
}

	// .globl	float444_sum_16
.entry float444_sum_16(
	.param .u64 .ptr .global .align 16 float444_sum_16_param_0,
	.param .u64 .ptr .global .align 8 float444_sum_16_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<129>;
	.reg .b32 	%r<18>;
	.reg .f64 	%fd<132>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [float444_sum_16_param_0];
	ld.param.u64 	%rd2, [float444_sum_16_param_1];
	mov.b32	%r8, %envreg3;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %ntid.x;
	mul.lo.s32 	%r11, %r9, %r10;
	add.s32 	%r12, %r11, %r8;
	mov.u32 	%r13, %tid.x;
	add.s32 	%r1, %r12, %r13;
	shl.b32 	%r14, %r8, 4;
	mad.lo.s32 	%r15, %r11, 16, %r14;
	mad.lo.s32 	%r16, %r13, 16, %r15;
	mov.f64 	%fd131, 0d0000000000000000;
	mov.u32 	%r17, -16;

BB54_1:
	mul.wide.u32 	%rd3, %r16, 256;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd4];
	cvt.f64.f32	%fd4, %f1;
	add.f64 	%fd5, %fd131, %fd4;
	cvt.f64.f32	%fd6, %f2;
	add.f64 	%fd7, %fd6, %fd5;
	cvt.f64.f32	%fd8, %f3;
	add.f64 	%fd9, %fd8, %fd7;
	cvt.f64.f32	%fd10, %f4;
	add.f64 	%fd11, %fd10, %fd9;
	ld.global.v4.f32 	{%f9, %f10, %f11, %f12}, [%rd4+16];
	cvt.f64.f32	%fd12, %f9;
	add.f64 	%fd13, %fd12, %fd11;
	cvt.f64.f32	%fd14, %f10;
	add.f64 	%fd15, %fd14, %fd13;
	cvt.f64.f32	%fd16, %f11;
	add.f64 	%fd17, %fd16, %fd15;
	cvt.f64.f32	%fd18, %f12;
	add.f64 	%fd19, %fd18, %fd17;
	ld.global.v4.f32 	{%f17, %f18, %f19, %f20}, [%rd4+32];
	cvt.f64.f32	%fd20, %f17;
	add.f64 	%fd21, %fd20, %fd19;
	cvt.f64.f32	%fd22, %f18;
	add.f64 	%fd23, %fd22, %fd21;
	cvt.f64.f32	%fd24, %f19;
	add.f64 	%fd25, %fd24, %fd23;
	cvt.f64.f32	%fd26, %f20;
	add.f64 	%fd27, %fd26, %fd25;
	ld.global.v4.f32 	{%f25, %f26, %f27, %f28}, [%rd4+48];
	cvt.f64.f32	%fd28, %f25;
	add.f64 	%fd29, %fd28, %fd27;
	cvt.f64.f32	%fd30, %f26;
	add.f64 	%fd31, %fd30, %fd29;
	cvt.f64.f32	%fd32, %f27;
	add.f64 	%fd33, %fd32, %fd31;
	cvt.f64.f32	%fd34, %f28;
	add.f64 	%fd35, %fd34, %fd33;
	ld.global.v4.f32 	{%f33, %f34, %f35, %f36}, [%rd4+64];
	cvt.f64.f32	%fd36, %f33;
	add.f64 	%fd37, %fd36, %fd35;
	cvt.f64.f32	%fd38, %f34;
	add.f64 	%fd39, %fd38, %fd37;
	cvt.f64.f32	%fd40, %f35;
	add.f64 	%fd41, %fd40, %fd39;
	cvt.f64.f32	%fd42, %f36;
	add.f64 	%fd43, %fd42, %fd41;
	ld.global.v4.f32 	{%f41, %f42, %f43, %f44}, [%rd4+80];
	cvt.f64.f32	%fd44, %f41;
	add.f64 	%fd45, %fd44, %fd43;
	cvt.f64.f32	%fd46, %f42;
	add.f64 	%fd47, %fd46, %fd45;
	cvt.f64.f32	%fd48, %f43;
	add.f64 	%fd49, %fd48, %fd47;
	cvt.f64.f32	%fd50, %f44;
	add.f64 	%fd51, %fd50, %fd49;
	ld.global.v4.f32 	{%f49, %f50, %f51, %f52}, [%rd4+96];
	cvt.f64.f32	%fd52, %f49;
	add.f64 	%fd53, %fd52, %fd51;
	cvt.f64.f32	%fd54, %f50;
	add.f64 	%fd55, %fd54, %fd53;
	cvt.f64.f32	%fd56, %f51;
	add.f64 	%fd57, %fd56, %fd55;
	cvt.f64.f32	%fd58, %f52;
	add.f64 	%fd59, %fd58, %fd57;
	ld.global.v4.f32 	{%f57, %f58, %f59, %f60}, [%rd4+112];
	cvt.f64.f32	%fd60, %f57;
	add.f64 	%fd61, %fd60, %fd59;
	cvt.f64.f32	%fd62, %f58;
	add.f64 	%fd63, %fd62, %fd61;
	cvt.f64.f32	%fd64, %f59;
	add.f64 	%fd65, %fd64, %fd63;
	cvt.f64.f32	%fd66, %f60;
	add.f64 	%fd67, %fd66, %fd65;
	ld.global.v4.f32 	{%f65, %f66, %f67, %f68}, [%rd4+128];
	cvt.f64.f32	%fd68, %f65;
	add.f64 	%fd69, %fd68, %fd67;
	cvt.f64.f32	%fd70, %f66;
	add.f64 	%fd71, %fd70, %fd69;
	cvt.f64.f32	%fd72, %f67;
	add.f64 	%fd73, %fd72, %fd71;
	cvt.f64.f32	%fd74, %f68;
	add.f64 	%fd75, %fd74, %fd73;
	ld.global.v4.f32 	{%f73, %f74, %f75, %f76}, [%rd4+144];
	cvt.f64.f32	%fd76, %f73;
	add.f64 	%fd77, %fd76, %fd75;
	cvt.f64.f32	%fd78, %f74;
	add.f64 	%fd79, %fd78, %fd77;
	cvt.f64.f32	%fd80, %f75;
	add.f64 	%fd81, %fd80, %fd79;
	cvt.f64.f32	%fd82, %f76;
	add.f64 	%fd83, %fd82, %fd81;
	ld.global.v4.f32 	{%f81, %f82, %f83, %f84}, [%rd4+160];
	cvt.f64.f32	%fd84, %f81;
	add.f64 	%fd85, %fd84, %fd83;
	cvt.f64.f32	%fd86, %f82;
	add.f64 	%fd87, %fd86, %fd85;
	cvt.f64.f32	%fd88, %f83;
	add.f64 	%fd89, %fd88, %fd87;
	cvt.f64.f32	%fd90, %f84;
	add.f64 	%fd91, %fd90, %fd89;
	ld.global.v4.f32 	{%f89, %f90, %f91, %f92}, [%rd4+176];
	cvt.f64.f32	%fd92, %f89;
	add.f64 	%fd93, %fd92, %fd91;
	cvt.f64.f32	%fd94, %f90;
	add.f64 	%fd95, %fd94, %fd93;
	cvt.f64.f32	%fd96, %f91;
	add.f64 	%fd97, %fd96, %fd95;
	cvt.f64.f32	%fd98, %f92;
	add.f64 	%fd99, %fd98, %fd97;
	ld.global.v4.f32 	{%f97, %f98, %f99, %f100}, [%rd4+192];
	cvt.f64.f32	%fd100, %f97;
	add.f64 	%fd101, %fd100, %fd99;
	cvt.f64.f32	%fd102, %f98;
	add.f64 	%fd103, %fd102, %fd101;
	cvt.f64.f32	%fd104, %f99;
	add.f64 	%fd105, %fd104, %fd103;
	cvt.f64.f32	%fd106, %f100;
	add.f64 	%fd107, %fd106, %fd105;
	ld.global.v4.f32 	{%f105, %f106, %f107, %f108}, [%rd4+208];
	cvt.f64.f32	%fd108, %f105;
	add.f64 	%fd109, %fd108, %fd107;
	cvt.f64.f32	%fd110, %f106;
	add.f64 	%fd111, %fd110, %fd109;
	cvt.f64.f32	%fd112, %f107;
	add.f64 	%fd113, %fd112, %fd111;
	cvt.f64.f32	%fd114, %f108;
	add.f64 	%fd115, %fd114, %fd113;
	ld.global.v4.f32 	{%f113, %f114, %f115, %f116}, [%rd4+224];
	cvt.f64.f32	%fd116, %f113;
	add.f64 	%fd117, %fd116, %fd115;
	cvt.f64.f32	%fd118, %f114;
	add.f64 	%fd119, %fd118, %fd117;
	cvt.f64.f32	%fd120, %f115;
	add.f64 	%fd121, %fd120, %fd119;
	cvt.f64.f32	%fd122, %f116;
	add.f64 	%fd123, %fd122, %fd121;
	ld.global.v4.f32 	{%f121, %f122, %f123, %f124}, [%rd4+240];
	cvt.f64.f32	%fd124, %f121;
	add.f64 	%fd125, %fd124, %fd123;
	cvt.f64.f32	%fd126, %f122;
	add.f64 	%fd127, %fd126, %fd125;
	cvt.f64.f32	%fd128, %f123;
	add.f64 	%fd129, %fd128, %fd127;
	cvt.f64.f32	%fd130, %f124;
	add.f64 	%fd131, %fd130, %fd129;
	add.s32 	%r16, %r16, 1;
	add.s32 	%r17, %r17, 1;
	setp.ne.s32	%p1, %r17, 0;
	@%p1 bra 	BB54_1;

	mul.wide.u32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd2, %rd5;
	st.global.f64 	[%rd6], %fd131;
	ret;
}

	// .globl	float444_sum_N
.entry float444_sum_N(
	.param .u64 .ptr .global .align 16 float444_sum_N_param_0,
	.param .u32 float444_sum_N_param_1,
	.param .u32 float444_sum_N_param_2,
	.param .u32 float444_sum_N_param_3,
	.param .u64 .ptr .global .align 8 float444_sum_N_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<129>;
	.reg .b32 	%r<20>;
	.reg .f64 	%fd<134>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [float444_sum_N_param_0];
	ld.param.u32 	%r8, [float444_sum_N_param_1];
	ld.param.u32 	%r9, [float444_sum_N_param_2];
	ld.param.u32 	%r10, [float444_sum_N_param_3];
	ld.param.u64 	%rd2, [float444_sum_N_param_4];
	mov.b32	%r11, %envreg3;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %ntid.x;
	mad.lo.s32 	%r1, %r12, %r13, %r11;
	mov.u32 	%r2, %tid.x;
	setp.eq.s32	%p1, %r10, 0;
	mov.f64 	%fd133, 0d0000000000000000;
	@%p1 bra 	BB55_3;

	add.s32 	%r15, %r1, %r2;
	mad.lo.s32 	%r18, %r10, %r15, %r8;
	mov.u32 	%r19, 0;
	mov.f64 	%fd133, 0d0000000000000000;

BB55_2:
	mul.wide.u32 	%rd3, %r18, 256;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd4];
	cvt.f64.f32	%fd6, %f1;
	add.f64 	%fd7, %fd133, %fd6;
	cvt.f64.f32	%fd8, %f2;
	add.f64 	%fd9, %fd8, %fd7;
	cvt.f64.f32	%fd10, %f3;
	add.f64 	%fd11, %fd10, %fd9;
	cvt.f64.f32	%fd12, %f4;
	add.f64 	%fd13, %fd12, %fd11;
	ld.global.v4.f32 	{%f9, %f10, %f11, %f12}, [%rd4+16];
	cvt.f64.f32	%fd14, %f9;
	add.f64 	%fd15, %fd14, %fd13;
	cvt.f64.f32	%fd16, %f10;
	add.f64 	%fd17, %fd16, %fd15;
	cvt.f64.f32	%fd18, %f11;
	add.f64 	%fd19, %fd18, %fd17;
	cvt.f64.f32	%fd20, %f12;
	add.f64 	%fd21, %fd20, %fd19;
	ld.global.v4.f32 	{%f17, %f18, %f19, %f20}, [%rd4+32];
	cvt.f64.f32	%fd22, %f17;
	add.f64 	%fd23, %fd22, %fd21;
	cvt.f64.f32	%fd24, %f18;
	add.f64 	%fd25, %fd24, %fd23;
	cvt.f64.f32	%fd26, %f19;
	add.f64 	%fd27, %fd26, %fd25;
	cvt.f64.f32	%fd28, %f20;
	add.f64 	%fd29, %fd28, %fd27;
	ld.global.v4.f32 	{%f25, %f26, %f27, %f28}, [%rd4+48];
	cvt.f64.f32	%fd30, %f25;
	add.f64 	%fd31, %fd30, %fd29;
	cvt.f64.f32	%fd32, %f26;
	add.f64 	%fd33, %fd32, %fd31;
	cvt.f64.f32	%fd34, %f27;
	add.f64 	%fd35, %fd34, %fd33;
	cvt.f64.f32	%fd36, %f28;
	add.f64 	%fd37, %fd36, %fd35;
	ld.global.v4.f32 	{%f33, %f34, %f35, %f36}, [%rd4+64];
	cvt.f64.f32	%fd38, %f33;
	add.f64 	%fd39, %fd38, %fd37;
	cvt.f64.f32	%fd40, %f34;
	add.f64 	%fd41, %fd40, %fd39;
	cvt.f64.f32	%fd42, %f35;
	add.f64 	%fd43, %fd42, %fd41;
	cvt.f64.f32	%fd44, %f36;
	add.f64 	%fd45, %fd44, %fd43;
	ld.global.v4.f32 	{%f41, %f42, %f43, %f44}, [%rd4+80];
	cvt.f64.f32	%fd46, %f41;
	add.f64 	%fd47, %fd46, %fd45;
	cvt.f64.f32	%fd48, %f42;
	add.f64 	%fd49, %fd48, %fd47;
	cvt.f64.f32	%fd50, %f43;
	add.f64 	%fd51, %fd50, %fd49;
	cvt.f64.f32	%fd52, %f44;
	add.f64 	%fd53, %fd52, %fd51;
	ld.global.v4.f32 	{%f49, %f50, %f51, %f52}, [%rd4+96];
	cvt.f64.f32	%fd54, %f49;
	add.f64 	%fd55, %fd54, %fd53;
	cvt.f64.f32	%fd56, %f50;
	add.f64 	%fd57, %fd56, %fd55;
	cvt.f64.f32	%fd58, %f51;
	add.f64 	%fd59, %fd58, %fd57;
	cvt.f64.f32	%fd60, %f52;
	add.f64 	%fd61, %fd60, %fd59;
	ld.global.v4.f32 	{%f57, %f58, %f59, %f60}, [%rd4+112];
	cvt.f64.f32	%fd62, %f57;
	add.f64 	%fd63, %fd62, %fd61;
	cvt.f64.f32	%fd64, %f58;
	add.f64 	%fd65, %fd64, %fd63;
	cvt.f64.f32	%fd66, %f59;
	add.f64 	%fd67, %fd66, %fd65;
	cvt.f64.f32	%fd68, %f60;
	add.f64 	%fd69, %fd68, %fd67;
	ld.global.v4.f32 	{%f65, %f66, %f67, %f68}, [%rd4+128];
	cvt.f64.f32	%fd70, %f65;
	add.f64 	%fd71, %fd70, %fd69;
	cvt.f64.f32	%fd72, %f66;
	add.f64 	%fd73, %fd72, %fd71;
	cvt.f64.f32	%fd74, %f67;
	add.f64 	%fd75, %fd74, %fd73;
	cvt.f64.f32	%fd76, %f68;
	add.f64 	%fd77, %fd76, %fd75;
	ld.global.v4.f32 	{%f73, %f74, %f75, %f76}, [%rd4+144];
	cvt.f64.f32	%fd78, %f73;
	add.f64 	%fd79, %fd78, %fd77;
	cvt.f64.f32	%fd80, %f74;
	add.f64 	%fd81, %fd80, %fd79;
	cvt.f64.f32	%fd82, %f75;
	add.f64 	%fd83, %fd82, %fd81;
	cvt.f64.f32	%fd84, %f76;
	add.f64 	%fd85, %fd84, %fd83;
	ld.global.v4.f32 	{%f81, %f82, %f83, %f84}, [%rd4+160];
	cvt.f64.f32	%fd86, %f81;
	add.f64 	%fd87, %fd86, %fd85;
	cvt.f64.f32	%fd88, %f82;
	add.f64 	%fd89, %fd88, %fd87;
	cvt.f64.f32	%fd90, %f83;
	add.f64 	%fd91, %fd90, %fd89;
	cvt.f64.f32	%fd92, %f84;
	add.f64 	%fd93, %fd92, %fd91;
	ld.global.v4.f32 	{%f89, %f90, %f91, %f92}, [%rd4+176];
	cvt.f64.f32	%fd94, %f89;
	add.f64 	%fd95, %fd94, %fd93;
	cvt.f64.f32	%fd96, %f90;
	add.f64 	%fd97, %fd96, %fd95;
	cvt.f64.f32	%fd98, %f91;
	add.f64 	%fd99, %fd98, %fd97;
	cvt.f64.f32	%fd100, %f92;
	add.f64 	%fd101, %fd100, %fd99;
	ld.global.v4.f32 	{%f97, %f98, %f99, %f100}, [%rd4+192];
	cvt.f64.f32	%fd102, %f97;
	add.f64 	%fd103, %fd102, %fd101;
	cvt.f64.f32	%fd104, %f98;
	add.f64 	%fd105, %fd104, %fd103;
	cvt.f64.f32	%fd106, %f99;
	add.f64 	%fd107, %fd106, %fd105;
	cvt.f64.f32	%fd108, %f100;
	add.f64 	%fd109, %fd108, %fd107;
	ld.global.v4.f32 	{%f105, %f106, %f107, %f108}, [%rd4+208];
	cvt.f64.f32	%fd110, %f105;
	add.f64 	%fd111, %fd110, %fd109;
	cvt.f64.f32	%fd112, %f106;
	add.f64 	%fd113, %fd112, %fd111;
	cvt.f64.f32	%fd114, %f107;
	add.f64 	%fd115, %fd114, %fd113;
	cvt.f64.f32	%fd116, %f108;
	add.f64 	%fd117, %fd116, %fd115;
	ld.global.v4.f32 	{%f113, %f114, %f115, %f116}, [%rd4+224];
	cvt.f64.f32	%fd118, %f113;
	add.f64 	%fd119, %fd118, %fd117;
	cvt.f64.f32	%fd120, %f114;
	add.f64 	%fd121, %fd120, %fd119;
	cvt.f64.f32	%fd122, %f115;
	add.f64 	%fd123, %fd122, %fd121;
	cvt.f64.f32	%fd124, %f116;
	add.f64 	%fd125, %fd124, %fd123;
	ld.global.v4.f32 	{%f121, %f122, %f123, %f124}, [%rd4+240];
	cvt.f64.f32	%fd126, %f121;
	add.f64 	%fd127, %fd126, %fd125;
	cvt.f64.f32	%fd128, %f122;
	add.f64 	%fd129, %fd128, %fd127;
	cvt.f64.f32	%fd130, %f123;
	add.f64 	%fd131, %fd130, %fd129;
	cvt.f64.f32	%fd132, %f124;
	add.f64 	%fd133, %fd132, %fd131;
	add.s32 	%r18, %r18, 1;
	add.s32 	%r19, %r19, 1;
	setp.lt.u32	%p2, %r19, %r10;
	@%p2 bra 	BB55_2;

BB55_3:
	add.s32 	%r16, %r1, %r2;
	add.s32 	%r17, %r16, %r9;
	mul.wide.u32 	%rd5, %r17, 8;
	add.s64 	%rd6, %rd2, %rd5;
	st.global.f64 	[%rd6], %fd133;
	ret;
}

	// .globl	float444_sum2d_16
.entry float444_sum2d_16(
	.param .u64 .ptr .global .align 16 float444_sum2d_16_param_0,
	.param .u32 float444_sum2d_16_param_1,
	.param .u64 .ptr .global .align 8 float444_sum2d_16_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<129>;
	.reg .b32 	%r<23>;
	.reg .f64 	%fd<132>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [float444_sum2d_16_param_0];
	ld.param.u32 	%r10, [float444_sum2d_16_param_1];
	ld.param.u64 	%rd2, [float444_sum2d_16_param_2];
	setp.eq.s32	%p1, %r10, 0;
	@%p1 bra 	BB56_5;

	mov.b32	%r12, %envreg3;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %ntid.x;
	mad.lo.s32 	%r15, %r13, %r14, %r12;
	mov.u32 	%r16, %tid.x;
	add.s32 	%r17, %r15, %r16;
	mul.lo.s32 	%r1, %r17, %r10;
	shl.b32 	%r2, %r1, 4;
	mov.u32 	%r20, 0;

BB56_2:
	add.s32 	%r21, %r2, %r20;
	mov.f64 	%fd131, 0d0000000000000000;
	mov.u32 	%r22, -16;

BB56_3:
	mul.wide.u32 	%rd3, %r21, 256;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd4];
	cvt.f64.f32	%fd4, %f1;
	add.f64 	%fd5, %fd131, %fd4;
	cvt.f64.f32	%fd6, %f2;
	add.f64 	%fd7, %fd6, %fd5;
	cvt.f64.f32	%fd8, %f3;
	add.f64 	%fd9, %fd8, %fd7;
	cvt.f64.f32	%fd10, %f4;
	add.f64 	%fd11, %fd10, %fd9;
	ld.global.v4.f32 	{%f9, %f10, %f11, %f12}, [%rd4+16];
	cvt.f64.f32	%fd12, %f9;
	add.f64 	%fd13, %fd12, %fd11;
	cvt.f64.f32	%fd14, %f10;
	add.f64 	%fd15, %fd14, %fd13;
	cvt.f64.f32	%fd16, %f11;
	add.f64 	%fd17, %fd16, %fd15;
	cvt.f64.f32	%fd18, %f12;
	add.f64 	%fd19, %fd18, %fd17;
	ld.global.v4.f32 	{%f17, %f18, %f19, %f20}, [%rd4+32];
	cvt.f64.f32	%fd20, %f17;
	add.f64 	%fd21, %fd20, %fd19;
	cvt.f64.f32	%fd22, %f18;
	add.f64 	%fd23, %fd22, %fd21;
	cvt.f64.f32	%fd24, %f19;
	add.f64 	%fd25, %fd24, %fd23;
	cvt.f64.f32	%fd26, %f20;
	add.f64 	%fd27, %fd26, %fd25;
	ld.global.v4.f32 	{%f25, %f26, %f27, %f28}, [%rd4+48];
	cvt.f64.f32	%fd28, %f25;
	add.f64 	%fd29, %fd28, %fd27;
	cvt.f64.f32	%fd30, %f26;
	add.f64 	%fd31, %fd30, %fd29;
	cvt.f64.f32	%fd32, %f27;
	add.f64 	%fd33, %fd32, %fd31;
	cvt.f64.f32	%fd34, %f28;
	add.f64 	%fd35, %fd34, %fd33;
	ld.global.v4.f32 	{%f33, %f34, %f35, %f36}, [%rd4+64];
	cvt.f64.f32	%fd36, %f33;
	add.f64 	%fd37, %fd36, %fd35;
	cvt.f64.f32	%fd38, %f34;
	add.f64 	%fd39, %fd38, %fd37;
	cvt.f64.f32	%fd40, %f35;
	add.f64 	%fd41, %fd40, %fd39;
	cvt.f64.f32	%fd42, %f36;
	add.f64 	%fd43, %fd42, %fd41;
	ld.global.v4.f32 	{%f41, %f42, %f43, %f44}, [%rd4+80];
	cvt.f64.f32	%fd44, %f41;
	add.f64 	%fd45, %fd44, %fd43;
	cvt.f64.f32	%fd46, %f42;
	add.f64 	%fd47, %fd46, %fd45;
	cvt.f64.f32	%fd48, %f43;
	add.f64 	%fd49, %fd48, %fd47;
	cvt.f64.f32	%fd50, %f44;
	add.f64 	%fd51, %fd50, %fd49;
	ld.global.v4.f32 	{%f49, %f50, %f51, %f52}, [%rd4+96];
	cvt.f64.f32	%fd52, %f49;
	add.f64 	%fd53, %fd52, %fd51;
	cvt.f64.f32	%fd54, %f50;
	add.f64 	%fd55, %fd54, %fd53;
	cvt.f64.f32	%fd56, %f51;
	add.f64 	%fd57, %fd56, %fd55;
	cvt.f64.f32	%fd58, %f52;
	add.f64 	%fd59, %fd58, %fd57;
	ld.global.v4.f32 	{%f57, %f58, %f59, %f60}, [%rd4+112];
	cvt.f64.f32	%fd60, %f57;
	add.f64 	%fd61, %fd60, %fd59;
	cvt.f64.f32	%fd62, %f58;
	add.f64 	%fd63, %fd62, %fd61;
	cvt.f64.f32	%fd64, %f59;
	add.f64 	%fd65, %fd64, %fd63;
	cvt.f64.f32	%fd66, %f60;
	add.f64 	%fd67, %fd66, %fd65;
	ld.global.v4.f32 	{%f65, %f66, %f67, %f68}, [%rd4+128];
	cvt.f64.f32	%fd68, %f65;
	add.f64 	%fd69, %fd68, %fd67;
	cvt.f64.f32	%fd70, %f66;
	add.f64 	%fd71, %fd70, %fd69;
	cvt.f64.f32	%fd72, %f67;
	add.f64 	%fd73, %fd72, %fd71;
	cvt.f64.f32	%fd74, %f68;
	add.f64 	%fd75, %fd74, %fd73;
	ld.global.v4.f32 	{%f73, %f74, %f75, %f76}, [%rd4+144];
	cvt.f64.f32	%fd76, %f73;
	add.f64 	%fd77, %fd76, %fd75;
	cvt.f64.f32	%fd78, %f74;
	add.f64 	%fd79, %fd78, %fd77;
	cvt.f64.f32	%fd80, %f75;
	add.f64 	%fd81, %fd80, %fd79;
	cvt.f64.f32	%fd82, %f76;
	add.f64 	%fd83, %fd82, %fd81;
	ld.global.v4.f32 	{%f81, %f82, %f83, %f84}, [%rd4+160];
	cvt.f64.f32	%fd84, %f81;
	add.f64 	%fd85, %fd84, %fd83;
	cvt.f64.f32	%fd86, %f82;
	add.f64 	%fd87, %fd86, %fd85;
	cvt.f64.f32	%fd88, %f83;
	add.f64 	%fd89, %fd88, %fd87;
	cvt.f64.f32	%fd90, %f84;
	add.f64 	%fd91, %fd90, %fd89;
	ld.global.v4.f32 	{%f89, %f90, %f91, %f92}, [%rd4+176];
	cvt.f64.f32	%fd92, %f89;
	add.f64 	%fd93, %fd92, %fd91;
	cvt.f64.f32	%fd94, %f90;
	add.f64 	%fd95, %fd94, %fd93;
	cvt.f64.f32	%fd96, %f91;
	add.f64 	%fd97, %fd96, %fd95;
	cvt.f64.f32	%fd98, %f92;
	add.f64 	%fd99, %fd98, %fd97;
	ld.global.v4.f32 	{%f97, %f98, %f99, %f100}, [%rd4+192];
	cvt.f64.f32	%fd100, %f97;
	add.f64 	%fd101, %fd100, %fd99;
	cvt.f64.f32	%fd102, %f98;
	add.f64 	%fd103, %fd102, %fd101;
	cvt.f64.f32	%fd104, %f99;
	add.f64 	%fd105, %fd104, %fd103;
	cvt.f64.f32	%fd106, %f100;
	add.f64 	%fd107, %fd106, %fd105;
	ld.global.v4.f32 	{%f105, %f106, %f107, %f108}, [%rd4+208];
	cvt.f64.f32	%fd108, %f105;
	add.f64 	%fd109, %fd108, %fd107;
	cvt.f64.f32	%fd110, %f106;
	add.f64 	%fd111, %fd110, %fd109;
	cvt.f64.f32	%fd112, %f107;
	add.f64 	%fd113, %fd112, %fd111;
	cvt.f64.f32	%fd114, %f108;
	add.f64 	%fd115, %fd114, %fd113;
	ld.global.v4.f32 	{%f113, %f114, %f115, %f116}, [%rd4+224];
	cvt.f64.f32	%fd116, %f113;
	add.f64 	%fd117, %fd116, %fd115;
	cvt.f64.f32	%fd118, %f114;
	add.f64 	%fd119, %fd118, %fd117;
	cvt.f64.f32	%fd120, %f115;
	add.f64 	%fd121, %fd120, %fd119;
	cvt.f64.f32	%fd122, %f116;
	add.f64 	%fd123, %fd122, %fd121;
	ld.global.v4.f32 	{%f121, %f122, %f123, %f124}, [%rd4+240];
	cvt.f64.f32	%fd124, %f121;
	add.f64 	%fd125, %fd124, %fd123;
	cvt.f64.f32	%fd126, %f122;
	add.f64 	%fd127, %fd126, %fd125;
	cvt.f64.f32	%fd128, %f123;
	add.f64 	%fd129, %fd128, %fd127;
	cvt.f64.f32	%fd130, %f124;
	add.f64 	%fd131, %fd130, %fd129;
	add.s32 	%r21, %r21, %r10;
	add.s32 	%r22, %r22, 1;
	setp.ne.s32	%p2, %r22, 0;
	@%p2 bra 	BB56_3;

	add.s32 	%r19, %r20, %r1;
	mul.wide.u32 	%rd5, %r19, 8;
	add.s64 	%rd6, %rd2, %rd5;
	st.global.f64 	[%rd6], %fd131;
	add.s32 	%r20, %r20, 1;
	setp.lt.u32	%p3, %r20, %r10;
	@%p3 bra 	BB56_2;

BB56_5:
	ret;
}

	// .globl	float444_sum2d_N
.entry float444_sum2d_N(
	.param .u64 .ptr .global .align 16 float444_sum2d_N_param_0,
	.param .u32 float444_sum2d_N_param_1,
	.param .u32 float444_sum2d_N_param_2,
	.param .u32 float444_sum2d_N_param_3,
	.param .u32 float444_sum2d_N_param_4,
	.param .u64 .ptr .global .align 8 float444_sum2d_N_param_5
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<129>;
	.reg .b32 	%r<28>;
	.reg .f64 	%fd<136>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [float444_sum2d_N_param_0];
	ld.param.u32 	%r10, [float444_sum2d_N_param_1];
	ld.param.u32 	%r11, [float444_sum2d_N_param_2];
	ld.param.u32 	%r12, [float444_sum2d_N_param_3];
	ld.param.u32 	%r13, [float444_sum2d_N_param_4];
	ld.param.u64 	%rd2, [float444_sum2d_N_param_5];
	setp.eq.s32	%p1, %r10, 0;
	@%p1 bra 	BB57_5;

	mov.b32	%r15, %envreg3;
	mov.u32 	%r16, %ctaid.x;
	mov.u32 	%r17, %ntid.x;
	mad.lo.s32 	%r18, %r16, %r17, %r15;
	mov.u32 	%r19, %tid.x;
	add.s32 	%r20, %r18, %r19;
	mul.lo.s32 	%r21, %r20, %r10;
	add.s32 	%r1, %r21, %r12;
	mad.lo.s32 	%r2, %r21, %r13, %r11;
	mov.u32 	%r14, 0;
	mov.u32 	%r27, %r14;

BB57_2:
	add.s32 	%r24, %r2, %r27;
	setp.eq.s32	%p2, %r13, 0;
	mov.f64 	%fd135, 0d0000000000000000;
	mov.f64 	%fd134, %fd135;
	mov.u32 	%r26, %r14;
	@%p2 bra 	BB57_4;

BB57_3:
	mov.u32 	%r6, %r26;
	mul.wide.u32 	%rd3, %r24, 256;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd4];
	cvt.f64.f32	%fd6, %f1;
	add.f64 	%fd7, %fd135, %fd6;
	cvt.f64.f32	%fd8, %f2;
	add.f64 	%fd9, %fd8, %fd7;
	cvt.f64.f32	%fd10, %f3;
	add.f64 	%fd11, %fd10, %fd9;
	cvt.f64.f32	%fd12, %f4;
	add.f64 	%fd13, %fd12, %fd11;
	ld.global.v4.f32 	{%f9, %f10, %f11, %f12}, [%rd4+16];
	cvt.f64.f32	%fd14, %f9;
	add.f64 	%fd15, %fd14, %fd13;
	cvt.f64.f32	%fd16, %f10;
	add.f64 	%fd17, %fd16, %fd15;
	cvt.f64.f32	%fd18, %f11;
	add.f64 	%fd19, %fd18, %fd17;
	cvt.f64.f32	%fd20, %f12;
	add.f64 	%fd21, %fd20, %fd19;
	ld.global.v4.f32 	{%f17, %f18, %f19, %f20}, [%rd4+32];
	cvt.f64.f32	%fd22, %f17;
	add.f64 	%fd23, %fd22, %fd21;
	cvt.f64.f32	%fd24, %f18;
	add.f64 	%fd25, %fd24, %fd23;
	cvt.f64.f32	%fd26, %f19;
	add.f64 	%fd27, %fd26, %fd25;
	cvt.f64.f32	%fd28, %f20;
	add.f64 	%fd29, %fd28, %fd27;
	ld.global.v4.f32 	{%f25, %f26, %f27, %f28}, [%rd4+48];
	cvt.f64.f32	%fd30, %f25;
	add.f64 	%fd31, %fd30, %fd29;
	cvt.f64.f32	%fd32, %f26;
	add.f64 	%fd33, %fd32, %fd31;
	cvt.f64.f32	%fd34, %f27;
	add.f64 	%fd35, %fd34, %fd33;
	cvt.f64.f32	%fd36, %f28;
	add.f64 	%fd37, %fd36, %fd35;
	ld.global.v4.f32 	{%f33, %f34, %f35, %f36}, [%rd4+64];
	cvt.f64.f32	%fd38, %f33;
	add.f64 	%fd39, %fd38, %fd37;
	cvt.f64.f32	%fd40, %f34;
	add.f64 	%fd41, %fd40, %fd39;
	cvt.f64.f32	%fd42, %f35;
	add.f64 	%fd43, %fd42, %fd41;
	cvt.f64.f32	%fd44, %f36;
	add.f64 	%fd45, %fd44, %fd43;
	ld.global.v4.f32 	{%f41, %f42, %f43, %f44}, [%rd4+80];
	cvt.f64.f32	%fd46, %f41;
	add.f64 	%fd47, %fd46, %fd45;
	cvt.f64.f32	%fd48, %f42;
	add.f64 	%fd49, %fd48, %fd47;
	cvt.f64.f32	%fd50, %f43;
	add.f64 	%fd51, %fd50, %fd49;
	cvt.f64.f32	%fd52, %f44;
	add.f64 	%fd53, %fd52, %fd51;
	ld.global.v4.f32 	{%f49, %f50, %f51, %f52}, [%rd4+96];
	cvt.f64.f32	%fd54, %f49;
	add.f64 	%fd55, %fd54, %fd53;
	cvt.f64.f32	%fd56, %f50;
	add.f64 	%fd57, %fd56, %fd55;
	cvt.f64.f32	%fd58, %f51;
	add.f64 	%fd59, %fd58, %fd57;
	cvt.f64.f32	%fd60, %f52;
	add.f64 	%fd61, %fd60, %fd59;
	ld.global.v4.f32 	{%f57, %f58, %f59, %f60}, [%rd4+112];
	cvt.f64.f32	%fd62, %f57;
	add.f64 	%fd63, %fd62, %fd61;
	cvt.f64.f32	%fd64, %f58;
	add.f64 	%fd65, %fd64, %fd63;
	cvt.f64.f32	%fd66, %f59;
	add.f64 	%fd67, %fd66, %fd65;
	cvt.f64.f32	%fd68, %f60;
	add.f64 	%fd69, %fd68, %fd67;
	ld.global.v4.f32 	{%f65, %f66, %f67, %f68}, [%rd4+128];
	cvt.f64.f32	%fd70, %f65;
	add.f64 	%fd71, %fd70, %fd69;
	cvt.f64.f32	%fd72, %f66;
	add.f64 	%fd73, %fd72, %fd71;
	cvt.f64.f32	%fd74, %f67;
	add.f64 	%fd75, %fd74, %fd73;
	cvt.f64.f32	%fd76, %f68;
	add.f64 	%fd77, %fd76, %fd75;
	ld.global.v4.f32 	{%f73, %f74, %f75, %f76}, [%rd4+144];
	cvt.f64.f32	%fd78, %f73;
	add.f64 	%fd79, %fd78, %fd77;
	cvt.f64.f32	%fd80, %f74;
	add.f64 	%fd81, %fd80, %fd79;
	cvt.f64.f32	%fd82, %f75;
	add.f64 	%fd83, %fd82, %fd81;
	cvt.f64.f32	%fd84, %f76;
	add.f64 	%fd85, %fd84, %fd83;
	ld.global.v4.f32 	{%f81, %f82, %f83, %f84}, [%rd4+160];
	cvt.f64.f32	%fd86, %f81;
	add.f64 	%fd87, %fd86, %fd85;
	cvt.f64.f32	%fd88, %f82;
	add.f64 	%fd89, %fd88, %fd87;
	cvt.f64.f32	%fd90, %f83;
	add.f64 	%fd91, %fd90, %fd89;
	cvt.f64.f32	%fd92, %f84;
	add.f64 	%fd93, %fd92, %fd91;
	ld.global.v4.f32 	{%f89, %f90, %f91, %f92}, [%rd4+176];
	cvt.f64.f32	%fd94, %f89;
	add.f64 	%fd95, %fd94, %fd93;
	cvt.f64.f32	%fd96, %f90;
	add.f64 	%fd97, %fd96, %fd95;
	cvt.f64.f32	%fd98, %f91;
	add.f64 	%fd99, %fd98, %fd97;
	cvt.f64.f32	%fd100, %f92;
	add.f64 	%fd101, %fd100, %fd99;
	ld.global.v4.f32 	{%f97, %f98, %f99, %f100}, [%rd4+192];
	cvt.f64.f32	%fd102, %f97;
	add.f64 	%fd103, %fd102, %fd101;
	cvt.f64.f32	%fd104, %f98;
	add.f64 	%fd105, %fd104, %fd103;
	cvt.f64.f32	%fd106, %f99;
	add.f64 	%fd107, %fd106, %fd105;
	cvt.f64.f32	%fd108, %f100;
	add.f64 	%fd109, %fd108, %fd107;
	ld.global.v4.f32 	{%f105, %f106, %f107, %f108}, [%rd4+208];
	cvt.f64.f32	%fd110, %f105;
	add.f64 	%fd111, %fd110, %fd109;
	cvt.f64.f32	%fd112, %f106;
	add.f64 	%fd113, %fd112, %fd111;
	cvt.f64.f32	%fd114, %f107;
	add.f64 	%fd115, %fd114, %fd113;
	cvt.f64.f32	%fd116, %f108;
	add.f64 	%fd117, %fd116, %fd115;
	ld.global.v4.f32 	{%f113, %f114, %f115, %f116}, [%rd4+224];
	cvt.f64.f32	%fd118, %f113;
	add.f64 	%fd119, %fd118, %fd117;
	cvt.f64.f32	%fd120, %f114;
	add.f64 	%fd121, %fd120, %fd119;
	cvt.f64.f32	%fd122, %f115;
	add.f64 	%fd123, %fd122, %fd121;
	cvt.f64.f32	%fd124, %f116;
	add.f64 	%fd125, %fd124, %fd123;
	ld.global.v4.f32 	{%f121, %f122, %f123, %f124}, [%rd4+240];
	cvt.f64.f32	%fd126, %f121;
	add.f64 	%fd127, %fd126, %fd125;
	cvt.f64.f32	%fd128, %f122;
	add.f64 	%fd129, %fd128, %fd127;
	cvt.f64.f32	%fd130, %f123;
	add.f64 	%fd131, %fd130, %fd129;
	cvt.f64.f32	%fd132, %f124;
	add.f64 	%fd135, %fd132, %fd131;
	add.s32 	%r24, %r24, %r10;
	add.s32 	%r8, %r6, 1;
	setp.lt.u32	%p3, %r8, %r13;
	mov.u32 	%r26, %r8;
	mov.f64 	%fd134, %fd135;
	@%p3 bra 	BB57_3;

BB57_4:
	add.s32 	%r23, %r1, %r27;
	mul.wide.u32 	%rd5, %r23, 8;
	add.s64 	%rd6, %rd2, %rd5;
	st.global.f64 	[%rd6], %fd134;
	add.s32 	%r27, %r27, 1;
	setp.lt.u32	%p4, %r27, %r10;
	@%p4 bra 	BB57_2;

BB57_5:
	ret;
}

	// .globl	float444_sum2d_16_weighted
.entry float444_sum2d_16_weighted(
	.param .u64 .ptr .global .align 16 float444_sum2d_16_weighted_param_0,
	.param .u64 .ptr .global .align 16 float444_sum2d_16_weighted_param_1,
	.param .u32 float444_sum2d_16_weighted_param_2,
	.param .u64 .ptr .global .align 8 float444_sum2d_16_weighted_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<201>;
	.reg .b32 	%r<30>;
	.reg .f64 	%fd<132>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd1, [float444_sum2d_16_weighted_param_0];
	ld.param.u64 	%rd2, [float444_sum2d_16_weighted_param_1];
	ld.param.u32 	%r13, [float444_sum2d_16_weighted_param_2];
	ld.param.u64 	%rd3, [float444_sum2d_16_weighted_param_3];
	setp.eq.s32	%p1, %r13, 0;
	@%p1 bra 	BB58_5;

	mov.b32	%r15, %envreg3;
	mov.u32 	%r16, %ctaid.x;
	mov.u32 	%r17, %ntid.x;
	mul.lo.s32 	%r18, %r16, %r17;
	add.s32 	%r19, %r18, %r15;
	mov.u32 	%r20, %tid.x;
	add.s32 	%r21, %r19, %r20;
	mul.lo.s32 	%r1, %r21, %r13;
	shl.b32 	%r22, %r15, 4;
	mad.lo.s32 	%r23, %r18, 16, %r22;
	mad.lo.s32 	%r2, %r20, 16, %r23;
	shl.b32 	%r3, %r1, 4;
	mov.u32 	%r26, 0;

BB58_2:
	add.s32 	%r27, %r3, %r26;
	mov.f64 	%fd131, 0d0000000000000000;
	mov.u32 	%r29, -16;
	mov.u32 	%r28, %r2;

BB58_3:
	mov.u32 	%r7, %r28;
	mul.wide.u32 	%rd4, %r27, 256;
	add.s64 	%rd5, %rd2, %rd4;
	ld.global.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd5];
	mul.wide.u32 	%rd6, %r7, 16;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.v4.f32 	{%f6, %f7, %f8, %f9}, [%rd7];
	mul.f32 	%f11, %f1, %f6;
	cvt.f64.f32	%fd4, %f11;
	add.f64 	%fd5, %fd131, %fd4;
	mul.f32 	%f14, %f2, %f7;
	cvt.f64.f32	%fd6, %f14;
	add.f64 	%fd7, %fd5, %fd6;
	mul.f32 	%f17, %f3, %f8;
	cvt.f64.f32	%fd8, %f17;
	add.f64 	%fd9, %fd7, %fd8;
	mul.f32 	%f20, %f4, %f9;
	cvt.f64.f32	%fd10, %f20;
	add.f64 	%fd11, %fd9, %fd10;
	ld.global.v4.f32 	{%f21, %f22, %f23, %f24}, [%rd5+16];
	mul.f32 	%f26, %f21, %f6;
	cvt.f64.f32	%fd12, %f26;
	add.f64 	%fd13, %fd11, %fd12;
	mul.f32 	%f28, %f22, %f7;
	cvt.f64.f32	%fd14, %f28;
	add.f64 	%fd15, %fd13, %fd14;
	mul.f32 	%f30, %f23, %f8;
	cvt.f64.f32	%fd16, %f30;
	add.f64 	%fd17, %fd15, %fd16;
	mul.f32 	%f32, %f24, %f9;
	cvt.f64.f32	%fd18, %f32;
	add.f64 	%fd19, %fd17, %fd18;
	ld.global.v4.f32 	{%f33, %f34, %f35, %f36}, [%rd5+32];
	mul.f32 	%f38, %f33, %f6;
	cvt.f64.f32	%fd20, %f38;
	add.f64 	%fd21, %fd19, %fd20;
	mul.f32 	%f40, %f34, %f7;
	cvt.f64.f32	%fd22, %f40;
	add.f64 	%fd23, %fd21, %fd22;
	mul.f32 	%f42, %f35, %f8;
	cvt.f64.f32	%fd24, %f42;
	add.f64 	%fd25, %fd23, %fd24;
	mul.f32 	%f44, %f36, %f9;
	cvt.f64.f32	%fd26, %f44;
	add.f64 	%fd27, %fd25, %fd26;
	ld.global.v4.f32 	{%f45, %f46, %f47, %f48}, [%rd5+48];
	mul.f32 	%f50, %f45, %f6;
	cvt.f64.f32	%fd28, %f50;
	add.f64 	%fd29, %fd27, %fd28;
	mul.f32 	%f52, %f46, %f7;
	cvt.f64.f32	%fd30, %f52;
	add.f64 	%fd31, %fd29, %fd30;
	mul.f32 	%f54, %f47, %f8;
	cvt.f64.f32	%fd32, %f54;
	add.f64 	%fd33, %fd31, %fd32;
	mul.f32 	%f56, %f48, %f9;
	cvt.f64.f32	%fd34, %f56;
	add.f64 	%fd35, %fd33, %fd34;
	ld.global.v4.f32 	{%f57, %f58, %f59, %f60}, [%rd5+64];
	mul.f32 	%f62, %f57, %f6;
	cvt.f64.f32	%fd36, %f62;
	add.f64 	%fd37, %fd35, %fd36;
	mul.f32 	%f64, %f58, %f7;
	cvt.f64.f32	%fd38, %f64;
	add.f64 	%fd39, %fd37, %fd38;
	mul.f32 	%f66, %f59, %f8;
	cvt.f64.f32	%fd40, %f66;
	add.f64 	%fd41, %fd39, %fd40;
	mul.f32 	%f68, %f60, %f9;
	cvt.f64.f32	%fd42, %f68;
	add.f64 	%fd43, %fd41, %fd42;
	ld.global.v4.f32 	{%f69, %f70, %f71, %f72}, [%rd5+80];
	mul.f32 	%f74, %f69, %f6;
	cvt.f64.f32	%fd44, %f74;
	add.f64 	%fd45, %fd43, %fd44;
	mul.f32 	%f76, %f70, %f7;
	cvt.f64.f32	%fd46, %f76;
	add.f64 	%fd47, %fd45, %fd46;
	mul.f32 	%f78, %f71, %f8;
	cvt.f64.f32	%fd48, %f78;
	add.f64 	%fd49, %fd47, %fd48;
	mul.f32 	%f80, %f72, %f9;
	cvt.f64.f32	%fd50, %f80;
	add.f64 	%fd51, %fd49, %fd50;
	ld.global.v4.f32 	{%f81, %f82, %f83, %f84}, [%rd5+96];
	mul.f32 	%f86, %f81, %f6;
	cvt.f64.f32	%fd52, %f86;
	add.f64 	%fd53, %fd51, %fd52;
	mul.f32 	%f88, %f82, %f7;
	cvt.f64.f32	%fd54, %f88;
	add.f64 	%fd55, %fd53, %fd54;
	mul.f32 	%f90, %f83, %f8;
	cvt.f64.f32	%fd56, %f90;
	add.f64 	%fd57, %fd55, %fd56;
	mul.f32 	%f92, %f84, %f9;
	cvt.f64.f32	%fd58, %f92;
	add.f64 	%fd59, %fd57, %fd58;
	ld.global.v4.f32 	{%f93, %f94, %f95, %f96}, [%rd5+112];
	mul.f32 	%f98, %f93, %f6;
	cvt.f64.f32	%fd60, %f98;
	add.f64 	%fd61, %fd59, %fd60;
	mul.f32 	%f100, %f94, %f7;
	cvt.f64.f32	%fd62, %f100;
	add.f64 	%fd63, %fd61, %fd62;
	mul.f32 	%f102, %f95, %f8;
	cvt.f64.f32	%fd64, %f102;
	add.f64 	%fd65, %fd63, %fd64;
	mul.f32 	%f104, %f96, %f9;
	cvt.f64.f32	%fd66, %f104;
	add.f64 	%fd67, %fd65, %fd66;
	ld.global.v4.f32 	{%f105, %f106, %f107, %f108}, [%rd5+128];
	mul.f32 	%f110, %f105, %f6;
	cvt.f64.f32	%fd68, %f110;
	add.f64 	%fd69, %fd67, %fd68;
	mul.f32 	%f112, %f106, %f7;
	cvt.f64.f32	%fd70, %f112;
	add.f64 	%fd71, %fd69, %fd70;
	mul.f32 	%f114, %f107, %f8;
	cvt.f64.f32	%fd72, %f114;
	add.f64 	%fd73, %fd71, %fd72;
	mul.f32 	%f116, %f108, %f9;
	cvt.f64.f32	%fd74, %f116;
	add.f64 	%fd75, %fd73, %fd74;
	ld.global.v4.f32 	{%f117, %f118, %f119, %f120}, [%rd5+144];
	mul.f32 	%f122, %f117, %f6;
	cvt.f64.f32	%fd76, %f122;
	add.f64 	%fd77, %fd75, %fd76;
	mul.f32 	%f124, %f118, %f7;
	cvt.f64.f32	%fd78, %f124;
	add.f64 	%fd79, %fd77, %fd78;
	mul.f32 	%f126, %f119, %f8;
	cvt.f64.f32	%fd80, %f126;
	add.f64 	%fd81, %fd79, %fd80;
	mul.f32 	%f128, %f120, %f9;
	cvt.f64.f32	%fd82, %f128;
	add.f64 	%fd83, %fd81, %fd82;
	ld.global.v4.f32 	{%f129, %f130, %f131, %f132}, [%rd5+160];
	mul.f32 	%f134, %f129, %f6;
	cvt.f64.f32	%fd84, %f134;
	add.f64 	%fd85, %fd83, %fd84;
	mul.f32 	%f136, %f130, %f7;
	cvt.f64.f32	%fd86, %f136;
	add.f64 	%fd87, %fd85, %fd86;
	mul.f32 	%f138, %f131, %f8;
	cvt.f64.f32	%fd88, %f138;
	add.f64 	%fd89, %fd87, %fd88;
	mul.f32 	%f140, %f132, %f9;
	cvt.f64.f32	%fd90, %f140;
	add.f64 	%fd91, %fd89, %fd90;
	ld.global.v4.f32 	{%f141, %f142, %f143, %f144}, [%rd5+176];
	mul.f32 	%f146, %f141, %f6;
	cvt.f64.f32	%fd92, %f146;
	add.f64 	%fd93, %fd91, %fd92;
	mul.f32 	%f148, %f142, %f7;
	cvt.f64.f32	%fd94, %f148;
	add.f64 	%fd95, %fd93, %fd94;
	mul.f32 	%f150, %f143, %f8;
	cvt.f64.f32	%fd96, %f150;
	add.f64 	%fd97, %fd95, %fd96;
	mul.f32 	%f152, %f144, %f9;
	cvt.f64.f32	%fd98, %f152;
	add.f64 	%fd99, %fd97, %fd98;
	ld.global.v4.f32 	{%f153, %f154, %f155, %f156}, [%rd5+192];
	mul.f32 	%f158, %f153, %f6;
	cvt.f64.f32	%fd100, %f158;
	add.f64 	%fd101, %fd99, %fd100;
	mul.f32 	%f160, %f154, %f7;
	cvt.f64.f32	%fd102, %f160;
	add.f64 	%fd103, %fd101, %fd102;
	mul.f32 	%f162, %f155, %f8;
	cvt.f64.f32	%fd104, %f162;
	add.f64 	%fd105, %fd103, %fd104;
	mul.f32 	%f164, %f156, %f9;
	cvt.f64.f32	%fd106, %f164;
	add.f64 	%fd107, %fd105, %fd106;
	ld.global.v4.f32 	{%f165, %f166, %f167, %f168}, [%rd5+208];
	mul.f32 	%f170, %f165, %f6;
	cvt.f64.f32	%fd108, %f170;
	add.f64 	%fd109, %fd107, %fd108;
	mul.f32 	%f172, %f166, %f7;
	cvt.f64.f32	%fd110, %f172;
	add.f64 	%fd111, %fd109, %fd110;
	mul.f32 	%f174, %f167, %f8;
	cvt.f64.f32	%fd112, %f174;
	add.f64 	%fd113, %fd111, %fd112;
	mul.f32 	%f176, %f168, %f9;
	cvt.f64.f32	%fd114, %f176;
	add.f64 	%fd115, %fd113, %fd114;
	ld.global.v4.f32 	{%f177, %f178, %f179, %f180}, [%rd5+224];
	mul.f32 	%f182, %f177, %f6;
	cvt.f64.f32	%fd116, %f182;
	add.f64 	%fd117, %fd115, %fd116;
	mul.f32 	%f184, %f178, %f7;
	cvt.f64.f32	%fd118, %f184;
	add.f64 	%fd119, %fd117, %fd118;
	mul.f32 	%f186, %f179, %f8;
	cvt.f64.f32	%fd120, %f186;
	add.f64 	%fd121, %fd119, %fd120;
	mul.f32 	%f188, %f180, %f9;
	cvt.f64.f32	%fd122, %f188;
	add.f64 	%fd123, %fd121, %fd122;
	ld.global.v4.f32 	{%f189, %f190, %f191, %f192}, [%rd5+240];
	mul.f32 	%f194, %f189, %f6;
	cvt.f64.f32	%fd124, %f194;
	add.f64 	%fd125, %fd123, %fd124;
	mul.f32 	%f196, %f190, %f7;
	cvt.f64.f32	%fd126, %f196;
	add.f64 	%fd127, %fd125, %fd126;
	mul.f32 	%f198, %f191, %f8;
	cvt.f64.f32	%fd128, %f198;
	add.f64 	%fd129, %fd127, %fd128;
	mul.f32 	%f200, %f192, %f9;
	cvt.f64.f32	%fd130, %f200;
	add.f64 	%fd131, %fd129, %fd130;
	add.s32 	%r9, %r7, 1;
	add.s32 	%r27, %r27, %r13;
	add.s32 	%r29, %r29, 1;
	setp.ne.s32	%p2, %r29, 0;
	mov.u32 	%r28, %r9;
	@%p2 bra 	BB58_3;

	add.s32 	%r25, %r26, %r1;
	mul.wide.u32 	%rd8, %r25, 8;
	add.s64 	%rd9, %rd3, %rd8;
	st.global.f64 	[%rd9], %fd131;
	add.s32 	%r26, %r26, 1;
	setp.lt.u32	%p3, %r26, %r13;
	@%p3 bra 	BB58_2;

BB58_5:
	ret;
}

	// .globl	float444_sum2d_N_weighted
.entry float444_sum2d_N_weighted(
	.param .u64 .ptr .global .align 16 float444_sum2d_N_weighted_param_0,
	.param .u64 .ptr .global .align 16 float444_sum2d_N_weighted_param_1,
	.param .u32 float444_sum2d_N_weighted_param_2,
	.param .u32 float444_sum2d_N_weighted_param_3,
	.param .u32 float444_sum2d_N_weighted_param_4,
	.param .u32 float444_sum2d_N_weighted_param_5,
	.param .u64 .ptr .global .align 8 float444_sum2d_N_weighted_param_6
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<201>;
	.reg .b32 	%r<31>;
	.reg .f64 	%fd<136>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd1, [float444_sum2d_N_weighted_param_0];
	ld.param.u64 	%rd2, [float444_sum2d_N_weighted_param_1];
	ld.param.u32 	%r13, [float444_sum2d_N_weighted_param_2];
	ld.param.u32 	%r14, [float444_sum2d_N_weighted_param_3];
	ld.param.u32 	%r15, [float444_sum2d_N_weighted_param_4];
	ld.param.u32 	%r16, [float444_sum2d_N_weighted_param_5];
	ld.param.u64 	%rd3, [float444_sum2d_N_weighted_param_6];
	setp.eq.s32	%p1, %r13, 0;
	@%p1 bra 	BB59_5;

	mov.b32	%r18, %envreg3;
	mov.u32 	%r19, %ctaid.x;
	mov.u32 	%r20, %ntid.x;
	mad.lo.s32 	%r21, %r19, %r20, %r18;
	mov.u32 	%r22, %tid.x;
	add.s32 	%r23, %r21, %r22;
	mad.lo.s32 	%r1, %r23, %r13, %r15;
	mul.lo.s32 	%r2, %r16, %r23;
	mad.lo.s32 	%r3, %r2, %r13, %r14;
	mov.u32 	%r17, 0;
	mov.u32 	%r30, %r17;

BB59_2:
	add.s32 	%r26, %r3, %r30;
	setp.eq.s32	%p2, %r16, 0;
	mov.f64 	%fd135, 0d0000000000000000;
	mov.f64 	%fd134, %fd135;
	mov.u32 	%r27, %r2;
	mov.u32 	%r29, %r17;
	@%p2 bra 	BB59_4;

BB59_3:
	mov.u32 	%r8, %r29;
	mov.u32 	%r7, %r27;
	mul.wide.u32 	%rd4, %r26, 256;
	add.s64 	%rd5, %rd2, %rd4;
	ld.global.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd5];
	mul.wide.u32 	%rd6, %r7, 16;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.v4.f32 	{%f6, %f7, %f8, %f9}, [%rd7];
	mul.f32 	%f11, %f1, %f6;
	cvt.f64.f32	%fd6, %f11;
	add.f64 	%fd7, %fd135, %fd6;
	mul.f32 	%f14, %f2, %f7;
	cvt.f64.f32	%fd8, %f14;
	add.f64 	%fd9, %fd7, %fd8;
	mul.f32 	%f17, %f3, %f8;
	cvt.f64.f32	%fd10, %f17;
	add.f64 	%fd11, %fd9, %fd10;
	mul.f32 	%f20, %f4, %f9;
	cvt.f64.f32	%fd12, %f20;
	add.f64 	%fd13, %fd11, %fd12;
	ld.global.v4.f32 	{%f21, %f22, %f23, %f24}, [%rd5+16];
	mul.f32 	%f26, %f21, %f6;
	cvt.f64.f32	%fd14, %f26;
	add.f64 	%fd15, %fd13, %fd14;
	mul.f32 	%f28, %f22, %f7;
	cvt.f64.f32	%fd16, %f28;
	add.f64 	%fd17, %fd15, %fd16;
	mul.f32 	%f30, %f23, %f8;
	cvt.f64.f32	%fd18, %f30;
	add.f64 	%fd19, %fd17, %fd18;
	mul.f32 	%f32, %f24, %f9;
	cvt.f64.f32	%fd20, %f32;
	add.f64 	%fd21, %fd19, %fd20;
	ld.global.v4.f32 	{%f33, %f34, %f35, %f36}, [%rd5+32];
	mul.f32 	%f38, %f33, %f6;
	cvt.f64.f32	%fd22, %f38;
	add.f64 	%fd23, %fd21, %fd22;
	mul.f32 	%f40, %f34, %f7;
	cvt.f64.f32	%fd24, %f40;
	add.f64 	%fd25, %fd23, %fd24;
	mul.f32 	%f42, %f35, %f8;
	cvt.f64.f32	%fd26, %f42;
	add.f64 	%fd27, %fd25, %fd26;
	mul.f32 	%f44, %f36, %f9;
	cvt.f64.f32	%fd28, %f44;
	add.f64 	%fd29, %fd27, %fd28;
	ld.global.v4.f32 	{%f45, %f46, %f47, %f48}, [%rd5+48];
	mul.f32 	%f50, %f45, %f6;
	cvt.f64.f32	%fd30, %f50;
	add.f64 	%fd31, %fd29, %fd30;
	mul.f32 	%f52, %f46, %f7;
	cvt.f64.f32	%fd32, %f52;
	add.f64 	%fd33, %fd31, %fd32;
	mul.f32 	%f54, %f47, %f8;
	cvt.f64.f32	%fd34, %f54;
	add.f64 	%fd35, %fd33, %fd34;
	mul.f32 	%f56, %f48, %f9;
	cvt.f64.f32	%fd36, %f56;
	add.f64 	%fd37, %fd35, %fd36;
	ld.global.v4.f32 	{%f57, %f58, %f59, %f60}, [%rd5+64];
	mul.f32 	%f62, %f57, %f6;
	cvt.f64.f32	%fd38, %f62;
	add.f64 	%fd39, %fd37, %fd38;
	mul.f32 	%f64, %f58, %f7;
	cvt.f64.f32	%fd40, %f64;
	add.f64 	%fd41, %fd39, %fd40;
	mul.f32 	%f66, %f59, %f8;
	cvt.f64.f32	%fd42, %f66;
	add.f64 	%fd43, %fd41, %fd42;
	mul.f32 	%f68, %f60, %f9;
	cvt.f64.f32	%fd44, %f68;
	add.f64 	%fd45, %fd43, %fd44;
	ld.global.v4.f32 	{%f69, %f70, %f71, %f72}, [%rd5+80];
	mul.f32 	%f74, %f69, %f6;
	cvt.f64.f32	%fd46, %f74;
	add.f64 	%fd47, %fd45, %fd46;
	mul.f32 	%f76, %f70, %f7;
	cvt.f64.f32	%fd48, %f76;
	add.f64 	%fd49, %fd47, %fd48;
	mul.f32 	%f78, %f71, %f8;
	cvt.f64.f32	%fd50, %f78;
	add.f64 	%fd51, %fd49, %fd50;
	mul.f32 	%f80, %f72, %f9;
	cvt.f64.f32	%fd52, %f80;
	add.f64 	%fd53, %fd51, %fd52;
	ld.global.v4.f32 	{%f81, %f82, %f83, %f84}, [%rd5+96];
	mul.f32 	%f86, %f81, %f6;
	cvt.f64.f32	%fd54, %f86;
	add.f64 	%fd55, %fd53, %fd54;
	mul.f32 	%f88, %f82, %f7;
	cvt.f64.f32	%fd56, %f88;
	add.f64 	%fd57, %fd55, %fd56;
	mul.f32 	%f90, %f83, %f8;
	cvt.f64.f32	%fd58, %f90;
	add.f64 	%fd59, %fd57, %fd58;
	mul.f32 	%f92, %f84, %f9;
	cvt.f64.f32	%fd60, %f92;
	add.f64 	%fd61, %fd59, %fd60;
	ld.global.v4.f32 	{%f93, %f94, %f95, %f96}, [%rd5+112];
	mul.f32 	%f98, %f93, %f6;
	cvt.f64.f32	%fd62, %f98;
	add.f64 	%fd63, %fd61, %fd62;
	mul.f32 	%f100, %f94, %f7;
	cvt.f64.f32	%fd64, %f100;
	add.f64 	%fd65, %fd63, %fd64;
	mul.f32 	%f102, %f95, %f8;
	cvt.f64.f32	%fd66, %f102;
	add.f64 	%fd67, %fd65, %fd66;
	mul.f32 	%f104, %f96, %f9;
	cvt.f64.f32	%fd68, %f104;
	add.f64 	%fd69, %fd67, %fd68;
	ld.global.v4.f32 	{%f105, %f106, %f107, %f108}, [%rd5+128];
	mul.f32 	%f110, %f105, %f6;
	cvt.f64.f32	%fd70, %f110;
	add.f64 	%fd71, %fd69, %fd70;
	mul.f32 	%f112, %f106, %f7;
	cvt.f64.f32	%fd72, %f112;
	add.f64 	%fd73, %fd71, %fd72;
	mul.f32 	%f114, %f107, %f8;
	cvt.f64.f32	%fd74, %f114;
	add.f64 	%fd75, %fd73, %fd74;
	mul.f32 	%f116, %f108, %f9;
	cvt.f64.f32	%fd76, %f116;
	add.f64 	%fd77, %fd75, %fd76;
	ld.global.v4.f32 	{%f117, %f118, %f119, %f120}, [%rd5+144];
	mul.f32 	%f122, %f117, %f6;
	cvt.f64.f32	%fd78, %f122;
	add.f64 	%fd79, %fd77, %fd78;
	mul.f32 	%f124, %f118, %f7;
	cvt.f64.f32	%fd80, %f124;
	add.f64 	%fd81, %fd79, %fd80;
	mul.f32 	%f126, %f119, %f8;
	cvt.f64.f32	%fd82, %f126;
	add.f64 	%fd83, %fd81, %fd82;
	mul.f32 	%f128, %f120, %f9;
	cvt.f64.f32	%fd84, %f128;
	add.f64 	%fd85, %fd83, %fd84;
	ld.global.v4.f32 	{%f129, %f130, %f131, %f132}, [%rd5+160];
	mul.f32 	%f134, %f129, %f6;
	cvt.f64.f32	%fd86, %f134;
	add.f64 	%fd87, %fd85, %fd86;
	mul.f32 	%f136, %f130, %f7;
	cvt.f64.f32	%fd88, %f136;
	add.f64 	%fd89, %fd87, %fd88;
	mul.f32 	%f138, %f131, %f8;
	cvt.f64.f32	%fd90, %f138;
	add.f64 	%fd91, %fd89, %fd90;
	mul.f32 	%f140, %f132, %f9;
	cvt.f64.f32	%fd92, %f140;
	add.f64 	%fd93, %fd91, %fd92;
	ld.global.v4.f32 	{%f141, %f142, %f143, %f144}, [%rd5+176];
	mul.f32 	%f146, %f141, %f6;
	cvt.f64.f32	%fd94, %f146;
	add.f64 	%fd95, %fd93, %fd94;
	mul.f32 	%f148, %f142, %f7;
	cvt.f64.f32	%fd96, %f148;
	add.f64 	%fd97, %fd95, %fd96;
	mul.f32 	%f150, %f143, %f8;
	cvt.f64.f32	%fd98, %f150;
	add.f64 	%fd99, %fd97, %fd98;
	mul.f32 	%f152, %f144, %f9;
	cvt.f64.f32	%fd100, %f152;
	add.f64 	%fd101, %fd99, %fd100;
	ld.global.v4.f32 	{%f153, %f154, %f155, %f156}, [%rd5+192];
	mul.f32 	%f158, %f153, %f6;
	cvt.f64.f32	%fd102, %f158;
	add.f64 	%fd103, %fd101, %fd102;
	mul.f32 	%f160, %f154, %f7;
	cvt.f64.f32	%fd104, %f160;
	add.f64 	%fd105, %fd103, %fd104;
	mul.f32 	%f162, %f155, %f8;
	cvt.f64.f32	%fd106, %f162;
	add.f64 	%fd107, %fd105, %fd106;
	mul.f32 	%f164, %f156, %f9;
	cvt.f64.f32	%fd108, %f164;
	add.f64 	%fd109, %fd107, %fd108;
	ld.global.v4.f32 	{%f165, %f166, %f167, %f168}, [%rd5+208];
	mul.f32 	%f170, %f165, %f6;
	cvt.f64.f32	%fd110, %f170;
	add.f64 	%fd111, %fd109, %fd110;
	mul.f32 	%f172, %f166, %f7;
	cvt.f64.f32	%fd112, %f172;
	add.f64 	%fd113, %fd111, %fd112;
	mul.f32 	%f174, %f167, %f8;
	cvt.f64.f32	%fd114, %f174;
	add.f64 	%fd115, %fd113, %fd114;
	mul.f32 	%f176, %f168, %f9;
	cvt.f64.f32	%fd116, %f176;
	add.f64 	%fd117, %fd115, %fd116;
	ld.global.v4.f32 	{%f177, %f178, %f179, %f180}, [%rd5+224];
	mul.f32 	%f182, %f177, %f6;
	cvt.f64.f32	%fd118, %f182;
	add.f64 	%fd119, %fd117, %fd118;
	mul.f32 	%f184, %f178, %f7;
	cvt.f64.f32	%fd120, %f184;
	add.f64 	%fd121, %fd119, %fd120;
	mul.f32 	%f186, %f179, %f8;
	cvt.f64.f32	%fd122, %f186;
	add.f64 	%fd123, %fd121, %fd122;
	mul.f32 	%f188, %f180, %f9;
	cvt.f64.f32	%fd124, %f188;
	add.f64 	%fd125, %fd123, %fd124;
	ld.global.v4.f32 	{%f189, %f190, %f191, %f192}, [%rd5+240];
	mul.f32 	%f194, %f189, %f6;
	cvt.f64.f32	%fd126, %f194;
	add.f64 	%fd127, %fd125, %fd126;
	mul.f32 	%f196, %f190, %f7;
	cvt.f64.f32	%fd128, %f196;
	add.f64 	%fd129, %fd127, %fd128;
	mul.f32 	%f198, %f191, %f8;
	cvt.f64.f32	%fd130, %f198;
	add.f64 	%fd131, %fd129, %fd130;
	mul.f32 	%f200, %f192, %f9;
	cvt.f64.f32	%fd132, %f200;
	add.f64 	%fd135, %fd131, %fd132;
	add.s32 	%r9, %r7, 1;
	add.s32 	%r26, %r26, %r13;
	add.s32 	%r11, %r8, 1;
	setp.lt.u32	%p3, %r11, %r16;
	mov.u32 	%r27, %r9;
	mov.u32 	%r29, %r11;
	mov.f64 	%fd134, %fd135;
	@%p3 bra 	BB59_3;

BB59_4:
	add.s32 	%r25, %r1, %r30;
	mul.wide.u32 	%rd8, %r25, 8;
	add.s64 	%rd9, %rd3, %rd8;
	st.global.f64 	[%rd9], %fd134;
	add.s32 	%r30, %r30, 1;
	setp.lt.u32	%p4, %r30, %r13;
	@%p4 bra 	BB59_2;

BB59_5:
	ret;
}


  