//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-21458526
// Driver 375.26
// Based on LLVM 3.4svn
//

.version 5.0
.target sm_35, texmode_independent
.address_size 64

	// .globl	kernelmultiplycomplex

.entry kernelmultiplycomplex(
	.param .u64 .ptr .global .align 8 kernelmultiplycomplex_param_0,
	.param .u64 .ptr .global .align 8 kernelmultiplycomplex_param_1,
	.param .u64 .ptr .global .align 8 kernelmultiplycomplex_param_2
)
{
	.reg .f32 	%f<25>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [kernelmultiplycomplex_param_0];
	ld.param.u64 	%rd2, [kernelmultiplycomplex_param_1];
	ld.param.u64 	%rd3, [kernelmultiplycomplex_param_2];
	mov.b32	%r1, %envreg3;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mad.lo.s32 	%r4, %r3, %r2, %r1;
	mov.u32 	%r5, %tid.x;
	add.s32 	%r6, %r4, %r5;
	mul.wide.u32 	%rd4, %r6, 8;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.v2.f32 	{%f1, %f2}, [%rd5];
	add.s64 	%rd6, %rd2, %rd4;
	ld.global.v2.f32 	{%f4, %f5}, [%rd6];
	mul.f32 	%f9, %f2, %f5;
	neg.f32 	%f10, %f9;
	add.s64 	%rd7, %rd3, %rd4;
	ld.global.v2.f32 	{%f11, %f12}, [%rd7];
	fma.rn.f32 	%f13, %f1, %f4, %f10;
	st.global.v2.f32 	[%rd7], {%f13, %f12};
	ld.global.v2.f32 	{%f15, %f16}, [%rd5];
	ld.global.v2.f32 	{%f18, %f19}, [%rd6];
	mul.f32 	%f23, %f16, %f18;
	fma.rn.f32 	%f24, %f15, %f19, %f23;
	st.global.v2.f32 	[%rd7], {%f13, %f24};
	ret;
}

	// .globl	kerneladdcomplex
.entry kerneladdcomplex(
	.param .u64 .ptr .global .align 8 kerneladdcomplex_param_0,
	.param .u64 .ptr .global .align 8 kerneladdcomplex_param_1,
	.param .u64 .ptr .global .align 8 kerneladdcomplex_param_2
)
{
	.reg .f32 	%f<18>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [kerneladdcomplex_param_0];
	ld.param.u64 	%rd2, [kerneladdcomplex_param_1];
	ld.param.u64 	%rd3, [kerneladdcomplex_param_2];
	mov.b32	%r1, %envreg3;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mad.lo.s32 	%r4, %r3, %r2, %r1;
	mov.u32 	%r5, %tid.x;
	add.s32 	%r6, %r4, %r5;
	mul.wide.u32 	%rd4, %r6, 8;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.v2.f32 	{%f1, %f2}, [%rd5];
	add.s64 	%rd6, %rd2, %rd4;
	ld.global.v2.f32 	{%f4, %f5}, [%rd6];
	add.s64 	%rd7, %rd3, %rd4;
	ld.global.v2.f32 	{%f7, %f8}, [%rd7];
	add.f32 	%f9, %f1, %f4;
	st.global.v2.f32 	[%rd7], {%f9, %f8};
	ld.global.v2.f32 	{%f11, %f12}, [%rd5];
	ld.global.v2.f32 	{%f14, %f15}, [%rd6];
	add.f32 	%f17, %f12, %f15;
	st.global.v2.f32 	[%rd7], {%f9, %f17};
	ret;
}

	// .globl	kernelmultiplycomplexf
.entry kernelmultiplycomplexf(
	.param .u64 .ptr .global .align 8 kernelmultiplycomplexf_param_0,
	.param .u64 .ptr .global .align 4 kernelmultiplycomplexf_param_1,
	.param .u64 .ptr .global .align 8 kernelmultiplycomplexf_param_2
)
{
	.reg .f32 	%f<14>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [kernelmultiplycomplexf_param_0];
	ld.param.u64 	%rd2, [kernelmultiplycomplexf_param_1];
	ld.param.u64 	%rd3, [kernelmultiplycomplexf_param_2];
	mov.b32	%r1, %envreg3;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mad.lo.s32 	%r4, %r3, %r2, %r1;
	mov.u32 	%r5, %tid.x;
	add.s32 	%r6, %r4, %r5;
	mul.wide.u32 	%rd4, %r6, 8;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.v2.f32 	{%f1, %f2}, [%rd5];
	mul.wide.u32 	%rd6, %r6, 4;
	add.s64 	%rd7, %rd2, %rd6;
	ld.global.f32 	%f4, [%rd7];
	add.s64 	%rd8, %rd3, %rd4;
	ld.global.v2.f32 	{%f5, %f6}, [%rd8];
	mul.f32 	%f7, %f1, %f4;
	st.global.v2.f32 	[%rd8], {%f7, %f6};
	ld.global.v2.f32 	{%f9, %f10}, [%rd5];
	ld.global.f32 	%f12, [%rd7];
	mul.f32 	%f13, %f10, %f12;
	st.global.v2.f32 	[%rd8], {%f7, %f13};
	ret;
}

	// .globl	kernelcomplexsplit
.entry kernelcomplexsplit(
	.param .u64 .ptr .global .align 8 kernelcomplexsplit_param_0,
	.param .u64 .ptr .global .align 4 kernelcomplexsplit_param_1,
	.param .u64 .ptr .global .align 4 kernelcomplexsplit_param_2
)
{
	.reg .f32 	%f<7>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [kernelcomplexsplit_param_0];
	ld.param.u64 	%rd2, [kernelcomplexsplit_param_1];
	ld.param.u64 	%rd3, [kernelcomplexsplit_param_2];
	mov.b32	%r1, %envreg3;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mad.lo.s32 	%r4, %r3, %r2, %r1;
	mov.u32 	%r5, %tid.x;
	add.s32 	%r6, %r4, %r5;
	mul.wide.u32 	%rd4, %r6, 8;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.v2.f32 	{%f1, %f2}, [%rd5];
	mul.wide.u32 	%rd6, %r6, 4;
	add.s64 	%rd7, %rd2, %rd6;
	st.global.f32 	[%rd7], %f1;
	ld.global.v2.f32 	{%f4, %f5}, [%rd5];
	add.s64 	%rd8, %rd3, %rd6;
	st.global.f32 	[%rd8], %f5;
	ret;
}

	// .globl	kerneltensormult_c4_c
.entry kerneltensormult_c4_c(
	.param .u64 .ptr .global .align 16 kerneltensormult_c4_c_param_0,
	.param .u64 .ptr .global .align 16 kerneltensormult_c4_c_param_1,
	.param .u64 .ptr .global .align 4 kerneltensormult_c4_c_param_2,
	.param .u64 .ptr .global .align 4 kerneltensormult_c4_c_param_3,
	.param .u64 .ptr .global .align 16 kerneltensormult_c4_c_param_4,
	.param .u64 .ptr .global .align 16 kerneltensormult_c4_c_param_5
)
{
	.reg .f32 	%f<57>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<15>;


	ld.param.u64 	%rd1, [kerneltensormult_c4_c_param_0];
	ld.param.u64 	%rd2, [kerneltensormult_c4_c_param_1];
	ld.param.u64 	%rd3, [kerneltensormult_c4_c_param_2];
	ld.param.u64 	%rd4, [kerneltensormult_c4_c_param_3];
	ld.param.u64 	%rd5, [kerneltensormult_c4_c_param_4];
	ld.param.u64 	%rd6, [kerneltensormult_c4_c_param_5];
	mov.b32	%r1, %envreg3;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mad.lo.s32 	%r4, %r3, %r2, %r1;
	mov.u32 	%r5, %tid.x;
	add.s32 	%r6, %r4, %r5;
	mul.wide.u32 	%rd7, %r6, 16;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd8];
	mul.wide.u32 	%rd9, %r6, 4;
	add.s64 	%rd10, %rd3, %rd9;
	ld.global.f32 	%f9, [%rd10];
	add.s64 	%rd11, %rd2, %rd7;
	ld.global.v4.f32 	{%f10, %f11, %f12, %f13}, [%rd11];
	add.s64 	%rd12, %rd4, %rd9;
	ld.global.f32 	%f18, [%rd12];
	mul.f32 	%f19, %f13, %f18;
	mul.f32 	%f20, %f12, %f18;
	mul.f32 	%f21, %f11, %f18;
	mul.f32 	%f22, %f10, %f18;
	neg.f32 	%f23, %f22;
	neg.f32 	%f24, %f21;
	neg.f32 	%f25, %f20;
	neg.f32 	%f26, %f19;
	add.s64 	%rd13, %rd5, %rd7;
	fma.rn.f32 	%f27, %f4, %f9, %f26;
	fma.rn.f32 	%f28, %f3, %f9, %f25;
	fma.rn.f32 	%f29, %f2, %f9, %f24;
	fma.rn.f32 	%f30, %f1, %f9, %f23;
	st.global.v4.f32 	[%rd13], {%f30, %f29, %f28, %f27};
	ld.global.v4.f32 	{%f31, %f32, %f33, %f34}, [%rd8];
	ld.global.f32 	%f39, [%rd12];
	ld.global.v4.f32 	{%f40, %f41, %f42, %f43}, [%rd11];
	ld.global.f32 	%f48, [%rd10];
	mul.f32 	%f49, %f40, %f48;
	mul.f32 	%f50, %f41, %f48;
	mul.f32 	%f51, %f42, %f48;
	mul.f32 	%f52, %f43, %f48;
	add.s64 	%rd14, %rd6, %rd7;
	fma.rn.f32 	%f53, %f34, %f39, %f52;
	fma.rn.f32 	%f54, %f33, %f39, %f51;
	fma.rn.f32 	%f55, %f32, %f39, %f50;
	fma.rn.f32 	%f56, %f31, %f39, %f49;
	st.global.v4.f32 	[%rd14], {%f56, %f55, %f54, %f53};
	ret;
}

	// .globl	kerneltensormult_4_c
.entry kerneltensormult_4_c(
	.param .u64 .ptr .global .align 16 kerneltensormult_4_c_param_0,
	.param .u64 .ptr .global .align 4 kerneltensormult_4_c_param_1,
	.param .u64 .ptr .global .align 4 kerneltensormult_4_c_param_2,
	.param .u64 .ptr .global .align 16 kerneltensormult_4_c_param_3,
	.param .u64 .ptr .global .align 16 kerneltensormult_4_c_param_4
)
{
	.reg .f32 	%f<27>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd1, [kerneltensormult_4_c_param_0];
	ld.param.u64 	%rd2, [kerneltensormult_4_c_param_1];
	ld.param.u64 	%rd3, [kerneltensormult_4_c_param_2];
	ld.param.u64 	%rd4, [kerneltensormult_4_c_param_3];
	ld.param.u64 	%rd5, [kerneltensormult_4_c_param_4];
	mov.b32	%r1, %envreg3;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mad.lo.s32 	%r4, %r3, %r2, %r1;
	mov.u32 	%r5, %tid.x;
	add.s32 	%r6, %r4, %r5;
	mul.wide.u32 	%rd6, %r6, 16;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd7];
	mul.wide.u32 	%rd8, %r6, 4;
	add.s64 	%rd9, %rd2, %rd8;
	ld.global.f32 	%f9, [%rd9];
	add.s64 	%rd10, %rd4, %rd6;
	mul.f32 	%f10, %f4, %f9;
	mul.f32 	%f11, %f3, %f9;
	mul.f32 	%f12, %f2, %f9;
	mul.f32 	%f13, %f1, %f9;
	st.global.v4.f32 	[%rd10], {%f13, %f12, %f11, %f10};
	ld.global.v4.f32 	{%f14, %f15, %f16, %f17}, [%rd7];
	add.s64 	%rd11, %rd3, %rd8;
	ld.global.f32 	%f22, [%rd11];
	add.s64 	%rd12, %rd5, %rd6;
	mul.f32 	%f23, %f17, %f22;
	mul.f32 	%f24, %f16, %f22;
	mul.f32 	%f25, %f15, %f22;
	mul.f32 	%f26, %f14, %f22;
	st.global.v4.f32 	[%rd12], {%f26, %f25, %f24, %f23};
	ret;
}

	// .globl	kerneltensormult_c_c44
.entry kerneltensormult_c_c44(
	.param .u64 .ptr .global .align 4 kerneltensormult_c_c44_param_0,
	.param .u64 .ptr .global .align 4 kerneltensormult_c_c44_param_1,
	.param .u64 .ptr .global .align 16 kerneltensormult_c_c44_param_2,
	.param .u64 .ptr .global .align 16 kerneltensormult_c_c44_param_3,
	.param .u64 .ptr .global .align 16 kerneltensormult_c_c44_param_4,
	.param .u64 .ptr .global .align 16 kerneltensormult_c_c44_param_5
)
{
	.reg .f32 	%f<225>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<15>;


	ld.param.u64 	%rd1, [kerneltensormult_c_c44_param_0];
	ld.param.u64 	%rd2, [kerneltensormult_c_c44_param_1];
	ld.param.u64 	%rd3, [kerneltensormult_c_c44_param_2];
	ld.param.u64 	%rd4, [kerneltensormult_c_c44_param_3];
	ld.param.u64 	%rd5, [kerneltensormult_c_c44_param_4];
	ld.param.u64 	%rd6, [kerneltensormult_c_c44_param_5];
	mov.b32	%r1, %envreg3;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mad.lo.s32 	%r4, %r3, %r2, %r1;
	mov.u32 	%r5, %tid.x;
	add.s32 	%r6, %r4, %r5;
	mul.wide.u32 	%rd7, %r6, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.f32 	%f1, [%rd8];
	mul.wide.u32 	%rd9, %r6, 64;
	add.s64 	%rd10, %rd3, %rd9;
	ld.global.v4.f32 	{%f2, %f3, %f4, %f5}, [%rd10];
	add.s64 	%rd11, %rd2, %rd7;
	ld.global.f32 	%f10, [%rd11];
	add.s64 	%rd12, %rd4, %rd9;
	ld.global.v4.f32 	{%f11, %f12, %f13, %f14}, [%rd12];
	mul.f32 	%f19, %f14, %f10;
	mul.f32 	%f20, %f13, %f10;
	mul.f32 	%f21, %f12, %f10;
	mul.f32 	%f22, %f11, %f10;
	neg.f32 	%f23, %f22;
	neg.f32 	%f24, %f21;
	neg.f32 	%f25, %f20;
	neg.f32 	%f26, %f19;
	add.s64 	%rd13, %rd5, %rd9;
	fma.rn.f32 	%f27, %f1, %f5, %f26;
	fma.rn.f32 	%f28, %f1, %f4, %f25;
	fma.rn.f32 	%f29, %f1, %f3, %f24;
	fma.rn.f32 	%f30, %f1, %f2, %f23;
	st.global.v4.f32 	[%rd13], {%f30, %f29, %f28, %f27};
	ld.global.f32 	%f31, [%rd8];
	ld.global.v4.f32 	{%f32, %f33, %f34, %f35}, [%rd10+16];
	ld.global.f32 	%f40, [%rd11];
	ld.global.v4.f32 	{%f41, %f42, %f43, %f44}, [%rd12+16];
	mul.f32 	%f49, %f44, %f40;
	mul.f32 	%f50, %f43, %f40;
	mul.f32 	%f51, %f42, %f40;
	mul.f32 	%f52, %f41, %f40;
	neg.f32 	%f53, %f52;
	neg.f32 	%f54, %f51;
	neg.f32 	%f55, %f50;
	neg.f32 	%f56, %f49;
	fma.rn.f32 	%f57, %f31, %f35, %f56;
	fma.rn.f32 	%f58, %f31, %f34, %f55;
	fma.rn.f32 	%f59, %f31, %f33, %f54;
	fma.rn.f32 	%f60, %f31, %f32, %f53;
	st.global.v4.f32 	[%rd13+16], {%f60, %f59, %f58, %f57};
	ld.global.f32 	%f61, [%rd8];
	ld.global.v4.f32 	{%f62, %f63, %f64, %f65}, [%rd10+32];
	ld.global.f32 	%f70, [%rd11];
	ld.global.v4.f32 	{%f71, %f72, %f73, %f74}, [%rd12+32];
	mul.f32 	%f79, %f74, %f70;
	mul.f32 	%f80, %f73, %f70;
	mul.f32 	%f81, %f72, %f70;
	mul.f32 	%f82, %f71, %f70;
	neg.f32 	%f83, %f82;
	neg.f32 	%f84, %f81;
	neg.f32 	%f85, %f80;
	neg.f32 	%f86, %f79;
	fma.rn.f32 	%f87, %f61, %f65, %f86;
	fma.rn.f32 	%f88, %f61, %f64, %f85;
	fma.rn.f32 	%f89, %f61, %f63, %f84;
	fma.rn.f32 	%f90, %f61, %f62, %f83;
	st.global.v4.f32 	[%rd13+32], {%f90, %f89, %f88, %f87};
	ld.global.f32 	%f91, [%rd8];
	ld.global.v4.f32 	{%f92, %f93, %f94, %f95}, [%rd10+48];
	ld.global.f32 	%f100, [%rd11];
	ld.global.v4.f32 	{%f101, %f102, %f103, %f104}, [%rd12+48];
	mul.f32 	%f109, %f104, %f100;
	mul.f32 	%f110, %f103, %f100;
	mul.f32 	%f111, %f102, %f100;
	mul.f32 	%f112, %f101, %f100;
	neg.f32 	%f113, %f112;
	neg.f32 	%f114, %f111;
	neg.f32 	%f115, %f110;
	neg.f32 	%f116, %f109;
	fma.rn.f32 	%f117, %f91, %f95, %f116;
	fma.rn.f32 	%f118, %f91, %f94, %f115;
	fma.rn.f32 	%f119, %f91, %f93, %f114;
	fma.rn.f32 	%f120, %f91, %f92, %f113;
	st.global.v4.f32 	[%rd13+48], {%f120, %f119, %f118, %f117};
	ld.global.f32 	%f121, [%rd8];
	ld.global.v4.f32 	{%f122, %f123, %f124, %f125}, [%rd12];
	ld.global.f32 	%f130, [%rd11];
	ld.global.v4.f32 	{%f131, %f132, %f133, %f134}, [%rd10];
	mul.f32 	%f139, %f131, %f130;
	mul.f32 	%f140, %f132, %f130;
	mul.f32 	%f141, %f133, %f130;
	mul.f32 	%f142, %f134, %f130;
	add.s64 	%rd14, %rd6, %rd9;
	fma.rn.f32 	%f143, %f121, %f125, %f142;
	fma.rn.f32 	%f144, %f121, %f124, %f141;
	fma.rn.f32 	%f145, %f121, %f123, %f140;
	fma.rn.f32 	%f146, %f121, %f122, %f139;
	st.global.v4.f32 	[%rd14], {%f146, %f145, %f144, %f143};
	ld.global.f32 	%f147, [%rd8];
	ld.global.v4.f32 	{%f148, %f149, %f150, %f151}, [%rd12+16];
	ld.global.f32 	%f156, [%rd11];
	ld.global.v4.f32 	{%f157, %f158, %f159, %f160}, [%rd10+16];
	mul.f32 	%f165, %f157, %f156;
	mul.f32 	%f166, %f158, %f156;
	mul.f32 	%f167, %f159, %f156;
	mul.f32 	%f168, %f160, %f156;
	fma.rn.f32 	%f169, %f147, %f151, %f168;
	fma.rn.f32 	%f170, %f147, %f150, %f167;
	fma.rn.f32 	%f171, %f147, %f149, %f166;
	fma.rn.f32 	%f172, %f147, %f148, %f165;
	st.global.v4.f32 	[%rd14+16], {%f172, %f171, %f170, %f169};
	ld.global.f32 	%f173, [%rd8];
	ld.global.v4.f32 	{%f174, %f175, %f176, %f177}, [%rd12+32];
	ld.global.f32 	%f182, [%rd11];
	ld.global.v4.f32 	{%f183, %f184, %f185, %f186}, [%rd10+32];
	mul.f32 	%f191, %f183, %f182;
	mul.f32 	%f192, %f184, %f182;
	mul.f32 	%f193, %f185, %f182;
	mul.f32 	%f194, %f186, %f182;
	fma.rn.f32 	%f195, %f173, %f177, %f194;
	fma.rn.f32 	%f196, %f173, %f176, %f193;
	fma.rn.f32 	%f197, %f173, %f175, %f192;
	fma.rn.f32 	%f198, %f173, %f174, %f191;
	st.global.v4.f32 	[%rd14+32], {%f198, %f197, %f196, %f195};
	ld.global.f32 	%f199, [%rd8];
	ld.global.v4.f32 	{%f200, %f201, %f202, %f203}, [%rd12+48];
	ld.global.f32 	%f208, [%rd11];
	ld.global.v4.f32 	{%f209, %f210, %f211, %f212}, [%rd10+48];
	mul.f32 	%f217, %f209, %f208;
	mul.f32 	%f218, %f210, %f208;
	mul.f32 	%f219, %f211, %f208;
	mul.f32 	%f220, %f212, %f208;
	fma.rn.f32 	%f221, %f199, %f203, %f220;
	fma.rn.f32 	%f222, %f199, %f202, %f219;
	fma.rn.f32 	%f223, %f199, %f201, %f218;
	fma.rn.f32 	%f224, %f199, %f200, %f217;
	st.global.v4.f32 	[%rd14+48], {%f224, %f223, %f222, %f221};
	ret;
}


  